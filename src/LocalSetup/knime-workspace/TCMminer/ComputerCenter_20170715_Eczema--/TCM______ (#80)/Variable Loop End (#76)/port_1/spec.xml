<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.knime.org/2008/09/XMLConfig http://www.knime.org/XMLConfig_2008_09.xsd" key="spec.xml">
<entry key="spec_name" type="xstring" value="default"/>
<entry key="number_columns" type="xint" value="50"/>
<config key="column_spec_0">
<entry key="column_name" type="xstring" value="FailingNodeStackTrace"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="9"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:43:10 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:43:11 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:43:11 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:43:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:43:12 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:43:12 INFO Remoting: Starting remoting%%0001017/07/17 22:43:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:34187]%%0001017/07/17 22:43:12 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:34187]%%0001017/07/17 22:43:12 INFO Utils: Successfully started service 'sparkDriver' on port 34187.%%0001017/07/17 22:43:12 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:43:12 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:43:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3aa77834-d57a-4da7-896b-a2403d3e6330%%0001017/07/17 22:43:12 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:43:12 INFO HttpFileServer: HTTP File server directory is /tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282/httpd-500a6141-463c-41dc-8411-7e27aaabf48c%%0001017/07/17 22:43:12 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:43:12 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:43:12 INFO AbstractConnector: Started SocketConnector@0.0.0.0:40517%%0001017/07/17 22:43:12 INFO Utils: Successfully started service 'HTTP file server' on port 40517.%%0001017/07/17 22:43:12 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:43:12 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:43:13 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:43:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:43:13 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:43:13 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:43:13 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:43:13 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:43:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:43:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:43:13 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:43:13 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:43:13 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:43:13 INFO Client: Uploading resource file:/tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282/__spark_conf__4995776726975785966.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22071/__spark_conf__4995776726975785966.zip%%0001017/07/17 22:43:14 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:43:14 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:43:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:43:14 INFO Client: Submitting application 22071 to ResourceManager%%0001017/07/17 22:43:14 INFO YarnClientImpl: Submitted application application_1491786134915_22071%%0001017/07/17 22:43:15 INFO Client: Application report for application_1491786134915_22071 (state: ACCEPTED)%%0001017/07/17 22:43:15 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302594621%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22071/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:43:16 INFO Client: Application report for application_1491786134915_22071 (state: ACCEPTED)%%0001017/07/17 22:43:17 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:54359/user/YarnAM#-293742013])%%0001017/07/17 22:43:17 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22071,http://hcnnc117:8088/proxy/application_1491786134915_22071), /proxy/application_1491786134915_22071%%0001017/07/17 22:43:17 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:43:17 INFO Client: Application report for application_1491786134915_22071 (state: RUNNING)%%0001017/07/17 22:43:17 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302594621%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22071/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:43:17 INFO YarnClientSchedulerBackend: Application application_1491786134915_22071 has started running.%%0001017/07/17 22:43:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38085.%%0001017/07/17 22:43:17 INFO NettyBlockTransferService: Server created on 38085%%0001017/07/17 22:43:17 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:43:17 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:43:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:38085 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 38085)%%0001017/07/17 22:43:17 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:43:18 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22071%%0001017/07/17 22:43:18 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:38085 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:43:18 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:43:18 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.13:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.9:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.8:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.10:50010%%0001017/07/17 22:43:18 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:43:18 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:43:18 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:43:18 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:43:18 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:43:18 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:38085 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:43:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:43:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:43:18 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:43:20 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:43:21 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:43:22 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://driverPropsFetcher@hcdnc318:43038]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc318:43038] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc318:43038%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:43:22 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc318:54325/user/Executor#-690596482]) with ID 1%%0001017/07/17 22:43:22 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:43:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc318, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:43:23 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc318:34123 with 530.0 MB RAM, BlockManagerId(1, hcdnc318, 34123)%%0001017/07/17 22:43:23 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://driverPropsFetcher@hcdnc327:56951]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc327:56951] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc327:56951%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:43:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc318:34123 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:43:23 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc327:42987/user/Executor#1621135318]) with ID 2%%0001017/07/17 22:43:23 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:43:23 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc327, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:43:23 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc327:52424 with 530.0 MB RAM, BlockManagerId(2, hcdnc327, 52424)%%0001017/07/17 22:43:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc318:34123 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:43:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc327:52424 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:43:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc327:52424 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:43:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15328 ms on hcdnc318 (1/2)%%0001017/07/17 22:43:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 15512 ms on hcdnc327 (2/2)%%0001017/07/17 22:43:39 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:43:39 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.981 s%%0001017/07/17 22:43:39 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 20.147374 s%%0001017/07/17 22:43:39 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:43:39 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:43:39 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:43:39 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:43:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:43:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:43:39 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:43:39 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:43:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:43:39 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:43:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:43:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:38085 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:43:39 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:43:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:43:39 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:43:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc327, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:43:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc318, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:43:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc318:34123 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:43:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc327:52424 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:43:59 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20797 ms on hcdnc318 (1/2)%%0001017/07/17 22:44:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21759 ms on hcdnc327 (2/2)%%0001017/07/17 22:44:00 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:00 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.760 s%%0001017/07/17 22:44:00 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:44:00 INFO DAGScheduler: running: Set()%%0001017/07/17 22:44:00 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:44:00 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:44:00 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:44:00 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:44:00 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:44:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:44:00 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:44:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:44:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:38085 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:44:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:44:00 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:44:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc318, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:00 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc327, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc327:52424 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:44:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc318:34123 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:44:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc318:54325%%0001017/07/17 22:44:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:44:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc327:42987%%0001017/07/17 22:44:02 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1676 ms on hcdnc327 (1/2)%%0001017/07/17 22:44:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1697 ms on hcdnc318 (2/2)%%0001017/07/17 22:44:02 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:02 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.699 s%%0001017/07/17 22:44:02 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 23.489542 s%%0001017/07/17 22:44:02 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:44:02 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:44:02 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:44:02 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:44:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:44:02 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:44:02 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:44:02 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:44:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:44:02 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:44:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:44:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:38085 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:44:02 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:44:02 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:44:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc318, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:02 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc327, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc318:34123 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:44:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc327:52424 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:44:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3447 ms on hcdnc318 (1/2)%%0001017/07/17 22:44:06 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3451 ms on hcdnc327 (2/2)%%0001017/07/17 22:44:06 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:06 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.452 s%%0001017/07/17 22:44:06 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.465568 s%%0001017/07/17 22:44:06 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:44:06 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:44:06 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:44:06 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:44:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:44:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:44:06 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:44:06 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:44:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:44:06 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:44:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:44:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:38085 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:44:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:44:06 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:44:06 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc318, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:06 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc327, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc327:52424 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:44:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc318:34123 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3444 ms on hcdnc327 (1/2)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3464 ms on hcdnc318 (2/2)%%0001017/07/17 22:44:09 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:09 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.464 s%%0001017/07/17 22:44:09 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:44:09 INFO DAGScheduler: running: Set()%%0001017/07/17 22:44:09 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:44:09 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:44:09 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(1615) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1615.0 B, free 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:38085 (size: 1615.0 B, free: 530.2 MB)%%0001017/07/17 22:44:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:44:09 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc318, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc327, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc318:34123 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc327:52424 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc318:54325%%0001017/07/17 22:44:09 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 165 bytes%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc327:42987%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc318, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc327, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 45 ms on hcdnc318 (1/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 45 ms on hcdnc327 (2/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc318, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 14 ms on hcdnc318 (3/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc327, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc327 (4/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc318, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc318 (5/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc327, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 19 ms on hcdnc327 (6/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc318, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc318 (7/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc327, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc327 (8/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc318 (9/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc327 (10/10)%%0001017/07/17 22:44:09 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.103 s%%0001017/07/17 22:44:09 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:09 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.586812 s%%0001017/07/17 22:44:09 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:44:09 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:44:09 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:44:09 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:44:09 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:44:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:44:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(14344) called with curMem=288482, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.0 KB, free 530.0 MB)%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(7912) called with curMem=302826, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:38085 (size: 7.7 KB, free: 530.2 MB)%%0001017/07/17 22:44:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:44:09 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc318, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc327, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc327:52424 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc318:34123 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc318:54325%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc327:42987%%0001017/07/17 22:44:27 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 17415 ms on hcdnc327 (1/2)%%0001017/07/17 22:44:27 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 17917 ms on hcdnc318 (2/2)%%0001017/07/17 22:44:27 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:27 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 17.918 s%%0001017/07/17 22:44:27 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:44:27 INFO DAGScheduler: running: Set()%%0001017/07/17 22:44:27 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:44:27 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:44:27 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:44:27 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:44:27 INFO MemoryStore: ensureFreeSpace(17736) called with curMem=310738, maxMem=556038881%%0001017/07/17 22:44:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.3 KB, free 530.0 MB)%%0001017/07/17 22:44:27 INFO MemoryStore: ensureFreeSpace(9458) called with curMem=328474, maxMem=556038881%%0001017/07/17 22:44:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 530.0 MB)%%0001017/07/17 22:44:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:38085 (size: 9.2 KB, free: 530.2 MB)%%0001017/07/17 22:44:27 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:27 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:44:27 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:44:27 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc327, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:27 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc318, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc318:34123 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:44:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc327:52424 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:44:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc318:54325%%0001017/07/17 22:44:27 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 160 bytes%%0001017/07/17 22:44:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc327:42987%%0001017/07/17 22:44:28 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc318, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:28 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 806 ms on hcdnc318 (1/10)%%0001017/07/17 22:44:28 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc327, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:28 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 913 ms on hcdnc327 (2/10)%%0001017/07/17 22:44:28 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:44:29 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc318, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:29 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 692 ms on hcdnc318 (3/10)%%0001017/07/17 22:44:29 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc327, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:29 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 863 ms on hcdnc327 (4/10)%%0001017/07/17 22:44:29 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:44:29 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc318, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:29 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 598 ms on hcdnc318 (5/10)%%0001017/07/17 22:44:30 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc327, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:30 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 676 ms on hcdnc327 (6/10)%%0001017/07/17 22:44:30 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc318, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:30 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 627 ms on hcdnc318 (7/10)%%0001017/07/17 22:44:30 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc327, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:30 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 680 ms on hcdnc327 (8/10)%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://driverPropsFetcher@hcdnc230:39617]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:39617] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:39617%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 753 ms on hcdnc318 (9/10)%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc230:43571/user/Executor#820444799]) with ID 3%%0001017/07/17 22:44:31 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:44:31 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc230:41028 with 530.0 MB RAM, BlockManagerId(3, hcdnc230, 41028)%%0001017/07/17 22:44:31 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 600 ms on hcdnc327 (10/10)%%0001017/07/17 22:44:31 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:31 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.731 s%%0001017/07/17 22:44:31 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 21.704609 s%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:44:31 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:44:31 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://sparkExecutor@hcdnc230:43571]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc230:43571] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc230:43571%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://sparkExecutor@hcdnc327:42987]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc327:42987] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc327:42987%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://sparkExecutor@hcdnc318:54325]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc318:54325] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc318:54325%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:44:31 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:44:31 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:44:31 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:44:31 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:44:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:44:31 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:44:31 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:44:31 INFO Remoting: Remoting shut down%%0001017/07/17 22:44:31 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:44:32 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:44:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282/pyspark-17e8b355-0669-425d-a2ab-59ad01481841%%0001017/07/17 22:44:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="1">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:44:47 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:44:47 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:44:47 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:44:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:44:48 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:44:48 INFO Remoting: Starting remoting%%0001017/07/17 22:44:48 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:44514]%%0001017/07/17 22:44:48 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:44514]%%0001017/07/17 22:44:48 INFO Utils: Successfully started service 'sparkDriver' on port 44514.%%0001017/07/17 22:44:48 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:44:48 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:44:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd2530ac-8db0-4528-9bee-12978641c60f%%0001017/07/17 22:44:48 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:44:48 INFO HttpFileServer: HTTP File server directory is /tmp/spark-855cc671-eae5-42a4-9ab3-70a7ed3fc949/httpd-8570321d-39e7-42c0-b900-b83b9cb78206%%0001017/07/17 22:44:48 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:44:49 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:44:49 INFO AbstractConnector: Started SocketConnector@0.0.0.0:41916%%0001017/07/17 22:44:49 INFO Utils: Successfully started service 'HTTP file server' on port 41916.%%0001017/07/17 22:44:49 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:44:49 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:44:49 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:44:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:44:49 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:44:49 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:44:49 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:44:50 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:44:50 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:44:50 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:44:50 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:44:50 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:44:50 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:44:50 INFO Client: Uploading resource file:/tmp/spark-855cc671-eae5-42a4-9ab3-70a7ed3fc949/__spark_conf__4539740636631601780.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22072/__spark_conf__4539740636631601780.zip%%0001017/07/17 22:44:51 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:44:51 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:44:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:44:51 INFO Client: Submitting application 22072 to ResourceManager%%0001017/07/17 22:44:51 INFO YarnClientImpl: Submitted application application_1491786134915_22072%%0001017/07/17 22:44:52 INFO Client: Application report for application_1491786134915_22072 (state: ACCEPTED)%%0001017/07/17 22:44:52 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302691248%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22072/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:44:53 INFO Client: Application report for application_1491786134915_22072 (state: ACCEPTED)%%0001017/07/17 22:44:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:48447/user/YarnAM#1468649141])%%0001017/07/17 22:44:54 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22072,http://hcnnc117:8088/proxy/application_1491786134915_22072), /proxy/application_1491786134915_22072%%0001017/07/17 22:44:54 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:44:54 INFO Client: Application report for application_1491786134915_22072 (state: ACCEPTED)%%0001017/07/17 22:44:55 INFO Client: Application report for application_1491786134915_22072 (state: RUNNING)%%0001017/07/17 22:44:55 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302691248%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22072/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:44:55 INFO YarnClientSchedulerBackend: Application application_1491786134915_22072 has started running.%%0001017/07/17 22:44:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42963.%%0001017/07/17 22:44:55 INFO NettyBlockTransferService: Server created on 42963%%0001017/07/17 22:44:55 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:44:55 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:44:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:42963 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 42963)%%0001017/07/17 22:44:55 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:44:55 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22072%%0001017/07/17 22:44:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:42963 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:44:56 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:44:56 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.15:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.40:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.35:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.34:50010%%0001017/07/17 22:44:56 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:44:56 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:44:56 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:44:56 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:44:56 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:44:56 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:42963 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:44:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:44:56 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:44:57 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:44:58 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:45:00 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://driverPropsFetcher@hcdnc226:58305]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc226:58305] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc226:58305%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:45:00 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc226:59302/user/Executor#-224899339]) with ID 1%%0001017/07/17 22:45:00 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:45:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc226, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:45:00 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc226:48101 with 530.0 MB RAM, BlockManagerId(1, hcdnc226, 48101)%%0001017/07/17 22:45:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc226:48101 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:01 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://driverPropsFetcher@hcdnc225:35484]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc225:35484] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc225:35484%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:45:01 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc225:47955/user/Executor#335037470]) with ID 2%%0001017/07/17 22:45:01 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:45:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc226:48101 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:45:01 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc225:49691 with 530.0 MB RAM, BlockManagerId(2, hcdnc225, 49691)%%0001017/07/17 22:45:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc225, partition 1,ANY, 2147 bytes)%%0001017/07/17 22:45:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc225:49691 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc225:49691 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:45:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 16069 ms on hcdnc226 (1/2)%%0001017/07/17 22:45:18 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14453 ms on hcdnc225 (2/2)%%0001017/07/17 22:45:18 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:18 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 22.078 s%%0001017/07/17 22:45:18 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 22.248535 s%%0001017/07/17 22:45:18 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:45:18 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:45:18 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:45:18 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:45:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:45:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:45:18 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:45:18 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:45:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:45:18 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:45:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:45:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:42963 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:45:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:45:18 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:45:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc225, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:45:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc225:49691 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:45:20 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:45:22 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc226, partition 1,ANY, 2136 bytes)%%0001017/07/17 22:45:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc226:48101 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:45:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 20303 ms on hcdnc225 (1/2)%%0001017/07/17 22:45:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 21051 ms on hcdnc226 (2/2)%%0001017/07/17 22:45:43 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:43 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.525 s%%0001017/07/17 22:45:43 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:45:43 INFO DAGScheduler: running: Set()%%0001017/07/17 22:45:43 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:45:43 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:45:43 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:45:43 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:45:43 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:45:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:45:43 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:45:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:42963 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:45:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:45:43 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:45:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc225, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc226, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc226:48101 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc225:49691 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc226:59302%%0001017/07/17 22:45:43 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:45:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc225:47955%%0001017/07/17 22:45:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1651 ms on hcdnc225 (1/2)%%0001017/07/17 22:45:45 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1689 ms on hcdnc226 (2/2)%%0001017/07/17 22:45:45 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:45 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.692 s%%0001017/07/17 22:45:45 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 26.251993 s%%0001017/07/17 22:45:45 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:45:45 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:45:45 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:45:45 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:45:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:45:45 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:45:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:45:45 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:45:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:45:45 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:45:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:45:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:42963 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:45:45 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:45:45 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:45:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc226, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:45 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc225, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc226:48101 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:45:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc225:49691 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:45:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3401 ms on hcdnc226 (1/2)%%0001017/07/17 22:45:48 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3434 ms on hcdnc225 (2/2)%%0001017/07/17 22:45:48 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.435 s%%0001017/07/17 22:45:48 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:48 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.447850 s%%0001017/07/17 22:45:48 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:45:48 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:45:48 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:45:48 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:45:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:45:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:45:48 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:45:48 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:45:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:45:48 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:45:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:42963 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:45:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:45:48 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:45:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc226, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:48 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc225, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc226:48101 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc225:49691 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3394 ms on hcdnc226 (1/2)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3445 ms on hcdnc225 (2/2)%%0001017/07/17 22:45:52 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:52 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.446 s%%0001017/07/17 22:45:52 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:45:52 INFO DAGScheduler: running: Set()%%0001017/07/17 22:45:52 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:45:52 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:45:52 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(1615) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1615.0 B, free 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:42963 (size: 1615.0 B, free: 530.2 MB)%%0001017/07/17 22:45:52 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:45:52 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc225, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc226, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc225:49691 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc226:48101 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc225:47955%%0001017/07/17 22:45:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 164 bytes%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc226:59302%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc225, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc226, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 45 ms on hcdnc225 (1/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 46 ms on hcdnc226 (2/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc225, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 16 ms on hcdnc225 (3/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc226, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 16 ms on hcdnc226 (4/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc225, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 18 ms on hcdnc225 (5/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc226, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 20 ms on hcdnc226 (6/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc225, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 16 ms on hcdnc225 (7/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc226, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 16 ms on hcdnc226 (8/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc225 (9/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc226 (10/10)%%0001017/07/17 22:45:52 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.104 s%%0001017/07/17 22:45:52 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:52 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.596025 s%%0001017/07/17 22:45:52 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:45:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:45:52 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:45:52 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:45:52 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:45:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:45:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(14344) called with curMem=288482, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.0 KB, free 530.0 MB)%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(7913) called with curMem=302826, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:42963 (size: 7.7 KB, free: 530.2 MB)%%0001017/07/17 22:45:52 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:45:52 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc225, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc226, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc225:49691 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc226:48101 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc226:59302%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc225:47955%%0001017/07/17 22:46:11 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 19156 ms on hcdnc225 (1/2)%%0001017/07/17 22:46:12 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 20130 ms on hcdnc226 (2/2)%%0001017/07/17 22:46:12 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:46:12 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 20.132 s%%0001017/07/17 22:46:12 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:46:12 INFO DAGScheduler: running: Set()%%0001017/07/17 22:46:12 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:46:12 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:46:12 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:46:12 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:46:12 INFO MemoryStore: ensureFreeSpace(17736) called with curMem=310739, maxMem=556038881%%0001017/07/17 22:46:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.3 KB, free 530.0 MB)%%0001017/07/17 22:46:12 INFO MemoryStore: ensureFreeSpace(9458) called with curMem=328475, maxMem=556038881%%0001017/07/17 22:46:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 530.0 MB)%%0001017/07/17 22:46:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:42963 (size: 9.2 KB, free: 530.2 MB)%%0001017/07/17 22:46:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:46:12 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:46:12 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:46:12 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc226, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:12 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc225, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc226:48101 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:46:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc225:49691 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:46:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc225:47955%%0001017/07/17 22:46:12 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 159 bytes%%0001017/07/17 22:46:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc226:59302%%0001017/07/17 22:46:13 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc225, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:13 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 681 ms on hcdnc225 (1/10)%%0001017/07/17 22:46:13 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc226, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:13 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 775 ms on hcdnc226 (2/10)%%0001017/07/17 22:46:13 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:46:13 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc225, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:13 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 659 ms on hcdnc225 (3/10)%%0001017/07/17 22:46:14 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc226, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:14 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 704 ms on hcdnc226 (4/10)%%0001017/07/17 22:46:14 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc225, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:14 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 611 ms on hcdnc225 (5/10)%%0001017/07/17 22:46:14 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:46:14 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc226, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:14 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 605 ms on hcdnc226 (6/10)%%0001017/07/17 22:46:15 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc225, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 618 ms on hcdnc225 (7/10)%%0001017/07/17 22:46:15 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc226, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 629 ms on hcdnc226 (8/10)%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 687 ms on hcdnc225 (9/10)%%0001017/07/17 22:46:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://driverPropsFetcher@hcdnc322:41546]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc322:41546] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc322:41546%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 633 ms on hcdnc226 (10/10)%%0001017/07/17 22:46:15 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:46:15 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.342 s%%0001017/07/17 22:46:15 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 23.501468 s%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:46:16 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:46:16 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:46:16 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://sparkExecutor@hcdnc226:59302]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc226:59302] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc226:59302%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:16 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://sparkExecutor@hcdnc225:47955]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc225:47955] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc225:47955%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:46:16 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:46:16 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:46:16 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:46:16 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:46:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:46:16 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:46:16 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:46:16 INFO Remoting: Remoting shut down%%0001017/07/17 22:46:16 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:46:17 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:46:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-855cc671-eae5-42a4-9ab3-70a7ed3fc949%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="2">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:46:32 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:46:32 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:46:32 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:46:33 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:46:33 INFO Remoting: Starting remoting%%0001017/07/17 22:46:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:39426]%%0001017/07/17 22:46:33 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:39426]%%0001017/07/17 22:46:33 INFO Utils: Successfully started service 'sparkDriver' on port 39426.%%0001017/07/17 22:46:33 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:46:33 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:46:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0620ce98-5b62-4e9e-b5a6-900b4b0d4832%%0001017/07/17 22:46:33 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:46:34 INFO HttpFileServer: HTTP File server directory is /tmp/spark-77484cdc-4537-471a-95e0-96364a408f51/httpd-3e63628d-0c51-4c69-b91b-117e5139bb30%%0001017/07/17 22:46:34 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:46:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:46:34 INFO AbstractConnector: Started SocketConnector@0.0.0.0:39062%%0001017/07/17 22:46:34 INFO Utils: Successfully started service 'HTTP file server' on port 39062.%%0001017/07/17 22:46:34 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:46:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:46:34 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:46:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:46:34 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:46:34 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:46:34 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:46:34 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:46:34 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:46:34 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:46:34 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:46:34 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:46:34 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:46:35 INFO Client: Uploading resource file:/tmp/spark-77484cdc-4537-471a-95e0-96364a408f51/__spark_conf__2654259951614239138.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22073/__spark_conf__2654259951614239138.zip%%0001017/07/17 22:46:36 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:46:36 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:46:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:46:36 INFO Client: Submitting application 22073 to ResourceManager%%0001017/07/17 22:46:36 INFO YarnClientImpl: Submitted application application_1491786134915_22073%%0001017/07/17 22:46:37 INFO Client: Application report for application_1491786134915_22073 (state: ACCEPTED)%%0001017/07/17 22:46:37 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302796136%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22073/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:46:38 INFO Client: Application report for application_1491786134915_22073 (state: ACCEPTED)%%0001017/07/17 22:46:39 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:59404/user/YarnAM#-1588046408])%%0001017/07/17 22:46:39 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22073,http://hcnnc117:8088/proxy/application_1491786134915_22073), /proxy/application_1491786134915_22073%%0001017/07/17 22:46:39 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:46:39 INFO Client: Application report for application_1491786134915_22073 (state: RUNNING)%%0001017/07/17 22:46:39 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302796136%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22073/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:46:39 INFO YarnClientSchedulerBackend: Application application_1491786134915_22073 has started running.%%0001017/07/17 22:46:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35694.%%0001017/07/17 22:46:39 INFO NettyBlockTransferService: Server created on 35694%%0001017/07/17 22:46:39 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:46:39 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:46:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:35694 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 35694)%%0001017/07/17 22:46:39 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:46:39 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22073%%0001017/07/17 22:46:39 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:35694 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:46:40 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:46:40 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.6:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.19:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.25:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.26:50010%%0001017/07/17 22:46:40 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:46:40 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:46:40 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:46:40 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:46:40 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:46:40 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:35694 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:46:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:46:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:46:40 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:46:41 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:46:42 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:46:43 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://driverPropsFetcher@hcdnc332:54198]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc332:54198] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc332:54198%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:44 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc332:34164/user/Executor#1577283137]) with ID 1%%0001017/07/17 22:46:44 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:46:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc332, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:46:44 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc332:49250 with 530.0 MB RAM, BlockManagerId(1, hcdnc332, 49250)%%0001017/07/17 22:46:44 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://driverPropsFetcher@hcdnc230:56790]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:56790] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:56790%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc332:49250 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:46:44 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc230:42966/user/Executor#-717777621]) with ID 2%%0001017/07/17 22:46:44 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:46:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc230, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:46:45 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc230:49662 with 530.0 MB RAM, BlockManagerId(2, hcdnc230, 49662)%%0001017/07/17 22:46:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc332:49250 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:46:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc230:49662 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:46:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc230:49662 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:46:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 14856 ms on hcdnc332 (1/2)%%0001017/07/17 22:46:59 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14813 ms on hcdnc230 (2/2)%%0001017/07/17 22:46:59 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.348 s%%0001017/07/17 22:46:59 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:46:59 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 19.515972 s%%0001017/07/17 22:46:59 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:46:59 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:46:59 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:46:59 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:46:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:46:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:46:59 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:46:59 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:46:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:46:59 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:46:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:46:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:35694 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:46:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:46:59 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:46:59 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:46:59 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, hcdnc230, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:46:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, hcdnc332, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:46:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc332:49250 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:46:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc230:49662 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:47:20 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 20281 ms on hcdnc230 (1/2)%%0001017/07/17 22:47:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 20425 ms on hcdnc332 (2/2)%%0001017/07/17 22:47:20 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:20 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 20.427 s%%0001017/07/17 22:47:20 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:47:20 INFO DAGScheduler: running: Set()%%0001017/07/17 22:47:20 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:47:20 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:47:20 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:47:20 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:47:20 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:47:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:47:20 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:47:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:47:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:35694 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:47:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:47:20 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:47:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc230, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:20 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc332, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc230:49662 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:47:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc332:49250 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:47:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc230:42966%%0001017/07/17 22:47:20 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:47:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc332:34164%%0001017/07/17 22:47:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1674 ms on hcdnc230 (1/2)%%0001017/07/17 22:47:22 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1705 ms on hcdnc332 (2/2)%%0001017/07/17 22:47:22 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:22 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.708 s%%0001017/07/17 22:47:22 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 22.167477 s%%0001017/07/17 22:47:22 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:47:22 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:47:22 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:47:22 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:47:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:47:22 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:47:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:47:22 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:47:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:47:22 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:47:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:47:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:35694 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:47:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:47:22 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:47:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc230, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:22 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc332, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc332:49250 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:47:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc230:49662 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:47:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3447 ms on hcdnc230 (1/2)%%0001017/07/17 22:47:25 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3561 ms on hcdnc332 (2/2)%%0001017/07/17 22:47:25 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.562 s%%0001017/07/17 22:47:25 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:25 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.575781 s%%0001017/07/17 22:47:25 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:47:25 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:47:25 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:47:25 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:47:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:47:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:47:25 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:47:25 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:47:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:47:25 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:47:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:47:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:35694 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:47:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:25 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:47:25 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:47:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc332, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:25 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc230, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc332:49250 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:47:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc230:49662 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3406 ms on hcdnc332 (1/2)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3621 ms on hcdnc230 (2/2)%%0001017/07/17 22:47:29 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:29 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.622 s%%0001017/07/17 22:47:29 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:47:29 INFO DAGScheduler: running: Set()%%0001017/07/17 22:47:29 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:47:29 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:47:29 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(1615) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1615.0 B, free 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:35694 (size: 1615.0 B, free: 530.2 MB)%%0001017/07/17 22:47:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:47:29 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc332, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc230, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc230:49662 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc332:49250 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc230:42966%%0001017/07/17 22:47:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 166 bytes%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc332:34164%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc230, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 45 ms on hcdnc230 (1/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc332, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 50 ms on hcdnc332 (2/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc230, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 14 ms on hcdnc230 (3/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc332, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 18 ms on hcdnc332 (4/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc230, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc230 (5/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc332, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 17 ms on hcdnc332 (6/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc230, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 14 ms on hcdnc230 (7/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc332, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 14 ms on hcdnc332 (8/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc230 (9/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 10 ms on hcdnc332 (10/10)%%0001017/07/17 22:47:29 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.103 s%%0001017/07/17 22:47:29 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:29 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.744018 s%%0001017/07/17 22:47:29 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:47:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:47:29 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:47:29 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:47:29 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:47:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:47:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(14344) called with curMem=288482, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.0 KB, free 530.0 MB)%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(7912) called with curMem=302826, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:35694 (size: 7.7 KB, free: 530.2 MB)%%0001017/07/17 22:47:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:47:29 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc332, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc230, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc332:49250 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc230:49662 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc332:34164%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc230:42966%%0001017/07/17 22:47:48 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 18763 ms on hcdnc332 (1/2)%%0001017/07/17 22:47:50 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 20507 ms on hcdnc230 (2/2)%%0001017/07/17 22:47:50 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:50 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 20.509 s%%0001017/07/17 22:47:50 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:47:50 INFO DAGScheduler: running: Set()%%0001017/07/17 22:47:50 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:47:50 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:47:50 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:47:50 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:47:50 INFO MemoryStore: ensureFreeSpace(17736) called with curMem=310738, maxMem=556038881%%0001017/07/17 22:47:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.3 KB, free 530.0 MB)%%0001017/07/17 22:47:50 INFO MemoryStore: ensureFreeSpace(9458) called with curMem=328474, maxMem=556038881%%0001017/07/17 22:47:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 530.0 MB)%%0001017/07/17 22:47:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:35694 (size: 9.2 KB, free: 530.2 MB)%%0001017/07/17 22:47:50 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:50 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:47:50 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:47:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc230, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:50 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc332, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc230:49662 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:47:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc332:49250 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:47:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc230:42966%%0001017/07/17 22:47:50 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 161 bytes%%0001017/07/17 22:47:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc332:34164%%0001017/07/17 22:47:50 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc230, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:50 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 818 ms on hcdnc230 (1/10)%%0001017/07/17 22:47:51 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:47:51 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc332, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:51 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 1037 ms on hcdnc332 (2/10)%%0001017/07/17 22:47:51 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc230, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:51 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 759 ms on hcdnc230 (3/10)%%0001017/07/17 22:47:51 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc332, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:51 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 734 ms on hcdnc332 (4/10)%%0001017/07/17 22:47:52 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:47:52 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc230, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:52 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 594 ms on hcdnc230 (5/10)%%0001017/07/17 22:47:52 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc332, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:52 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 650 ms on hcdnc332 (6/10)%%0001017/07/17 22:47:52 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc230, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:52 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 634 ms on hcdnc230 (7/10)%%0001017/07/17 22:47:53 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc332, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:53 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 590 ms on hcdnc332 (8/10)%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://driverPropsFetcher@hcdnc838:57707]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc838:57707] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc838:57707%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 706 ms on hcdnc230 (9/10)%%0001017/07/17 22:47:53 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 612 ms on hcdnc332 (10/10)%%0001017/07/17 22:47:53 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:53 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.620 s%%0001017/07/17 22:47:53 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 24.155877 s%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc838:55357/user/Executor#940631458]) with ID 3%%0001017/07/17 22:47:53 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:47:53 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc838:41966 with 530.0 MB RAM, BlockManagerId(3, hcdnc838, 41966)%%0001017/07/17 22:47:53 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:47:53 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://sparkExecutor@hcdnc230:42966]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc230:42966] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc230:42966%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://sparkExecutor@hcdnc332:34164]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc332:34164] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc332:34164%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://sparkExecutor@hcdnc838:55357]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc838:55357] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc838:55357%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:47:53 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:47:53 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:47:53 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:47:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:47:53 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:47:53 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:47:53 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:47:54 INFO Remoting: Remoting shut down%%0001017/07/17 22:47:54 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:47:54 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:47:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-77484cdc-4537-471a-95e0-96364a408f51/pyspark-1825413e-60a9-4db9-9397-984396df8b31%%0001017/07/17 22:47:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-77484cdc-4537-471a-95e0-96364a408f51%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="3">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:48:08 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:48:09 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:48:09 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:48:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:48:10 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:48:10 INFO Remoting: Starting remoting%%0001017/07/17 22:48:10 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:46080]%%0001017/07/17 22:48:10 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:46080]%%0001017/07/17 22:48:10 INFO Utils: Successfully started service 'sparkDriver' on port 46080.%%0001017/07/17 22:48:10 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:48:10 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:48:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9c3176ac-3718-4e0c-942d-61475e852842%%0001017/07/17 22:48:10 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:48:10 INFO HttpFileServer: HTTP File server directory is /tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272/httpd-0ffd8150-fdd1-4cb5-a506-e0cd34ca09dc%%0001017/07/17 22:48:10 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:48:10 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:48:10 INFO AbstractConnector: Started SocketConnector@0.0.0.0:36943%%0001017/07/17 22:48:10 INFO Utils: Successfully started service 'HTTP file server' on port 36943.%%0001017/07/17 22:48:10 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:48:10 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:48:10 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:48:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:48:10 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:48:10 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:48:11 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:48:11 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:48:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:48:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:48:11 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:48:11 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:48:11 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:48:11 INFO Client: Uploading resource file:/tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272/__spark_conf__728497682939853107.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22074/__spark_conf__728497682939853107.zip%%0001017/07/17 22:48:12 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:48:12 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:48:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:48:12 INFO Client: Submitting application 22074 to ResourceManager%%0001017/07/17 22:48:12 INFO YarnClientImpl: Submitted application application_1491786134915_22074%%0001017/07/17 22:48:13 INFO Client: Application report for application_1491786134915_22074 (state: ACCEPTED)%%0001017/07/17 22:48:13 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302892050%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22074/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:48:14 INFO Client: Application report for application_1491786134915_22074 (state: ACCEPTED)%%0001017/07/17 22:48:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:43563/user/YarnAM#-508022274])%%0001017/07/17 22:48:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22074,http://hcnnc117:8088/proxy/application_1491786134915_22074), /proxy/application_1491786134915_22074%%0001017/07/17 22:48:15 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:48:15 INFO Client: Application report for application_1491786134915_22074 (state: RUNNING)%%0001017/07/17 22:48:15 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302892050%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22074/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:48:15 INFO YarnClientSchedulerBackend: Application application_1491786134915_22074 has started running.%%0001017/07/17 22:48:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40511.%%0001017/07/17 22:48:15 INFO NettyBlockTransferService: Server created on 40511%%0001017/07/17 22:48:15 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:48:15 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:48:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:40511 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 40511)%%0001017/07/17 22:48:15 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:48:16 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22074%%0001017/07/17 22:48:16 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:40511 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:48:16 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:48:16 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.17:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.28:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.4:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.19:50010%%0001017/07/17 22:48:16 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:48:16 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:48:16 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:48:16 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:48:16 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:48:16 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:40511 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:48:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:48:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:48:17 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:48:18 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:48:19 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:48:20 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://driverPropsFetcher@hcdnc404:40878]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc404:40878] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc404:40878%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:48:20 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc404:54207/user/Executor#1152446352]) with ID 1%%0001017/07/17 22:48:21 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:48:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc404, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:48:21 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc404:52186 with 530.0 MB RAM, BlockManagerId(1, hcdnc404, 52186)%%0001017/07/17 22:48:21 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://driverPropsFetcher@hcdnc421:43477]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc421:43477] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc421:43477%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:48:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc404:52186 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:48:21 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc421:36227/user/Executor#329786119]) with ID 2%%0001017/07/17 22:48:21 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:48:21 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc421:36055 with 530.0 MB RAM, BlockManagerId(2, hcdnc421, 36055)%%0001017/07/17 22:48:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc404:52186 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:48:24 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc421, partition 1,ANY, 2147 bytes)%%0001017/07/17 22:48:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc421:36055 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:48:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc421:36055 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:48:38 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14683 ms on hcdnc421 (1/2)%%0001017/07/17 22:48:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 20286 ms on hcdnc404 (2/2)%%0001017/07/17 22:48:41 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 24.183 s%%0001017/07/17 22:48:41 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:48:41 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 24.351231 s%%0001017/07/17 22:48:41 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:48:41 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:48:41 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:48:41 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:48:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:48:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:48:41 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:48:41 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:48:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:48:41 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:48:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:48:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:40511 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:48:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:48:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:48:41 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:48:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc404, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:48:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc404:52186 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:48:42 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:48:45 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc421, partition 1,ANY, 2136 bytes)%%0001017/07/17 22:48:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc421:36055 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:49:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21077 ms on hcdnc404 (1/2)%%0001017/07/17 22:49:05 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20190 ms on hcdnc421 (2/2)%%0001017/07/17 22:49:05 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:05 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.080 s%%0001017/07/17 22:49:05 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:49:05 INFO DAGScheduler: running: Set()%%0001017/07/17 22:49:05 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:49:05 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:49:05 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:49:05 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:49:05 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:49:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:49:05 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:49:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:49:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:40511 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:49:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:49:05 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:49:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc404, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:05 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc421, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc421:36055 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:49:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc404:52186 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:49:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc421:36227%%0001017/07/17 22:49:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:49:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc404:54207%%0001017/07/17 22:49:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1619 ms on hcdnc404 (1/2)%%0001017/07/17 22:49:07 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1742 ms on hcdnc421 (2/2)%%0001017/07/17 22:49:07 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:07 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.743 s%%0001017/07/17 22:49:07 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 25.856648 s%%0001017/07/17 22:49:07 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:49:07 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:49:07 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:49:07 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:49:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:49:07 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:49:07 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:49:07 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:49:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:49:07 INFO MemoryStore: ensureFreeSpace(3903) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:49:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:49:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:40511 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:49:07 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:49:07 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:49:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc421, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:07 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc404, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc421:36055 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:49:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc404:52186 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:49:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3339 ms on hcdnc421 (1/2)%%0001017/07/17 22:49:10 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3472 ms on hcdnc404 (2/2)%%0001017/07/17 22:49:10 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:10 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.473 s%%0001017/07/17 22:49:10 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.486533 s%%0001017/07/17 22:49:10 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:49:10 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:49:10 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:49:10 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:49:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:49:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:49:10 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:49:10 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272759, maxMem=556038881%%0001017/07/17 22:49:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:49:10 INFO MemoryStore: ensureFreeSpace(4151) called with curMem=279935, maxMem=556038881%%0001017/07/17 22:49:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:49:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:40511 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:49:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:49:10 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:49:10 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc421, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:10 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc404, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc404:52186 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:49:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc421:36055 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3308 ms on hcdnc421 (1/2)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3489 ms on hcdnc404 (2/2)%%0001017/07/17 22:49:14 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:14 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.490 s%%0001017/07/17 22:49:14 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:49:14 INFO DAGScheduler: running: Set()%%0001017/07/17 22:49:14 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:49:14 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:49:14 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284086, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286870, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:40511 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:49:14 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:49:14 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc421, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc404, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc404:52186 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc421:36055 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc404:54207%%0001017/07/17 22:49:14 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 165 bytes%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc421:36227%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc404, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 42 ms on hcdnc404 (1/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc421, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 50 ms on hcdnc421 (2/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc404, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 15 ms on hcdnc404 (3/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc421, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 18 ms on hcdnc421 (4/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc404, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 18 ms on hcdnc404 (5/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc421, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 19 ms on hcdnc421 (6/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc404, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc404 (7/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc421, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc421 (8/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc404 (9/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc421 (10/10)%%0001017/07/17 22:49:14 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.110 s%%0001017/07/17 22:49:14 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:14 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.639900 s%%0001017/07/17 22:49:14 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:49:14 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:49:14 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:49:14 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:49:14 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:49:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:49:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(13320) called with curMem=288483, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.0 KB, free 530.0 MB)%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(7335) called with curMem=301803, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:40511 (size: 7.2 KB, free: 530.2 MB)%%0001017/07/17 22:49:14 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:49:14 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc404, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc421, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc421:36055 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc404:52186 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc404:54207%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc421:36227%%0001017/07/17 22:49:31 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 17305 ms on hcdnc404 (1/2)%%0001017/07/17 22:49:33 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 18866 ms on hcdnc421 (2/2)%%0001017/07/17 22:49:33 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:33 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 18.868 s%%0001017/07/17 22:49:33 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:49:33 INFO DAGScheduler: running: Set()%%0001017/07/17 22:49:33 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:49:33 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:49:33 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:49:33 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:49:33 INFO MemoryStore: ensureFreeSpace(16480) called with curMem=309138, maxMem=556038881%%0001017/07/17 22:49:33 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.1 KB, free 530.0 MB)%%0001017/07/17 22:49:33 INFO MemoryStore: ensureFreeSpace(8726) called with curMem=325618, maxMem=556038881%%0001017/07/17 22:49:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:49:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:40511 (size: 8.5 KB, free: 530.2 MB)%%0001017/07/17 22:49:33 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:33 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:49:33 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:49:33 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc404, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:33 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc421, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc421:36055 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:49:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc404:52186 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:49:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc421:36227%%0001017/07/17 22:49:33 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 157 bytes%%0001017/07/17 22:49:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc404:54207%%0001017/07/17 22:49:34 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc421, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:34 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 980 ms on hcdnc421 (1/10)%%0001017/07/17 22:49:34 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc404, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:34 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 1026 ms on hcdnc404 (2/10)%%0001017/07/17 22:49:34 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:49:34 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc421, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:34 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 615 ms on hcdnc421 (3/10)%%0001017/07/17 22:49:35 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc404, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:35 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 770 ms on hcdnc404 (4/10)%%0001017/07/17 22:49:35 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:49:35 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc421, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:35 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 602 ms on hcdnc421 (5/10)%%0001017/07/17 22:49:35 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc404, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:35 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 639 ms on hcdnc404 (6/10)%%0001017/07/17 22:49:36 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc421, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:36 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 560 ms on hcdnc421 (7/10)%%0001017/07/17 22:49:36 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc404, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:36 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 704 ms on hcdnc404 (8/10)%%0001017/07/17 22:49:36 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://driverPropsFetcher@hcdnc412:55440]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc412:55440] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc412:55440%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:36 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 625 ms on hcdnc421 (9/10)%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc412:41395/user/Executor#-204020692]) with ID 3%%0001017/07/17 22:49:37 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:49:37 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 584 ms on hcdnc404 (10/10)%%0001017/07/17 22:49:37 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.719 s%%0001017/07/17 22:49:37 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:37 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 22.614461 s%%0001017/07/17 22:49:37 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc412:58622 with 530.0 MB RAM, BlockManagerId(3, hcdnc412, 58622)%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:49:37 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:49:37 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:49:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://sparkExecutor@hcdnc404:54207]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc404:54207] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc404:54207%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://sparkExecutor@hcdnc421:36227]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc421:36227] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc421:36227%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://sparkExecutor@hcdnc412:41395]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc412:41395] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc412:41395%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:49:37 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:49:37 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:49:37 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:49:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:49:37 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:49:37 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:49:37 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:49:37 INFO Remoting: Remoting shut down%%0001017/07/17 22:49:37 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:49:38 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272/pyspark-48d3e9e9-4600-40d1-b570-bcd32407f90f%%0001017/07/17 22:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="4">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:49:53 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:49:53 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:49:53 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:49:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:49:54 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:49:54 INFO Remoting: Starting remoting%%0001017/07/17 22:49:54 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:43851]%%0001017/07/17 22:49:54 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:43851]%%0001017/07/17 22:49:54 INFO Utils: Successfully started service 'sparkDriver' on port 43851.%%0001017/07/17 22:49:54 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:49:54 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:49:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7a9ab765-6cf4-4a67-8f1c-66903ebfa6c4%%0001017/07/17 22:49:54 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:49:54 INFO HttpFileServer: HTTP File server directory is /tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d/httpd-acf507e0-30ab-4959-a3bd-ccfbb28b26e8%%0001017/07/17 22:49:54 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:49:54 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:49:54 INFO AbstractConnector: Started SocketConnector@0.0.0.0:36646%%0001017/07/17 22:49:54 INFO Utils: Successfully started service 'HTTP file server' on port 36646.%%0001017/07/17 22:49:54 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:49:54 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:49:55 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:49:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:49:55 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:49:55 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:49:55 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:49:56 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:49:56 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:49:56 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:49:56 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:49:56 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:49:56 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:49:56 INFO Client: Uploading resource file:/tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d/__spark_conf__4486679632002301877.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22075/__spark_conf__4486679632002301877.zip%%0001017/07/17 22:49:56 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:49:56 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:49:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:49:56 INFO Client: Submitting application 22075 to ResourceManager%%0001017/07/17 22:49:57 INFO YarnClientImpl: Submitted application application_1491786134915_22075%%0001017/07/17 22:49:58 INFO Client: Application report for application_1491786134915_22075 (state: ACCEPTED)%%0001017/07/17 22:49:58 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302996865%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22075/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:49:59 INFO Client: Application report for application_1491786134915_22075 (state: ACCEPTED)%%0001017/07/17 22:50:00 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:48159/user/YarnAM#859327838])%%0001017/07/17 22:50:00 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22075,http://hcnnc117:8088/proxy/application_1491786134915_22075), /proxy/application_1491786134915_22075%%0001017/07/17 22:50:00 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:50:00 INFO Client: Application report for application_1491786134915_22075 (state: ACCEPTED)%%0001017/07/17 22:50:01 INFO Client: Application report for application_1491786134915_22075 (state: RUNNING)%%0001017/07/17 22:50:01 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302996865%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22075/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:50:01 INFO YarnClientSchedulerBackend: Application application_1491786134915_22075 has started running.%%0001017/07/17 22:50:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39985.%%0001017/07/17 22:50:01 INFO NettyBlockTransferService: Server created on 39985%%0001017/07/17 22:50:01 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:50:01 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:50:01 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:39985 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 39985)%%0001017/07/17 22:50:01 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:50:01 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22075%%0001017/07/17 22:50:01 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:50:01 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:50:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:50:01 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:50:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:50:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:39985 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:50:01 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:50:02 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.21:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.33:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.27:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.31:50010%%0001017/07/17 22:50:02 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:50:02 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:50:02 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:50:02 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:50:02 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:50:02 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:50:02 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:50:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:50:02 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:50:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:50:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:39985 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:50:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:50:02 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:50:03 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:50:04 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:50:05 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://driverPropsFetcher@hcdnc406:37566]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc406:37566] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc406:37566%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:50:06 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc406:44474/user/Executor#412856414]) with ID 1%%0001017/07/17 22:50:06 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:50:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0, hcdnc406, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:50:06 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc406:48356 with 530.0 MB RAM, BlockManagerId(1, hcdnc406, 48356)%%0001017/07/17 22:50:06 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://driverPropsFetcher@hcdnc823:49248]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:49248] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:49248%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:50:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc406:48356 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:06 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc823:48321/user/Executor#-1348166762]) with ID 2%%0001017/07/17 22:50:06 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:50:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1, hcdnc823, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:50:06 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc823:59036 with 530.0 MB RAM, BlockManagerId(2, hcdnc823, 59036)%%0001017/07/17 22:50:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc406:48356 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:50:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc823:59036 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc823:59036 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:50:21 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 14868 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 15142 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:21 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.785 s%%0001017/07/17 22:50:21 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:22 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 19.948811 s%%0001017/07/17 22:50:22 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:50:22 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:50:22 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:50:22 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:50:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:50:22 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:50:22 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:50:22 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:50:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:50:22 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:50:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:50:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:39985 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:50:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:22 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:50:22 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:50:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc823, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:50:22 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc406, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:50:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc823:59036 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc406:48356 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20313 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21019 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:43 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.021 s%%0001017/07/17 22:50:43 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:43 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:50:43 INFO DAGScheduler: running: Set()%%0001017/07/17 22:50:43 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:50:43 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:50:43 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:50:43 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:50:43 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:50:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:50:43 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:50:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:50:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:39985 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:50:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:50:43 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:50:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc406, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc823, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc823:59036 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc406:48356 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:48321%%0001017/07/17 22:50:43 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:50:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc406:44474%%0001017/07/17 22:50:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1605 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:44 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1689 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:44 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:44 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.692 s%%0001017/07/17 22:50:44 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 22.743526 s%%0001017/07/17 22:50:44 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:50:44 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:50:44 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:50:44 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:50:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:50:44 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:50:44 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:50:44 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:50:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:50:44 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:50:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:50:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:39985 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:50:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:50:44 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:50:44 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:44 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc406, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc406:48356 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:50:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc823:59036 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:50:48 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3395 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3439 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:48 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.439 s%%0001017/07/17 22:50:48 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:48 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.452480 s%%0001017/07/17 22:50:48 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:50:48 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:50:48 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:50:48 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:50:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:50:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:50:48 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:50:48 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:50:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:50:48 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:50:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:50:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:39985 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:50:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:50:48 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:50:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc406, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:48 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc406:48356 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:50:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc823:59036 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3381 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3587 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:51 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:51 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.588 s%%0001017/07/17 22:50:51 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:50:51 INFO DAGScheduler: running: Set()%%0001017/07/17 22:50:51 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:50:51 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:50:51 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:50:51 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:50:51 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:50:51 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:50:51 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:50:51 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:50:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:39985 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:50:51 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:51 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:50:51 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc406, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc823:59036 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:50:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc406:48356 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:50:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc823:48321%%0001017/07/17 22:50:51 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 166 bytes%%0001017/07/17 22:50:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc406:44474%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc823, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 44 ms on hcdnc823 (1/10)%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc406, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 48 ms on hcdnc406 (2/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc823, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 14 ms on hcdnc823 (3/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc406, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc406 (4/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc823, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 16 ms on hcdnc823 (5/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc406, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc823, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 21 ms on hcdnc406 (6/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc823 (7/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc406, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc406 (8/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc823 (9/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc406 (10/10)%%0001017/07/17 22:50:52 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.108 s%%0001017/07/17 22:50:52 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:52 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.743436 s%%0001017/07/17 22:50:52 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:50:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:50:52 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:50:52 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:50:52 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:50:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:50:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:50:52 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:50:52 INFO MemoryStore: ensureFreeSpace(13320) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:50:52 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.0 KB, free 530.0 MB)%%0001017/07/17 22:50:52 INFO MemoryStore: ensureFreeSpace(7329) called with curMem=301800, maxMem=556038881%%0001017/07/17 22:50:52 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 530.0 MB)%%0001017/07/17 22:50:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:39985 (size: 7.2 KB, free: 530.2 MB)%%0001017/07/17 22:50:52 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:50:52 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc406, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc406:48356 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc823:59036 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc406:44474%%0001017/07/17 22:50:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:48321%%0001017/07/17 22:51:08 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 16308 ms on hcdnc406 (1/2)%%0001017/07/17 22:51:11 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 19680 ms on hcdnc823 (2/2)%%0001017/07/17 22:51:11 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:51:11 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 19.682 s%%0001017/07/17 22:51:11 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:51:11 INFO DAGScheduler: running: Set()%%0001017/07/17 22:51:11 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:51:11 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:51:11 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:51:11 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:51:11 INFO MemoryStore: ensureFreeSpace(16480) called with curMem=309129, maxMem=556038881%%0001017/07/17 22:51:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.1 KB, free 530.0 MB)%%0001017/07/17 22:51:11 INFO MemoryStore: ensureFreeSpace(8730) called with curMem=325609, maxMem=556038881%%0001017/07/17 22:51:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:51:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:39985 (size: 8.5 KB, free: 530.2 MB)%%0001017/07/17 22:51:11 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:51:11 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:51:11 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:51:11 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:11 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc406, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc823:59036 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:51:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc406:48356 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:51:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc823:48321%%0001017/07/17 22:51:11 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 158 bytes%%0001017/07/17 22:51:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc406:44474%%0001017/07/17 22:51:12 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc406, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:12 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 798 ms on hcdnc406 (1/10)%%0001017/07/17 22:51:12 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc823, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:12 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 884 ms on hcdnc823 (2/10)%%0001017/07/17 22:51:12 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:51:13 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc406, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:13 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 796 ms on hcdnc406 (3/10)%%0001017/07/17 22:51:13 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc823, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:13 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 805 ms on hcdnc823 (4/10)%%0001017/07/17 22:51:13 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:51:13 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc406, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:13 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 558 ms on hcdnc406 (5/10)%%0001017/07/17 22:51:14 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc823, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:14 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 797 ms on hcdnc823 (6/10)%%0001017/07/17 22:51:14 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc406, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:14 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 504 ms on hcdnc406 (7/10)%%0001017/07/17 22:51:14 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc823, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:14 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 582 ms on hcdnc823 (8/10)%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://driverPropsFetcher@hcdnc217:58309]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc217:58309] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc217:58309%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 620 ms on hcdnc406 (9/10)%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc217:48862/user/Executor#896528954]) with ID 3%%0001017/07/17 22:51:15 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:51:15 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc217:60710 with 530.0 MB RAM, BlockManagerId(3, hcdnc217, 60710)%%0001017/07/17 22:51:15 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 568 ms on hcdnc823 (10/10)%%0001017/07/17 22:51:15 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:51:15 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.633 s%%0001017/07/17 22:51:15 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 23.341989 s%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:51:15 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:51:15 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://sparkExecutor@hcdnc217:48862]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc217:48862] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc217:48862%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://sparkExecutor@hcdnc823:48321]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc823:48321] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc823:48321%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://sparkExecutor@hcdnc406:44474]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc406:44474] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc406:44474%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:51:15 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:51:15 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:51:15 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:51:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:51:15 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:51:15 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:51:15 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:51:15 INFO Remoting: Remoting shut down%%0001017/07/17 22:51:15 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:51:16 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:51:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d/pyspark-fb8538d5-418f-4a8d-b902-f5dd1aec3172%%0001017/07/17 22:51:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="5">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:51:32 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:51:32 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:51:32 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:51:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:51:33 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:51:33 INFO Remoting: Starting remoting%%0001017/07/17 22:51:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:42112]%%0001017/07/17 22:51:33 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:42112]%%0001017/07/17 22:51:33 INFO Utils: Successfully started service 'sparkDriver' on port 42112.%%0001017/07/17 22:51:33 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:51:33 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:51:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c81417c4-353a-44af-ae45-99fdd06fdf0d%%0001017/07/17 22:51:33 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:51:34 INFO HttpFileServer: HTTP File server directory is /tmp/spark-76e3aa34-9463-4bf6-8a0a-0b38288ab660/httpd-8fda9927-cc88-4aa5-89a0-8794ea3d7938%%0001017/07/17 22:51:34 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:51:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:51:34 INFO AbstractConnector: Started SocketConnector@0.0.0.0:40037%%0001017/07/17 22:51:34 INFO Utils: Successfully started service 'HTTP file server' on port 40037.%%0001017/07/17 22:51:34 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:51:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:51:34 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:51:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:51:34 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:51:34 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:51:35 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:51:35 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:51:35 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:51:35 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:51:35 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:51:35 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:51:35 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:51:35 INFO Client: Uploading resource file:/tmp/spark-76e3aa34-9463-4bf6-8a0a-0b38288ab660/__spark_conf__4891555663197474770.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22076/__spark_conf__4891555663197474770.zip%%0001017/07/17 22:51:35 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:51:35 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:51:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:51:35 INFO Client: Submitting application 22076 to ResourceManager%%0001017/07/17 22:51:36 INFO YarnClientImpl: Submitted application application_1491786134915_22076%%0001017/07/17 22:51:37 INFO Client: Application report for application_1491786134915_22076 (state: ACCEPTED)%%0001017/07/17 22:51:37 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303096001%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22076/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:51:38 INFO Client: Application report for application_1491786134915_22076 (state: ACCEPTED)%%0001017/07/17 22:51:39 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:49556/user/YarnAM#-836501001])%%0001017/07/17 22:51:39 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22076,http://hcnnc117:8088/proxy/application_1491786134915_22076), /proxy/application_1491786134915_22076%%0001017/07/17 22:51:39 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:51:39 INFO Client: Application report for application_1491786134915_22076 (state: RUNNING)%%0001017/07/17 22:51:39 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303096001%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22076/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:51:39 INFO YarnClientSchedulerBackend: Application application_1491786134915_22076 has started running.%%0001017/07/17 22:51:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39013.%%0001017/07/17 22:51:39 INFO NettyBlockTransferService: Server created on 39013%%0001017/07/17 22:51:39 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:51:39 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:51:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:39013 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 39013)%%0001017/07/17 22:51:39 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:51:40 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22076%%0001017/07/17 22:51:40 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:39013 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:51:40 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:51:40 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.39:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.30:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.20:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.5:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.38:50010%%0001017/07/17 22:51:40 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:51:40 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:51:40 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:51:40 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:51:40 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:51:40 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:39013 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:51:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:51:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:51:40 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:51:41 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:51:42 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:51:44 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://driverPropsFetcher@hcdnc310:50563]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc310:50563] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc310:50563%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:44 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc310:57760/user/Executor#-171104123]) with ID 1%%0001017/07/17 22:51:44 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:51:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc310, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:51:44 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc310:52993 with 530.0 MB RAM, BlockManagerId(1, hcdnc310, 52993)%%0001017/07/17 22:51:44 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://driverPropsFetcher@hcdnc301:33858]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc301:33858] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc301:33858%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc310:52993 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:51:45 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc301:57727/user/Executor#-966426685]) with ID 2%%0001017/07/17 22:51:45 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:51:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc301, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:51:45 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc301:48467 with 530.0 MB RAM, BlockManagerId(2, hcdnc301, 48467)%%0001017/07/17 22:51:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc310:52993 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:51:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc301:48467 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:51:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc301:48467 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:51:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 14905 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 15533 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:00 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 20.007 s%%0001017/07/17 22:52:00 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:00 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 20.155176 s%%0001017/07/17 22:52:00 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:52:00 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:52:00 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:52:00 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:52:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:52:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:52:00 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:52:00 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:52:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:52:00 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:52:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:52:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:39013 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:52:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:00 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:52:00 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:52:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc310, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:52:00 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc301, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:52:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc301:48467 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc310:52993 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:21 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20907 ms on hcdnc301 (1/2)%%0001017/07/17 22:52:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21112 ms on hcdnc310 (2/2)%%0001017/07/17 22:52:22 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:22 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.113 s%%0001017/07/17 22:52:22 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:52:22 INFO DAGScheduler: running: Set()%%0001017/07/17 22:52:22 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:52:22 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:52:22 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:52:22 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:52:22 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:52:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:52:22 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:52:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:52:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:39013 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:52:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:52:22 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:52:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc310, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:22 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc301, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc310:52993 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:52:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc301:48467 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:52:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc310:57760%%0001017/07/17 22:52:22 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:52:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc301:57727%%0001017/07/17 22:52:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1682 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:23 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1779 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:23 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:23 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.780 s%%0001017/07/17 22:52:23 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 22.926765 s%%0001017/07/17 22:52:23 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:52:23 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:52:23 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:52:23 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:52:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:52:23 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:52:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:52:23 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:52:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:52:23 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:52:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:52:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:39013 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:52:23 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:52:23 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:52:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc310, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:23 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc301, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc301:48467 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:52:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc310:52993 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:52:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3485 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:27 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3572 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:27 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:27 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.574 s%%0001017/07/17 22:52:27 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.587185 s%%0001017/07/17 22:52:27 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:52:27 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:52:27 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:52:27 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:52:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:52:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:52:27 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:52:27 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:52:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:52:27 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:52:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:52:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:39013 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:52:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:27 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:52:27 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:52:27 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc310, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:27 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc301, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc310:52993 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:52:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc301:48467 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3516 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3525 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:31 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:31 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.527 s%%0001017/07/17 22:52:31 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:52:31 INFO DAGScheduler: running: Set()%%0001017/07/17 22:52:31 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:52:31 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:52:31 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:39013 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:52:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:52:31 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc301, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc310, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc310:52993 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc301:48467 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc310:57760%%0001017/07/17 22:52:31 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 165 bytes%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc301:57727%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc310, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 34 ms on hcdnc310 (1/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc301, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 44 ms on hcdnc301 (2/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc310, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 13 ms on hcdnc310 (3/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc301, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 15 ms on hcdnc301 (4/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc310, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 18 ms on hcdnc310 (5/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc301, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc310, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 12 ms on hcdnc310 (6/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 17 ms on hcdnc301 (7/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc310, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc310 (8/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 16 ms on hcdnc301 (9/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc310 (10/10)%%0001017/07/17 22:52:31 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.098 s%%0001017/07/17 22:52:31 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:31 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.671597 s%%0001017/07/17 22:52:31 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:52:31 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:52:31 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:52:31 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:52:31 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:52:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:52:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(13320) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.0 KB, free 530.0 MB)%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(7329) called with curMem=301800, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:39013 (size: 7.2 KB, free: 530.2 MB)%%0001017/07/17 22:52:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:52:31 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc301, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc310, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc301:48467 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc310:52993 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc310:57760%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc301:57727%%0001017/07/17 22:52:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 18620 ms on hcdnc301 (1/2)%%0001017/07/17 22:52:49 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 18781 ms on hcdnc310 (2/2)%%0001017/07/17 22:52:49 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:49 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 18.782 s%%0001017/07/17 22:52:49 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:52:49 INFO DAGScheduler: running: Set()%%0001017/07/17 22:52:49 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:52:49 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:52:49 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:52:49 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:52:49 INFO MemoryStore: ensureFreeSpace(16480) called with curMem=309129, maxMem=556038881%%0001017/07/17 22:52:49 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.1 KB, free 530.0 MB)%%0001017/07/17 22:52:49 INFO MemoryStore: ensureFreeSpace(8730) called with curMem=325609, maxMem=556038881%%0001017/07/17 22:52:49 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:52:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:39013 (size: 8.5 KB, free: 530.2 MB)%%0001017/07/17 22:52:49 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:49 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:52:49 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc310, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc301, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc310:52993 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:52:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc301:48467 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:52:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc301:57727%%0001017/07/17 22:52:50 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 157 bytes%%0001017/07/17 22:52:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc310:57760%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc310, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 827 ms on hcdnc310 (1/10)%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc301, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 936 ms on hcdnc301 (2/10)%%0001017/07/17 22:52:51 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:52:51 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc310, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:51 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 560 ms on hcdnc310 (3/10)%%0001017/07/17 22:52:51 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc301, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:51 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 724 ms on hcdnc301 (4/10)%%0001017/07/17 22:52:51 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc310, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:51 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 550 ms on hcdnc310 (5/10)%%0001017/07/17 22:52:52 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:52:52 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc301, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:52 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 659 ms on hcdnc301 (6/10)%%0001017/07/17 22:52:52 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc310, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:52 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 537 ms on hcdnc310 (7/10)%%0001017/07/17 22:52:52 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc301, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:52 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 677 ms on hcdnc301 (8/10)%%0001017/07/17 22:52:53 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 562 ms on hcdnc310 (9/10)%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://driverPropsFetcher@hcdnc403:43670]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc403:43670] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc403:43670%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 567 ms on hcdnc301 (10/10)%%0001017/07/17 22:52:53 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:53 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.563 s%%0001017/07/17 22:52:53 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 22.372064 s%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc403:37422/user/Executor#-672322753]) with ID 3%%0001017/07/17 22:52:53 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:52:53 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:52:53 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://sparkExecutor@hcdnc301:57727]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc301:57727] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc301:57727%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://sparkExecutor@hcdnc310:57760]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc310:57760] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc310:57760%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc403:38823 with 530.0 MB RAM, BlockManagerId(3, hcdnc403, 38823)%%0001017/07/17 22:52:53 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockManagerAdded(1500303173760,BlockManagerId(3, hcdnc403, 38823),555755765)%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://sparkExecutor@hcdnc403:37422]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc403:37422] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc403:37422%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:52:53 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:52:53 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:52:53 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:52:53 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:52:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:52:53 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:52:53 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:52:53 INFO Remoting: Remoting shut down%%0001017/07/17 22:52:53 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:52:54 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:52:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-76e3aa34-9463-4bf6-8a0a-0b38288ab660%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="6">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:53:09 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:53:09 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:53:09 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:53:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:53:10 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:53:10 INFO Remoting: Starting remoting%%0001017/07/17 22:53:10 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:35095]%%0001017/07/17 22:53:10 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:35095]%%0001017/07/17 22:53:10 INFO Utils: Successfully started service 'sparkDriver' on port 35095.%%0001017/07/17 22:53:10 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:53:10 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:53:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0b2d4a83-57dc-4f1f-bbb4-005e5fc1acbc%%0001017/07/17 22:53:10 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:53:11 INFO HttpFileServer: HTTP File server directory is /tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48/httpd-126d83f5-1d94-4417-b2a9-21a1637a492e%%0001017/07/17 22:53:11 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:53:11 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:53:11 INFO AbstractConnector: Started SocketConnector@0.0.0.0:42803%%0001017/07/17 22:53:11 INFO Utils: Successfully started service 'HTTP file server' on port 42803.%%0001017/07/17 22:53:11 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:53:11 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:53:12 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:53:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:53:12 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:53:12 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:53:12 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:53:12 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:53:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:53:12 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:53:12 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:53:12 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:53:12 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:53:13 INFO Client: Uploading resource file:/tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48/__spark_conf__6698094196194503130.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22077/__spark_conf__6698094196194503130.zip%%0001017/07/17 22:53:13 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:53:13 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:53:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:53:13 INFO Client: Submitting application 22077 to ResourceManager%%0001017/07/17 22:53:13 INFO YarnClientImpl: Submitted application application_1491786134915_22077%%0001017/07/17 22:53:14 INFO Client: Application report for application_1491786134915_22077 (state: ACCEPTED)%%0001017/07/17 22:53:14 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303193492%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22077/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:53:15 INFO Client: Application report for application_1491786134915_22077 (state: ACCEPTED)%%0001017/07/17 22:53:16 INFO Client: Application report for application_1491786134915_22077 (state: ACCEPTED)%%0001017/07/17 22:53:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:45041/user/YarnAM#920749199])%%0001017/07/17 22:53:16 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22077,http://hcnnc117:8088/proxy/application_1491786134915_22077), /proxy/application_1491786134915_22077%%0001017/07/17 22:53:16 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:53:17 INFO Client: Application report for application_1491786134915_22077 (state: RUNNING)%%0001017/07/17 22:53:17 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303193492%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22077/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:53:17 INFO YarnClientSchedulerBackend: Application application_1491786134915_22077 has started running.%%0001017/07/17 22:53:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38757.%%0001017/07/17 22:53:17 INFO NettyBlockTransferService: Server created on 38757%%0001017/07/17 22:53:17 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:53:17 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:53:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:38757 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 38757)%%0001017/07/17 22:53:17 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:53:18 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22077%%0001017/07/17 22:53:18 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:38757 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:53:18 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:53:18 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.30:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.9:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.4:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.32:50010%%0001017/07/17 22:53:18 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:53:18 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:53:18 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:53:18 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:53:18 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:53:18 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:38757 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:53:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:53:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:53:18 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:53:19 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:53:20 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:53:22 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://driverPropsFetcher@hcdnc330:37003]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc330:37003] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc330:37003%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:53:22 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc330:57355/user/Executor#16623872]) with ID 1%%0001017/07/17 22:53:22 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:53:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc330, partition 0,NODE_LOCAL, 2147 bytes)%%0001017/07/17 22:53:22 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc330:34190 with 530.0 MB RAM, BlockManagerId(1, hcdnc330, 34190)%%0001017/07/17 22:53:22 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://driverPropsFetcher@hcdnc320:39346]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc320:39346] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc320:39346%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:53:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc330:34190 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:53:23 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc320:59130/user/Executor#-602691211]) with ID 2%%0001017/07/17 22:53:23 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:53:23 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc320:47660 with 530.0 MB RAM, BlockManagerId(2, hcdnc320, 47660)%%0001017/07/17 22:53:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc330:34190 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:53:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc320, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:53:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc320:47660 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:53:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc320:47660 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:53:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15779 ms on hcdnc330 (1/2)%%0001017/07/17 22:53:41 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 15987 ms on hcdnc320 (2/2)%%0001017/07/17 22:53:41 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 22.901 s%%0001017/07/17 22:53:41 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:53:41 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 23.077911 s%%0001017/07/17 22:53:41 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:53:41 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:53:41 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:53:41 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:53:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:53:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:53:41 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:53:41 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:53:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:53:41 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:53:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:53:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:38757 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:53:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:53:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:53:41 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:53:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc320, partition 0,NODE_LOCAL, 2136 bytes)%%0001017/07/17 22:53:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc320:47660 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:53:42 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:53:45 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc330, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:53:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc330:34190 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:54:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 22084 ms on hcdnc320 (1/2)%%0001017/07/17 22:54:06 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 21018 ms on hcdnc330 (2/2)%%0001017/07/17 22:54:06 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:06 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.959 s%%0001017/07/17 22:54:06 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:54:06 INFO DAGScheduler: running: Set()%%0001017/07/17 22:54:06 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:54:06 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:54:06 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:54:06 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:54:06 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:54:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:54:06 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:54:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:54:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:38757 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:54:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:54:06 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:54:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc330, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:06 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc320, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc330:34190 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:54:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc320:47660 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:54:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc330:57355%%0001017/07/17 22:54:06 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:54:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc320:59130%%0001017/07/17 22:54:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1658 ms on hcdnc330 (1/2)%%0001017/07/17 22:54:08 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1739 ms on hcdnc320 (2/2)%%0001017/07/17 22:54:08 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:08 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.741 s%%0001017/07/17 22:54:08 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 26.732969 s%%0001017/07/17 22:54:08 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:54:08 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:54:08 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:54:08 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:54:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:54:08 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:54:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:54:08 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:54:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:54:08 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:54:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:54:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:38757 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:54:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:54:08 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:54:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc330, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:08 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc320, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc320:47660 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:54:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc330:34190 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:54:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3465 ms on hcdnc330 (1/2)%%0001017/07/17 22:54:12 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3597 ms on hcdnc320 (2/2)%%0001017/07/17 22:54:12 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:12 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.599 s%%0001017/07/17 22:54:12 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.614624 s%%0001017/07/17 22:54:12 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:54:12 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:54:12 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:54:12 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:54:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:54:12 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:54:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:54:12 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:54:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:54:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:38757 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:54:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:54:12 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:54:12 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc320, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:12 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc330, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc330:34190 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:54:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc320:47660 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3544 ms on hcdnc320 (1/2)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3568 ms on hcdnc330 (2/2)%%0001017/07/17 22:54:15 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:15 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.569 s%%0001017/07/17 22:54:15 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:54:15 INFO DAGScheduler: running: Set()%%0001017/07/17 22:54:15 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:54:15 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:54:15 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:38757 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:54:15 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:54:15 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc320, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc330, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc330:34190 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc320:47660 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc320:59130%%0001017/07/17 22:54:15 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 164 bytes%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc330:57355%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc330, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 46 ms on hcdnc330 (1/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc320, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 50 ms on hcdnc320 (2/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc330, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 15 ms on hcdnc330 (3/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc320, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc320 (4/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc330, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 20 ms on hcdnc330 (5/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc320, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 18 ms on hcdnc320 (6/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc330, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc320, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc330 (7/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 14 ms on hcdnc320 (8/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc330 (9/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc320 (10/10)%%0001017/07/17 22:54:15 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:15 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.106 s%%0001017/07/17 22:54:15 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.699156 s%%0001017/07/17 22:54:15 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:54:15 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:54:15 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:54:15 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:54:15 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:54:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:54:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(12520) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 530.0 MB)%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(6887) called with curMem=301000, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.7 KB, free 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:38757 (size: 6.7 KB, free: 530.2 MB)%%0001017/07/17 22:54:15 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:54:15 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc320, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc330, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc320:47660 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc330:34190 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc320:59130%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc330:57355%%0001017/07/17 22:54:28 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 12884 ms on hcdnc330 (1/2)%%0001017/07/17 22:54:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 13455 ms on hcdnc320 (2/2)%%0001017/07/17 22:54:29 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:29 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 13.456 s%%0001017/07/17 22:54:29 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:54:29 INFO DAGScheduler: running: Set()%%0001017/07/17 22:54:29 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:54:29 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:54:29 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:54:29 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:54:29 INFO MemoryStore: ensureFreeSpace(15504) called with curMem=307887, maxMem=556038881%%0001017/07/17 22:54:29 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.1 KB, free 530.0 MB)%%0001017/07/17 22:54:29 INFO MemoryStore: ensureFreeSpace(8173) called with curMem=323391, maxMem=556038881%%0001017/07/17 22:54:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 530.0 MB)%%0001017/07/17 22:54:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:38757 (size: 8.0 KB, free: 530.2 MB)%%0001017/07/17 22:54:29 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:29 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:54:29 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc320, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc330, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc320:47660 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:54:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc330:34190 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:54:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc320:59130%%0001017/07/17 22:54:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 160 bytes%%0001017/07/17 22:54:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc330:57355%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc330, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 550 ms on hcdnc330 (1/10)%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc320, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 565 ms on hcdnc320 (2/10)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc330, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 454 ms on hcdnc330 (3/10)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc320, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 446 ms on hcdnc320 (4/10)%%0001017/07/17 22:54:30 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc330, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 444 ms on hcdnc330 (5/10)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc320, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 477 ms on hcdnc320 (6/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc330, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 449 ms on hcdnc330 (7/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc320, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 428 ms on hcdnc320 (8/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 499 ms on hcdnc330 (9/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 578 ms on hcdnc320 (10/10)%%0001017/07/17 22:54:31 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:31 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 2.490 s%%0001017/07/17 22:54:31 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 15.973522 s%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:54:32 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:54:32 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:54:32 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://sparkExecutor@hcdnc330:57355]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc330:57355] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc330:57355%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:54:32 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://sparkExecutor@hcdnc320:59130]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc320:59130] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc320:59130%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:54:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:54:32 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:54:32 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:54:32 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:54:32 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:54:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:54:32 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:54:32 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:54:32 INFO Remoting: Remoting shut down%%0001017/07/17 22:54:32 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:54:34 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:54:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48/pyspark-871dfbab-a140-4841-8937-029089e1a71e%%0001017/07/17 22:54:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="7">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:54:48 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:54:49 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:54:49 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:54:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:54:49 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:54:50 INFO Remoting: Starting remoting%%0001017/07/17 22:54:50 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:46487]%%0001017/07/17 22:54:50 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:46487]%%0001017/07/17 22:54:50 INFO Utils: Successfully started service 'sparkDriver' on port 46487.%%0001017/07/17 22:54:50 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:54:50 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:54:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3dcd78b0-4e3d-49d2-adc6-71755624913e%%0001017/07/17 22:54:50 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:54:50 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45/httpd-d76724b4-6bfc-40cc-9d85-943120e99b81%%0001017/07/17 22:54:50 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:54:50 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:54:50 INFO AbstractConnector: Started SocketConnector@0.0.0.0:40438%%0001017/07/17 22:54:50 INFO Utils: Successfully started service 'HTTP file server' on port 40438.%%0001017/07/17 22:54:50 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:54:51 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:54:51 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:54:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:54:51 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:54:51 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:54:52 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:54:52 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:54:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:54:52 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:54:52 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:54:52 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:54:52 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:54:52 INFO Client: Uploading resource file:/tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45/__spark_conf__6610156780387149534.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22078/__spark_conf__6610156780387149534.zip%%0001017/07/17 22:54:52 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:54:52 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:54:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:54:52 INFO Client: Submitting application 22078 to ResourceManager%%0001017/07/17 22:54:53 INFO YarnClientImpl: Submitted application application_1491786134915_22078%%0001017/07/17 22:54:54 INFO Client: Application report for application_1491786134915_22078 (state: ACCEPTED)%%0001017/07/17 22:54:54 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303292900%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22078/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:54:55 INFO Client: Application report for application_1491786134915_22078 (state: ACCEPTED)%%0001017/07/17 22:54:55 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:35633/user/YarnAM#-976557814])%%0001017/07/17 22:54:55 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22078,http://hcnnc117:8088/proxy/application_1491786134915_22078), /proxy/application_1491786134915_22078%%0001017/07/17 22:54:55 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:54:56 INFO Client: Application report for application_1491786134915_22078 (state: RUNNING)%%0001017/07/17 22:54:56 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303292900%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22078/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:54:56 INFO YarnClientSchedulerBackend: Application application_1491786134915_22078 has started running.%%0001017/07/17 22:54:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35797.%%0001017/07/17 22:54:56 INFO NettyBlockTransferService: Server created on 35797%%0001017/07/17 22:54:56 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:54:56 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:54:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:35797 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 35797)%%0001017/07/17 22:54:56 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:54:56 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22078%%0001017/07/17 22:54:56 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:54:56 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:54:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:54:56 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:54:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:54:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:35797 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:54:56 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:54:57 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.12:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.13:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.31:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.25:50010%%0001017/07/17 22:54:57 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:54:57 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:54:57 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:54:57 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:54:57 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:54:57 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:54:57 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:54:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:54:57 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:54:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:54:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:35797 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:54:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:54:57 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:54:58 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:54:59 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:55:00 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://driverPropsFetcher@hcdnc823:45961]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:45961] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:45961%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:55:01 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc823:40788/user/Executor#-1325376196]) with ID 1%%0001017/07/17 22:55:01 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:55:01 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0, hcdnc823, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:55:01 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc823:51216 with 530.0 MB RAM, BlockManagerId(1, hcdnc823, 51216)%%0001017/07/17 22:55:01 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://driverPropsFetcher@hcdnc825:48017]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc825:48017] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc825:48017%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:55:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc823:51216 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:01 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc825:34020/user/Executor#376040495]) with ID 2%%0001017/07/17 22:55:01 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:55:01 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc825:58420 with 530.0 MB RAM, BlockManagerId(2, hcdnc825, 58420)%%0001017/07/17 22:55:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc823:51216 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:55:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1, hcdnc825, partition 0,ANY, 2147 bytes)%%0001017/07/17 22:55:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc825:58420 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc825:58420 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:55:16 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 15347 ms on hcdnc823 (1/2)%%0001017/07/17 22:55:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 15812 ms on hcdnc825 (2/2)%%0001017/07/17 22:55:20 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:20 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 23.671 s%%0001017/07/17 22:55:20 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 23.854399 s%%0001017/07/17 22:55:20 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:55:20 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:55:20 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:55:20 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:55:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:55:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:55:20 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:55:20 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:55:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:55:20 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:55:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:55:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:35797 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:55:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:20 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:55:20 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:55:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, hcdnc825, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:55:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc825:58420 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:55:22 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:55:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, hcdnc823, partition 0,ANY, 2136 bytes)%%0001017/07/17 22:55:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc823:51216 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:55:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 21732 ms on hcdnc825 (1/2)%%0001017/07/17 22:55:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 21159 ms on hcdnc823 (2/2)%%0001017/07/17 22:55:45 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:45 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.278 s%%0001017/07/17 22:55:45 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:55:45 INFO DAGScheduler: running: Set()%%0001017/07/17 22:55:45 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:55:45 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:55:45 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:55:45 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:55:45 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:55:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:55:45 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:55:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:55:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:35797 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:55:45 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:55:45 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:55:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:45 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc825, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc823:51216 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc825:58420 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc825:34020%%0001017/07/17 22:55:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:55:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:40788%%0001017/07/17 22:55:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1615 ms on hcdnc823 (1/2)%%0001017/07/17 22:55:47 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1776 ms on hcdnc825 (2/2)%%0001017/07/17 22:55:47 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:47 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.778 s%%0001017/07/17 22:55:47 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 26.087244 s%%0001017/07/17 22:55:47 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:55:47 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:55:47 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:55:47 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:55:47 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:55:47 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:55:47 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:55:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:55:47 INFO MemoryStore: ensureFreeSpace(3904) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:55:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:35797 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:55:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:55:47 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:55:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc825, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:47 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc823, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc825:58420 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc823:51216 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:55:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3440 ms on hcdnc825 (1/2)%%0001017/07/17 22:55:50 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3451 ms on hcdnc823 (2/2)%%0001017/07/17 22:55:50 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.453 s%%0001017/07/17 22:55:50 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:50 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.466924 s%%0001017/07/17 22:55:50 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:55:50 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:55:50 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:55:50 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:55:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:55:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:55:50 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:55:50 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272760, maxMem=556038881%%0001017/07/17 22:55:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:55:50 INFO MemoryStore: ensureFreeSpace(4151) called with curMem=279936, maxMem=556038881%%0001017/07/17 22:55:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:55:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:35797 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:55:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:50 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:55:50 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:55:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc825, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:50 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc823:51216 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:55:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc825:58420 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3460 ms on hcdnc823 (1/2)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3496 ms on hcdnc825 (2/2)%%0001017/07/17 22:55:54 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:54 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.497 s%%0001017/07/17 22:55:54 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:55:54 INFO DAGScheduler: running: Set()%%0001017/07/17 22:55:54 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:55:54 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:55:54 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284087, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286871, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:35797 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:55:54 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:55:54 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc825, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc823, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc825:58420 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc823:51216 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc825:34020%%0001017/07/17 22:55:54 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 164 bytes%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc823:40788%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc825, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc823, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 45 ms on hcdnc825 (1/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 45 ms on hcdnc823 (2/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc825, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc823, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 18 ms on hcdnc825 (3/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc823 (4/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc825, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc825 (5/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc823, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 20 ms on hcdnc823 (6/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc825, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc825 (7/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc823, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 14 ms on hcdnc823 (8/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc825 (9/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc823 (10/10)%%0001017/07/17 22:55:54 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.103 s%%0001017/07/17 22:55:54 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:54 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.621325 s%%0001017/07/17 22:55:54 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:55:54 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:55:54 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:55:54 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:55:54 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:55:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:55:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(12520) called with curMem=288484, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 530.0 MB)%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(6889) called with curMem=301004, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.7 KB, free 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:35797 (size: 6.7 KB, free: 530.2 MB)%%0001017/07/17 22:55:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:55:54 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc825, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc825:58420 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc823:51216 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc825:34020%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:40788%%0001017/07/17 22:56:07 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 13446 ms on hcdnc825 (1/2)%%0001017/07/17 22:56:08 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 14284 ms on hcdnc823 (2/2)%%0001017/07/17 22:56:08 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:56:08 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 14.286 s%%0001017/07/17 22:56:08 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:56:08 INFO DAGScheduler: running: Set()%%0001017/07/17 22:56:08 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:56:08 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:56:08 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:56:08 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:56:08 INFO MemoryStore: ensureFreeSpace(15504) called with curMem=307893, maxMem=556038881%%0001017/07/17 22:56:08 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.1 KB, free 530.0 MB)%%0001017/07/17 22:56:08 INFO MemoryStore: ensureFreeSpace(8176) called with curMem=323397, maxMem=556038881%%0001017/07/17 22:56:08 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 530.0 MB)%%0001017/07/17 22:56:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:35797 (size: 8.0 KB, free: 530.2 MB)%%0001017/07/17 22:56:08 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:56:08 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:56:08 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:56:08 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:08 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc825, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc823:51216 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:56:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc825:58420 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:56:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc825:34020%%0001017/07/17 22:56:08 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 160 bytes%%0001017/07/17 22:56:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc823:40788%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc825, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 586 ms on hcdnc825 (1/10)%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc823, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 678 ms on hcdnc823 (2/10)%%0001017/07/17 22:56:09 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc825, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 539 ms on hcdnc825 (3/10)%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc823, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 497 ms on hcdnc823 (4/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc825, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 415 ms on hcdnc825 (5/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc823, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 464 ms on hcdnc823 (6/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc825, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 461 ms on hcdnc825 (7/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc823, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 472 ms on hcdnc823 (8/10)%%0001017/07/17 22:56:11 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 491 ms on hcdnc825 (9/10)%%0001017/07/17 22:56:11 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 531 ms on hcdnc823 (10/10)%%0001017/07/17 22:56:11 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:56:11 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 2.638 s%%0001017/07/17 22:56:11 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 16.950343 s%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:56:11 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:56:11 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:56:11 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://sparkExecutor@hcdnc825:34020]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc825:34020] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc825:34020%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:11 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://sparkExecutor@hcdnc823:40788]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc823:40788] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc823:40788%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:56:11 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:56:11 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:56:11 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:56:11 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:56:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:56:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:56:11 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:56:11 INFO Remoting: Remoting shut down%%0001017/07/17 22:56:11 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:56:12 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:56:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45/pyspark-2342f14d-a3a8-4f50-ba61-fec8ac943925%%0001017/07/17 22:56:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
<config key="8">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="org.pasteur.pf2.tools.InterruptedExecutionException: STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:56:26 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:56:27 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:56:27 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:56:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:56:27 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:56:27 INFO Remoting: Starting remoting%%0001017/07/17 22:56:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:35779]%%0001017/07/17 22:56:27 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:35779]%%0001017/07/17 22:56:27 INFO Utils: Successfully started service 'sparkDriver' on port 35779.%%0001017/07/17 22:56:27 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:56:27 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:56:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7c9ffc16-fdb1-4eed-9472-75805bfb9c91%%0001017/07/17 22:56:27 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:56:27 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6289b39e-6363-449c-b361-9f7949145b57/httpd-787e9417-b4ea-449f-8e30-75296370cdea%%0001017/07/17 22:56:27 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:56:28 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:56:28 INFO AbstractConnector: Started SocketConnector@0.0.0.0:38817%%0001017/07/17 22:56:28 INFO Utils: Successfully started service 'HTTP file server' on port 38817.%%0001017/07/17 22:56:28 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:56:28 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:56:28 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:56:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:56:28 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:56:28 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:56:28 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:56:28 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:56:28 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:56:28 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:56:28 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:56:28 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:56:28 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:56:29 INFO Client: Uploading resource file:/tmp/spark-6289b39e-6363-449c-b361-9f7949145b57/__spark_conf__5587776092955324535.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22079/__spark_conf__5587776092955324535.zip%%0001017/07/17 22:56:30 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:56:30 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:56:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:56:30 INFO Client: Submitting application 22079 to ResourceManager%%0001017/07/17 22:56:30 INFO YarnClientImpl: Submitted application application_1491786134915_22079%%0001017/07/17 22:56:31 INFO Client: Application report for application_1491786134915_22079 (state: ACCEPTED)%%0001017/07/17 22:56:31 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303390346%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22079/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:56:32 INFO Client: Application report for application_1491786134915_22079 (state: ACCEPTED)%%0001017/07/17 22:56:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:51525/user/YarnAM#-1584097551])%%0001017/07/17 22:56:33 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22079,http://hcnnc117:8088/proxy/application_1491786134915_22079), /proxy/application_1491786134915_22079%%0001017/07/17 22:56:33 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:56:33 INFO Client: Application report for application_1491786134915_22079 (state: RUNNING)%%0001017/07/17 22:56:33 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303390346%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22079/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:56:33 INFO YarnClientSchedulerBackend: Application application_1491786134915_22079 has started running.%%0001017/07/17 22:56:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33185.%%0001017/07/17 22:56:33 INFO NettyBlockTransferService: Server created on 33185%%0001017/07/17 22:56:33 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:56:33 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:56:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:33185 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 33185)%%0001017/07/17 22:56:33 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:56:33 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22079%%0001017/07/17 22:56:33 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:33185 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:56:34 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:56:34 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.31:50010%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.35:50010%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.29:50010%%0001017/07/17 22:56:34 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:56:34 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:56:34 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:56:34 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:56:34 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:56:34 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:33185 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:56:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:56:34 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:56:34 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:56:35 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:56:36 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:56:38 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://driverPropsFetcher@hcdnc323:50799]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc323:50799] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc323:50799%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:38 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc323:39660/user/Executor#-348958769]) with ID 1%%0001017/07/17 22:56:38 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:56:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc323, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:56:38 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc323:38204 with 530.0 MB RAM, BlockManagerId(1, hcdnc323, 38204)%%0001017/07/17 22:56:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc323:38204 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:56:38 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://driverPropsFetcher@hcdnc827:44467]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc827:44467] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc827:44467%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:39 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc827:37555/user/Executor#1208374042]) with ID 2%%0001017/07/17 22:56:39 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:56:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc827, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:56:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc323:38204 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:56:39 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc827:51633 with 530.0 MB RAM, BlockManagerId(2, hcdnc827, 51633)%%0001017/07/17 22:56:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc827:51633 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:56:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc827:51633 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:56:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15367 ms on hcdnc323 (1/2)%%0001017/07/17 22:56:53 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14699 ms on hcdnc827 (2/2)%%0001017/07/17 22:56:53 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.309 s%%0001017/07/17 22:56:53 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:56:53 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 19.456195 s%%0001017/07/17 22:56:54 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:56:54 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:56:54 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:56:54 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:56:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:56:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:56:54 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:56:54 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:56:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:56:54 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:56:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:56:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:33185 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:56:54 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:56:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:56:54 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:56:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc323, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:56:54 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc827, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:56:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc323:38204 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:56:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc827:51633 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:57:13 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 19928 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21714 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:15 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:15 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.714 s%%0001017/07/17 22:57:15 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:57:15 INFO DAGScheduler: running: Set()%%0001017/07/17 22:57:15 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:57:15 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:57:15 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:57:15 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:57:15 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:57:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:57:15 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:57:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:57:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:33185 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:57:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:57:15 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:57:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc323, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:15 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc827, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc323:38204 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:57:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc827:51633 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:57:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc323:39660%%0001017/07/17 22:57:15 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:57:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc827:37555%%0001017/07/17 22:57:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1646 ms on hcdnc323 (1/2)%%0001017/07/17 22:57:17 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1710 ms on hcdnc827 (2/2)%%0001017/07/17 22:57:17 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:17 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.711 s%%0001017/07/17 22:57:17 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 23.458945 s%%0001017/07/17 22:57:17 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:57:17 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:57:17 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:57:17 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:57:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:57:17 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:57:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:57:17 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:57:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:57:17 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:57:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:57:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:33185 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:57:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:57:17 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:57:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc323, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:17 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc827, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc827:51633 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:57:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc323:38204 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:57:20 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3419 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 4321 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:21 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:21 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 4.322 s%%0001017/07/17 22:57:21 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 4.335987 s%%0001017/07/17 22:57:21 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:57:21 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:57:21 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:57:21 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:57:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:57:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:57:21 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:57:21 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:57:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:57:21 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:57:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:57:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:33185 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:57:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:57:21 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:57:21 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc827, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:21 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc323, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc323:38204 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:57:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc827:51633 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:57:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3469 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 4543 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:26 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:26 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 4.545 s%%0001017/07/17 22:57:26 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:57:26 INFO DAGScheduler: running: Set()%%0001017/07/17 22:57:26 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:57:26 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:57:26 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:33185 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:57:26 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:57:26 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc827, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc323, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc323:38204 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc827:51633 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc323:39660%%0001017/07/17 22:57:26 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 166 bytes%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc827:37555%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc323, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc827, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 47 ms on hcdnc323 (1/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 47 ms on hcdnc827 (2/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc323, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc827, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 17 ms on hcdnc323 (3/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 16 ms on hcdnc827 (4/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc323, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc323 (5/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc827, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 19 ms on hcdnc827 (6/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc323, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc323 (7/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc827, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc827 (8/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc323 (9/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc827 (10/10)%%0001017/07/17 22:57:26 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:26 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.106 s%%0001017/07/17 22:57:26 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 4.673833 s%%0001017/07/17 22:57:26 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:57:26 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:57:26 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:57:26 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:57:26 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:57:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:57:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(12520) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 530.0 MB)%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(6887) called with curMem=301000, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.7 KB, free 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:33185 (size: 6.7 KB, free: 530.2 MB)%%0001017/07/17 22:57:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:57:26 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc827, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc323, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc827:51633 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc323:38204 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc323:39660%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc827:37555%%0001017/07/17 22:57:40 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 13722 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:40 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 14031 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:40 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:40 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 14.032 s%%0001017/07/17 22:57:40 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:57:40 INFO DAGScheduler: running: Set()%%0001017/07/17 22:57:40 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:57:40 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:57:40 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:57:40 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:57:40 INFO MemoryStore: ensureFreeSpace(15504) called with curMem=307887, maxMem=556038881%%0001017/07/17 22:57:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.1 KB, free 530.0 MB)%%0001017/07/17 22:57:40 INFO MemoryStore: ensureFreeSpace(8173) called with curMem=323391, maxMem=556038881%%0001017/07/17 22:57:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 530.0 MB)%%0001017/07/17 22:57:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:33185 (size: 8.0 KB, free: 530.2 MB)%%0001017/07/17 22:57:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:40 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:57:40 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:57:40 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc323, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:40 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc827, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc323:38204 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:57:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc827:51633 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:57:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc323:39660%%0001017/07/17 22:57:40 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 162 bytes%%0001017/07/17 22:57:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc827:37555%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc827, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 578 ms on hcdnc827 (1/10)%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc323, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 633 ms on hcdnc323 (2/10)%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc827, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 459 ms on hcdnc827 (3/10)%%0001017/07/17 22:57:41 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc323, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 498 ms on hcdnc323 (4/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc827, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 437 ms on hcdnc827 (5/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc323, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 497 ms on hcdnc323 (6/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc827, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 428 ms on hcdnc827 (7/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc323, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 448 ms on hcdnc323 (8/10)%%0001017/07/17 22:57:43 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 516 ms on hcdnc827 (9/10)%%0001017/07/17 22:57:43 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 520 ms on hcdnc323 (10/10)%%0001017/07/17 22:57:43 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:43 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 2.594 s%%0001017/07/17 22:57:43 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 16.652190 s%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:57:43 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:57:43 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:57:43 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://sparkExecutor@hcdnc323:39660]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc323:39660] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc323:39660%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:57:43 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://sparkExecutor@hcdnc827:37555]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc827:37555] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc827:37555%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:57:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:57:43 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:57:43 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:57:43 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:57:43 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:57:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:57:43 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:57:43 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:57:43 INFO Remoting: Remoting shut down%%0001017/07/17 22:57:43 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:57:44 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:57:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-6289b39e-6363-449c-b361-9f7949145b57%%00010%%00013%%00010%%00009at org.pasteur.pf2.tools.BashNodeModel.execute(BashNodeModel.java:151)%%00013%%00010%%00009at org.knime.core.node.NodeModel.execute(NodeModel.java:733)%%00013%%00010%%00009at org.knime.core.node.NodeModel.executeModel(NodeModel.java:567)%%00013%%00010%%00009at org.knime.core.node.Node.invokeFullyNodeModelExecute(Node.java:1128)%%00013%%00010%%00009at org.knime.core.node.Node.execute(Node.java:915)%%00013%%00010%%00009at org.knime.core.node.workflow.NativeNodeContainer.performExecuteNode(NativeNodeContainer.java:561)%%00013%%00010%%00009at org.knime.core.node.exec.LocalNodeExecutionJob.mainExecute(LocalNodeExecutionJob.java:95)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.internalRun(NodeExecutionJob.java:179)%%00013%%00010%%00009at org.knime.core.node.workflow.NodeExecutionJob.run(NodeExecutionJob.java:110)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContextImpl.runWithContext(ThreadUtils.java:328)%%00013%%00010%%00009at org.knime.core.util.ThreadUtils$RunnableWithContext.run(ThreadUtils.java:204)%%00013%%00010%%00009at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)%%00013%%00010%%00009at java.util.concurrent.FutureTask.run(FutureTask.java:266)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$MyFuture.run(ThreadPool.java:123)%%00013%%00010%%00009at org.knime.core.util.ThreadPool$Worker.run(ThreadPool.java:246)%%00013%%00010"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_1">
<entry key="column_name" type="xstring" value="FailingNodeMessage"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="9"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:43:10 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:43:11 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:43:11 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:43:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:43:12 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:43:12 INFO Remoting: Starting remoting%%0001017/07/17 22:43:12 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:34187]%%0001017/07/17 22:43:12 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:34187]%%0001017/07/17 22:43:12 INFO Utils: Successfully started service 'sparkDriver' on port 34187.%%0001017/07/17 22:43:12 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:43:12 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:43:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3aa77834-d57a-4da7-896b-a2403d3e6330%%0001017/07/17 22:43:12 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:43:12 INFO HttpFileServer: HTTP File server directory is /tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282/httpd-500a6141-463c-41dc-8411-7e27aaabf48c%%0001017/07/17 22:43:12 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:43:12 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:43:12 INFO AbstractConnector: Started SocketConnector@0.0.0.0:40517%%0001017/07/17 22:43:12 INFO Utils: Successfully started service 'HTTP file server' on port 40517.%%0001017/07/17 22:43:12 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:43:12 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:43:13 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:43:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:43:13 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:43:13 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:43:13 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:43:13 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:43:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:43:13 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:43:13 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:43:13 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:43:13 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:43:13 INFO Client: Uploading resource file:/tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282/__spark_conf__4995776726975785966.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22071/__spark_conf__4995776726975785966.zip%%0001017/07/17 22:43:14 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:43:14 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:43:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:43:14 INFO Client: Submitting application 22071 to ResourceManager%%0001017/07/17 22:43:14 INFO YarnClientImpl: Submitted application application_1491786134915_22071%%0001017/07/17 22:43:15 INFO Client: Application report for application_1491786134915_22071 (state: ACCEPTED)%%0001017/07/17 22:43:15 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302594621%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22071/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:43:16 INFO Client: Application report for application_1491786134915_22071 (state: ACCEPTED)%%0001017/07/17 22:43:17 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:54359/user/YarnAM#-293742013])%%0001017/07/17 22:43:17 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22071,http://hcnnc117:8088/proxy/application_1491786134915_22071), /proxy/application_1491786134915_22071%%0001017/07/17 22:43:17 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:43:17 INFO Client: Application report for application_1491786134915_22071 (state: RUNNING)%%0001017/07/17 22:43:17 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302594621%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22071/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:43:17 INFO YarnClientSchedulerBackend: Application application_1491786134915_22071 has started running.%%0001017/07/17 22:43:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38085.%%0001017/07/17 22:43:17 INFO NettyBlockTransferService: Server created on 38085%%0001017/07/17 22:43:17 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:43:17 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:43:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:38085 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 38085)%%0001017/07/17 22:43:17 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:43:18 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22071%%0001017/07/17 22:43:18 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:38085 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:43:18 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:43:18 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.13:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.9:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.8:50010%%0001017/07/17 22:43:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.10:50010%%0001017/07/17 22:43:18 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:43:18 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:43:18 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:43:18 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:43:18 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:43:18 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:43:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:43:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:38085 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:43:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:43:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:43:18 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:43:20 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:43:21 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:43:22 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://driverPropsFetcher@hcdnc318:43038]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc318:43038] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc318:43038%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:43:22 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc318:54325/user/Executor#-690596482]) with ID 1%%0001017/07/17 22:43:22 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:43:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc318, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:43:23 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc318:34123 with 530.0 MB RAM, BlockManagerId(1, hcdnc318, 34123)%%0001017/07/17 22:43:23 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://driverPropsFetcher@hcdnc327:56951]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc327:56951] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc327:56951%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:43:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc318:34123 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:43:23 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc327:42987/user/Executor#1621135318]) with ID 2%%0001017/07/17 22:43:23 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:43:23 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc327, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:43:23 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc327:52424 with 530.0 MB RAM, BlockManagerId(2, hcdnc327, 52424)%%0001017/07/17 22:43:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc318:34123 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:43:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc327:52424 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:43:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc327:52424 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:43:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15328 ms on hcdnc318 (1/2)%%0001017/07/17 22:43:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 15512 ms on hcdnc327 (2/2)%%0001017/07/17 22:43:39 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:43:39 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.981 s%%0001017/07/17 22:43:39 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 20.147374 s%%0001017/07/17 22:43:39 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:43:39 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:43:39 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:43:39 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:43:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:43:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:43:39 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:43:39 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:43:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:43:39 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:43:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:43:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:38085 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:43:39 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:43:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:43:39 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:43:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc327, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:43:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc318, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:43:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc318:34123 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:43:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc327:52424 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:43:59 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20797 ms on hcdnc318 (1/2)%%0001017/07/17 22:44:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21759 ms on hcdnc327 (2/2)%%0001017/07/17 22:44:00 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:00 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.760 s%%0001017/07/17 22:44:00 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:44:00 INFO DAGScheduler: running: Set()%%0001017/07/17 22:44:00 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:44:00 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:44:00 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:44:00 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:44:00 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:44:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:44:00 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:44:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:44:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:38085 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:44:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:44:00 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:44:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc318, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:00 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc327, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc327:52424 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:44:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc318:34123 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:44:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc318:54325%%0001017/07/17 22:44:00 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:44:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc327:42987%%0001017/07/17 22:44:02 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1676 ms on hcdnc327 (1/2)%%0001017/07/17 22:44:02 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1697 ms on hcdnc318 (2/2)%%0001017/07/17 22:44:02 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:02 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.699 s%%0001017/07/17 22:44:02 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 23.489542 s%%0001017/07/17 22:44:02 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:44:02 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:44:02 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:44:02 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:44:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:44:02 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:44:02 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:44:02 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:44:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:44:02 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:44:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:44:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:38085 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:44:02 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:44:02 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:44:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc318, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:02 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc327, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc318:34123 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:44:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc327:52424 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:44:06 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3447 ms on hcdnc318 (1/2)%%0001017/07/17 22:44:06 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3451 ms on hcdnc327 (2/2)%%0001017/07/17 22:44:06 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:06 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.452 s%%0001017/07/17 22:44:06 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.465568 s%%0001017/07/17 22:44:06 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:44:06 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:44:06 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:44:06 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:44:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:44:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:44:06 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:44:06 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:44:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:44:06 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:44:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:44:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:38085 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:44:06 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:44:06 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:44:06 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc318, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:06 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc327, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc327:52424 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:44:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc318:34123 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3444 ms on hcdnc327 (1/2)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3464 ms on hcdnc318 (2/2)%%0001017/07/17 22:44:09 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:09 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.464 s%%0001017/07/17 22:44:09 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:44:09 INFO DAGScheduler: running: Set()%%0001017/07/17 22:44:09 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:44:09 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:44:09 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(1615) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1615.0 B, free 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:38085 (size: 1615.0 B, free: 530.2 MB)%%0001017/07/17 22:44:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:44:09 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc318, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc327, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc318:34123 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc327:52424 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc318:54325%%0001017/07/17 22:44:09 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 165 bytes%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc327:42987%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc318, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc327, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 45 ms on hcdnc318 (1/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 45 ms on hcdnc327 (2/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc318, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 14 ms on hcdnc318 (3/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc327, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc327 (4/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc318, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc318 (5/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc327, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 19 ms on hcdnc327 (6/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc318, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc318 (7/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc327, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc327 (8/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc318 (9/10)%%0001017/07/17 22:44:09 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc327 (10/10)%%0001017/07/17 22:44:09 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.103 s%%0001017/07/17 22:44:09 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:09 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.586812 s%%0001017/07/17 22:44:09 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:44:09 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:44:09 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:44:09 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:44:09 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:44:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:44:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(14344) called with curMem=288482, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.0 KB, free 530.0 MB)%%0001017/07/17 22:44:09 INFO MemoryStore: ensureFreeSpace(7912) called with curMem=302826, maxMem=556038881%%0001017/07/17 22:44:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:38085 (size: 7.7 KB, free: 530.2 MB)%%0001017/07/17 22:44:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:44:09 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc318, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:09 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc327, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc327:52424 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc318:34123 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc318:54325%%0001017/07/17 22:44:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc327:42987%%0001017/07/17 22:44:27 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 17415 ms on hcdnc327 (1/2)%%0001017/07/17 22:44:27 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 17917 ms on hcdnc318 (2/2)%%0001017/07/17 22:44:27 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:27 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 17.918 s%%0001017/07/17 22:44:27 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:44:27 INFO DAGScheduler: running: Set()%%0001017/07/17 22:44:27 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:44:27 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:44:27 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:44:27 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:44:27 INFO MemoryStore: ensureFreeSpace(17736) called with curMem=310738, maxMem=556038881%%0001017/07/17 22:44:27 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.3 KB, free 530.0 MB)%%0001017/07/17 22:44:27 INFO MemoryStore: ensureFreeSpace(9458) called with curMem=328474, maxMem=556038881%%0001017/07/17 22:44:27 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 530.0 MB)%%0001017/07/17 22:44:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:38085 (size: 9.2 KB, free: 530.2 MB)%%0001017/07/17 22:44:27 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:27 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:44:27 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:44:27 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc327, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:27 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc318, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc318:34123 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:44:27 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc327:52424 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:44:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc318:54325%%0001017/07/17 22:44:27 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 160 bytes%%0001017/07/17 22:44:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc327:42987%%0001017/07/17 22:44:28 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc318, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:28 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 806 ms on hcdnc318 (1/10)%%0001017/07/17 22:44:28 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc327, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:28 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 913 ms on hcdnc327 (2/10)%%0001017/07/17 22:44:28 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:44:29 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc318, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:29 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 692 ms on hcdnc318 (3/10)%%0001017/07/17 22:44:29 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc327, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:29 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 863 ms on hcdnc327 (4/10)%%0001017/07/17 22:44:29 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:44:29 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc318, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:29 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 598 ms on hcdnc318 (5/10)%%0001017/07/17 22:44:30 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc327, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:30 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 676 ms on hcdnc327 (6/10)%%0001017/07/17 22:44:30 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc318, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:30 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 627 ms on hcdnc318 (7/10)%%0001017/07/17 22:44:30 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc327, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:44:30 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 680 ms on hcdnc327 (8/10)%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://driverPropsFetcher@hcdnc230:39617]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:39617] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:39617%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 753 ms on hcdnc318 (9/10)%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc230:43571/user/Executor#820444799]) with ID 3%%0001017/07/17 22:44:31 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:44:31 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc230:41028 with 530.0 MB RAM, BlockManagerId(3, hcdnc230, 41028)%%0001017/07/17 22:44:31 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 600 ms on hcdnc327 (10/10)%%0001017/07/17 22:44:31 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:44:31 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.731 s%%0001017/07/17 22:44:31 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 21.704609 s%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:44:31 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:44:31 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:44:31 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:44:31 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://sparkExecutor@hcdnc230:43571]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc230:43571] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc230:43571%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://sparkExecutor@hcdnc327:42987]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc327:42987] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc327:42987%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:34187] &lt;- [akka.tcp://sparkExecutor@hcdnc318:54325]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc318:54325] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc318:54325%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:44:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:44:31 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:44:31 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:44:31 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:44:31 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:44:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:44:31 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:44:31 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:44:31 INFO Remoting: Remoting shut down%%0001017/07/17 22:44:31 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:44:32 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:44:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282/pyspark-17e8b355-0669-425d-a2ab-59ad01481841%%0001017/07/17 22:44:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-eff7de3a-3ef8-4b4b-b84a-3366d46dd282%%00010"/>
</config>
</config>
<config key="1">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:44:47 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:44:47 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:44:47 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:44:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:44:48 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:44:48 INFO Remoting: Starting remoting%%0001017/07/17 22:44:48 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:44514]%%0001017/07/17 22:44:48 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:44514]%%0001017/07/17 22:44:48 INFO Utils: Successfully started service 'sparkDriver' on port 44514.%%0001017/07/17 22:44:48 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:44:48 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:44:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd2530ac-8db0-4528-9bee-12978641c60f%%0001017/07/17 22:44:48 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:44:48 INFO HttpFileServer: HTTP File server directory is /tmp/spark-855cc671-eae5-42a4-9ab3-70a7ed3fc949/httpd-8570321d-39e7-42c0-b900-b83b9cb78206%%0001017/07/17 22:44:48 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:44:49 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:44:49 INFO AbstractConnector: Started SocketConnector@0.0.0.0:41916%%0001017/07/17 22:44:49 INFO Utils: Successfully started service 'HTTP file server' on port 41916.%%0001017/07/17 22:44:49 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:44:49 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:44:49 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:44:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:44:49 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:44:49 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:44:49 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:44:50 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:44:50 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:44:50 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:44:50 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:44:50 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:44:50 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:44:50 INFO Client: Uploading resource file:/tmp/spark-855cc671-eae5-42a4-9ab3-70a7ed3fc949/__spark_conf__4539740636631601780.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22072/__spark_conf__4539740636631601780.zip%%0001017/07/17 22:44:51 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:44:51 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:44:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:44:51 INFO Client: Submitting application 22072 to ResourceManager%%0001017/07/17 22:44:51 INFO YarnClientImpl: Submitted application application_1491786134915_22072%%0001017/07/17 22:44:52 INFO Client: Application report for application_1491786134915_22072 (state: ACCEPTED)%%0001017/07/17 22:44:52 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302691248%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22072/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:44:53 INFO Client: Application report for application_1491786134915_22072 (state: ACCEPTED)%%0001017/07/17 22:44:54 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:48447/user/YarnAM#1468649141])%%0001017/07/17 22:44:54 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22072,http://hcnnc117:8088/proxy/application_1491786134915_22072), /proxy/application_1491786134915_22072%%0001017/07/17 22:44:54 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:44:54 INFO Client: Application report for application_1491786134915_22072 (state: ACCEPTED)%%0001017/07/17 22:44:55 INFO Client: Application report for application_1491786134915_22072 (state: RUNNING)%%0001017/07/17 22:44:55 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302691248%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22072/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:44:55 INFO YarnClientSchedulerBackend: Application application_1491786134915_22072 has started running.%%0001017/07/17 22:44:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42963.%%0001017/07/17 22:44:55 INFO NettyBlockTransferService: Server created on 42963%%0001017/07/17 22:44:55 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:44:55 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:44:55 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:42963 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 42963)%%0001017/07/17 22:44:55 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:44:55 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22072%%0001017/07/17 22:44:55 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:42963 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:44:56 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:44:56 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.15:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.40:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.35:50010%%0001017/07/17 22:44:56 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.34:50010%%0001017/07/17 22:44:56 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:44:56 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:44:56 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:44:56 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:44:56 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:44:56 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:44:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:44:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:42963 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:44:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:44:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:44:56 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:44:57 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:44:58 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:45:00 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://driverPropsFetcher@hcdnc226:58305]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc226:58305] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc226:58305%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:45:00 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc226:59302/user/Executor#-224899339]) with ID 1%%0001017/07/17 22:45:00 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:45:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc226, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:45:00 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc226:48101 with 530.0 MB RAM, BlockManagerId(1, hcdnc226, 48101)%%0001017/07/17 22:45:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc226:48101 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:01 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://driverPropsFetcher@hcdnc225:35484]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc225:35484] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc225:35484%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:45:01 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc225:47955/user/Executor#335037470]) with ID 2%%0001017/07/17 22:45:01 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:45:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc226:48101 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:45:01 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc225:49691 with 530.0 MB RAM, BlockManagerId(2, hcdnc225, 49691)%%0001017/07/17 22:45:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc225, partition 1,ANY, 2147 bytes)%%0001017/07/17 22:45:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc225:49691 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc225:49691 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:45:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 16069 ms on hcdnc226 (1/2)%%0001017/07/17 22:45:18 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14453 ms on hcdnc225 (2/2)%%0001017/07/17 22:45:18 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:18 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 22.078 s%%0001017/07/17 22:45:18 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 22.248535 s%%0001017/07/17 22:45:18 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:45:18 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:45:18 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:45:18 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:45:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:45:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:45:18 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:45:18 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:45:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:45:18 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:45:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:45:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:42963 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:45:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:45:18 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:45:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc225, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:45:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc225:49691 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:45:20 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:45:22 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc226, partition 1,ANY, 2136 bytes)%%0001017/07/17 22:45:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc226:48101 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:45:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 20303 ms on hcdnc225 (1/2)%%0001017/07/17 22:45:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 21051 ms on hcdnc226 (2/2)%%0001017/07/17 22:45:43 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:43 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.525 s%%0001017/07/17 22:45:43 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:45:43 INFO DAGScheduler: running: Set()%%0001017/07/17 22:45:43 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:45:43 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:45:43 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:45:43 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:45:43 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:45:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:45:43 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:45:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:42963 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:45:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:45:43 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:45:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc225, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc226, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc226:48101 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc225:49691 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:45:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc226:59302%%0001017/07/17 22:45:43 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:45:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc225:47955%%0001017/07/17 22:45:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1651 ms on hcdnc225 (1/2)%%0001017/07/17 22:45:45 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1689 ms on hcdnc226 (2/2)%%0001017/07/17 22:45:45 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:45 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.692 s%%0001017/07/17 22:45:45 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 26.251993 s%%0001017/07/17 22:45:45 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:45:45 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:45:45 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:45:45 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:45:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:45:45 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:45:45 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:45:45 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:45:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:45:45 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:45:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:45:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:42963 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:45:45 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:45:45 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:45:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc226, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:45 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc225, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc226:48101 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:45:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc225:49691 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:45:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3401 ms on hcdnc226 (1/2)%%0001017/07/17 22:45:48 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3434 ms on hcdnc225 (2/2)%%0001017/07/17 22:45:48 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.435 s%%0001017/07/17 22:45:48 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:48 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.447850 s%%0001017/07/17 22:45:48 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:45:48 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:45:48 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:45:48 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:45:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:45:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:45:48 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:45:48 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:45:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:45:48 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:45:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:42963 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:45:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:45:48 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:45:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc226, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:48 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc225, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc226:48101 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:45:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc225:49691 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3394 ms on hcdnc226 (1/2)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3445 ms on hcdnc225 (2/2)%%0001017/07/17 22:45:52 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:52 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.446 s%%0001017/07/17 22:45:52 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:45:52 INFO DAGScheduler: running: Set()%%0001017/07/17 22:45:52 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:45:52 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:45:52 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(1615) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1615.0 B, free 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:42963 (size: 1615.0 B, free: 530.2 MB)%%0001017/07/17 22:45:52 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:45:52 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc225, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc226, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc225:49691 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc226:48101 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc225:47955%%0001017/07/17 22:45:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 164 bytes%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc226:59302%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc225, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc226, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 45 ms on hcdnc225 (1/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 46 ms on hcdnc226 (2/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc225, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 16 ms on hcdnc225 (3/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc226, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 16 ms on hcdnc226 (4/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc225, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 18 ms on hcdnc225 (5/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc226, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 20 ms on hcdnc226 (6/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc225, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 16 ms on hcdnc225 (7/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc226, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 16 ms on hcdnc226 (8/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc225 (9/10)%%0001017/07/17 22:45:52 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc226 (10/10)%%0001017/07/17 22:45:52 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.104 s%%0001017/07/17 22:45:52 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:45:52 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.596025 s%%0001017/07/17 22:45:52 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:45:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:45:52 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:45:52 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:45:52 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:45:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:45:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(14344) called with curMem=288482, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.0 KB, free 530.0 MB)%%0001017/07/17 22:45:52 INFO MemoryStore: ensureFreeSpace(7913) called with curMem=302826, maxMem=556038881%%0001017/07/17 22:45:52 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:42963 (size: 7.7 KB, free: 530.2 MB)%%0001017/07/17 22:45:52 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:45:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:45:52 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc225, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:52 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc226, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc225:49691 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc226:48101 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc226:59302%%0001017/07/17 22:45:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc225:47955%%0001017/07/17 22:46:11 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 19156 ms on hcdnc225 (1/2)%%0001017/07/17 22:46:12 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 20130 ms on hcdnc226 (2/2)%%0001017/07/17 22:46:12 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:46:12 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 20.132 s%%0001017/07/17 22:46:12 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:46:12 INFO DAGScheduler: running: Set()%%0001017/07/17 22:46:12 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:46:12 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:46:12 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:46:12 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:46:12 INFO MemoryStore: ensureFreeSpace(17736) called with curMem=310739, maxMem=556038881%%0001017/07/17 22:46:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.3 KB, free 530.0 MB)%%0001017/07/17 22:46:12 INFO MemoryStore: ensureFreeSpace(9458) called with curMem=328475, maxMem=556038881%%0001017/07/17 22:46:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 530.0 MB)%%0001017/07/17 22:46:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:42963 (size: 9.2 KB, free: 530.2 MB)%%0001017/07/17 22:46:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:46:12 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:46:12 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:46:12 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc226, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:12 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc225, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc226:48101 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:46:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc225:49691 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:46:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc225:47955%%0001017/07/17 22:46:12 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 159 bytes%%0001017/07/17 22:46:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc226:59302%%0001017/07/17 22:46:13 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc225, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:13 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 681 ms on hcdnc225 (1/10)%%0001017/07/17 22:46:13 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc226, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:13 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 775 ms on hcdnc226 (2/10)%%0001017/07/17 22:46:13 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:46:13 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc225, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:13 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 659 ms on hcdnc225 (3/10)%%0001017/07/17 22:46:14 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc226, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:14 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 704 ms on hcdnc226 (4/10)%%0001017/07/17 22:46:14 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc225, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:14 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 611 ms on hcdnc225 (5/10)%%0001017/07/17 22:46:14 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:46:14 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc226, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:14 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 605 ms on hcdnc226 (6/10)%%0001017/07/17 22:46:15 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc225, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 618 ms on hcdnc225 (7/10)%%0001017/07/17 22:46:15 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc226, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 629 ms on hcdnc226 (8/10)%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 687 ms on hcdnc225 (9/10)%%0001017/07/17 22:46:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://driverPropsFetcher@hcdnc322:41546]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc322:41546] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc322:41546%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:15 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 633 ms on hcdnc226 (10/10)%%0001017/07/17 22:46:15 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:46:15 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.342 s%%0001017/07/17 22:46:15 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 23.501468 s%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:46:16 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:46:16 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:46:16 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:46:16 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:46:16 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://sparkExecutor@hcdnc226:59302]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc226:59302] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc226:59302%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:16 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:44514] &lt;- [akka.tcp://sparkExecutor@hcdnc225:47955]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc225:47955] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc225:47955%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:46:16 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:46:16 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:46:16 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:46:16 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:46:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:46:16 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:46:16 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:46:16 INFO Remoting: Remoting shut down%%0001017/07/17 22:46:16 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:46:17 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:46:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-855cc671-eae5-42a4-9ab3-70a7ed3fc949%%00010"/>
</config>
</config>
<config key="2">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:46:32 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:46:32 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:46:32 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:46:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:46:33 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:46:33 INFO Remoting: Starting remoting%%0001017/07/17 22:46:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:39426]%%0001017/07/17 22:46:33 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:39426]%%0001017/07/17 22:46:33 INFO Utils: Successfully started service 'sparkDriver' on port 39426.%%0001017/07/17 22:46:33 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:46:33 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:46:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0620ce98-5b62-4e9e-b5a6-900b4b0d4832%%0001017/07/17 22:46:33 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:46:34 INFO HttpFileServer: HTTP File server directory is /tmp/spark-77484cdc-4537-471a-95e0-96364a408f51/httpd-3e63628d-0c51-4c69-b91b-117e5139bb30%%0001017/07/17 22:46:34 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:46:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:46:34 INFO AbstractConnector: Started SocketConnector@0.0.0.0:39062%%0001017/07/17 22:46:34 INFO Utils: Successfully started service 'HTTP file server' on port 39062.%%0001017/07/17 22:46:34 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:46:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:46:34 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:46:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:46:34 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:46:34 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:46:34 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:46:34 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:46:34 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:46:34 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:46:34 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:46:34 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:46:34 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:46:35 INFO Client: Uploading resource file:/tmp/spark-77484cdc-4537-471a-95e0-96364a408f51/__spark_conf__2654259951614239138.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22073/__spark_conf__2654259951614239138.zip%%0001017/07/17 22:46:36 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:46:36 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:46:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:46:36 INFO Client: Submitting application 22073 to ResourceManager%%0001017/07/17 22:46:36 INFO YarnClientImpl: Submitted application application_1491786134915_22073%%0001017/07/17 22:46:37 INFO Client: Application report for application_1491786134915_22073 (state: ACCEPTED)%%0001017/07/17 22:46:37 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302796136%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22073/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:46:38 INFO Client: Application report for application_1491786134915_22073 (state: ACCEPTED)%%0001017/07/17 22:46:39 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:59404/user/YarnAM#-1588046408])%%0001017/07/17 22:46:39 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22073,http://hcnnc117:8088/proxy/application_1491786134915_22073), /proxy/application_1491786134915_22073%%0001017/07/17 22:46:39 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:46:39 INFO Client: Application report for application_1491786134915_22073 (state: RUNNING)%%0001017/07/17 22:46:39 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302796136%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22073/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:46:39 INFO YarnClientSchedulerBackend: Application application_1491786134915_22073 has started running.%%0001017/07/17 22:46:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35694.%%0001017/07/17 22:46:39 INFO NettyBlockTransferService: Server created on 35694%%0001017/07/17 22:46:39 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:46:39 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:46:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:35694 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 35694)%%0001017/07/17 22:46:39 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:46:39 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22073%%0001017/07/17 22:46:39 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:35694 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:46:40 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:46:40 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.6:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.19:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.25:50010%%0001017/07/17 22:46:40 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.26:50010%%0001017/07/17 22:46:40 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:46:40 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:46:40 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:46:40 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:46:40 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:46:40 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:46:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:46:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:35694 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:46:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:46:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:46:40 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:46:41 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:46:42 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:46:43 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://driverPropsFetcher@hcdnc332:54198]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc332:54198] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc332:54198%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:44 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc332:34164/user/Executor#1577283137]) with ID 1%%0001017/07/17 22:46:44 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:46:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc332, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:46:44 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc332:49250 with 530.0 MB RAM, BlockManagerId(1, hcdnc332, 49250)%%0001017/07/17 22:46:44 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://driverPropsFetcher@hcdnc230:56790]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:56790] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc230:56790%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:46:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc332:49250 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:46:44 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc230:42966/user/Executor#-717777621]) with ID 2%%0001017/07/17 22:46:44 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:46:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc230, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:46:45 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc230:49662 with 530.0 MB RAM, BlockManagerId(2, hcdnc230, 49662)%%0001017/07/17 22:46:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc332:49250 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:46:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc230:49662 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:46:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc230:49662 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:46:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 14856 ms on hcdnc332 (1/2)%%0001017/07/17 22:46:59 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14813 ms on hcdnc230 (2/2)%%0001017/07/17 22:46:59 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.348 s%%0001017/07/17 22:46:59 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:46:59 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 19.515972 s%%0001017/07/17 22:46:59 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:46:59 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:46:59 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:46:59 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:46:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:46:59 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:46:59 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:46:59 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:46:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:46:59 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:46:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:46:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:35694 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:46:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:46:59 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:46:59 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:46:59 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, hcdnc230, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:46:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, hcdnc332, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:46:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc332:49250 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:46:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc230:49662 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:47:20 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 20281 ms on hcdnc230 (1/2)%%0001017/07/17 22:47:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 20425 ms on hcdnc332 (2/2)%%0001017/07/17 22:47:20 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:20 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 20.427 s%%0001017/07/17 22:47:20 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:47:20 INFO DAGScheduler: running: Set()%%0001017/07/17 22:47:20 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:47:20 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:47:20 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:47:20 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:47:20 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:47:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:47:20 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:47:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:47:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:35694 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:47:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:47:20 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:47:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc230, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:20 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc332, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc230:49662 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:47:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc332:49250 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:47:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc230:42966%%0001017/07/17 22:47:20 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:47:20 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc332:34164%%0001017/07/17 22:47:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1674 ms on hcdnc230 (1/2)%%0001017/07/17 22:47:22 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1705 ms on hcdnc332 (2/2)%%0001017/07/17 22:47:22 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:22 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.708 s%%0001017/07/17 22:47:22 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 22.167477 s%%0001017/07/17 22:47:22 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:47:22 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:47:22 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:47:22 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:47:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:47:22 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:47:22 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:47:22 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:47:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:47:22 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:47:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:47:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:35694 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:47:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:47:22 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:47:22 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc230, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:22 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc332, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc332:49250 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:47:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc230:49662 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:47:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3447 ms on hcdnc230 (1/2)%%0001017/07/17 22:47:25 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3561 ms on hcdnc332 (2/2)%%0001017/07/17 22:47:25 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.562 s%%0001017/07/17 22:47:25 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:25 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.575781 s%%0001017/07/17 22:47:25 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:47:25 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:47:25 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:47:25 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:47:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:47:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:47:25 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:47:25 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:47:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:47:25 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:47:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:47:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:35694 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:47:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:25 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:47:25 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:47:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc332, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:25 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc230, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc332:49250 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:47:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc230:49662 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3406 ms on hcdnc332 (1/2)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3621 ms on hcdnc230 (2/2)%%0001017/07/17 22:47:29 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:29 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.622 s%%0001017/07/17 22:47:29 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:47:29 INFO DAGScheduler: running: Set()%%0001017/07/17 22:47:29 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:47:29 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:47:29 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(1615) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1615.0 B, free 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:35694 (size: 1615.0 B, free: 530.2 MB)%%0001017/07/17 22:47:29 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:47:29 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc332, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc230, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc230:49662 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc332:49250 (size: 1615.0 B, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc230:42966%%0001017/07/17 22:47:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 166 bytes%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc332:34164%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc230, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 45 ms on hcdnc230 (1/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc332, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 50 ms on hcdnc332 (2/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc230, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 14 ms on hcdnc230 (3/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc332, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 18 ms on hcdnc332 (4/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc230, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc230 (5/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc332, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 17 ms on hcdnc332 (6/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc230, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 14 ms on hcdnc230 (7/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc332, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 14 ms on hcdnc332 (8/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc230 (9/10)%%0001017/07/17 22:47:29 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 10 ms on hcdnc332 (10/10)%%0001017/07/17 22:47:29 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.103 s%%0001017/07/17 22:47:29 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:29 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.744018 s%%0001017/07/17 22:47:29 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:47:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:47:29 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:47:29 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:47:29 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:47:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:47:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(14344) called with curMem=288482, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.0 KB, free 530.0 MB)%%0001017/07/17 22:47:29 INFO MemoryStore: ensureFreeSpace(7912) called with curMem=302826, maxMem=556038881%%0001017/07/17 22:47:29 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.7 KB, free 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:35694 (size: 7.7 KB, free: 530.2 MB)%%0001017/07/17 22:47:29 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:29 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:47:29 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc332, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:29 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc230, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc332:49250 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc230:49662 (size: 7.7 KB, free: 530.0 MB)%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc332:34164%%0001017/07/17 22:47:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc230:42966%%0001017/07/17 22:47:48 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 18763 ms on hcdnc332 (1/2)%%0001017/07/17 22:47:50 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 20507 ms on hcdnc230 (2/2)%%0001017/07/17 22:47:50 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:50 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 20.509 s%%0001017/07/17 22:47:50 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:47:50 INFO DAGScheduler: running: Set()%%0001017/07/17 22:47:50 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:47:50 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:47:50 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:47:50 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:47:50 INFO MemoryStore: ensureFreeSpace(17736) called with curMem=310738, maxMem=556038881%%0001017/07/17 22:47:50 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.3 KB, free 530.0 MB)%%0001017/07/17 22:47:50 INFO MemoryStore: ensureFreeSpace(9458) called with curMem=328474, maxMem=556038881%%0001017/07/17 22:47:50 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 530.0 MB)%%0001017/07/17 22:47:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:35694 (size: 9.2 KB, free: 530.2 MB)%%0001017/07/17 22:47:50 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:47:50 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:47:50 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:47:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc230, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:50 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc332, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc230:49662 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:47:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc332:49250 (size: 9.2 KB, free: 529.9 MB)%%0001017/07/17 22:47:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc230:42966%%0001017/07/17 22:47:50 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 161 bytes%%0001017/07/17 22:47:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc332:34164%%0001017/07/17 22:47:50 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc230, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:50 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 818 ms on hcdnc230 (1/10)%%0001017/07/17 22:47:51 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:47:51 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc332, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:51 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 1037 ms on hcdnc332 (2/10)%%0001017/07/17 22:47:51 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc230, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:51 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 759 ms on hcdnc230 (3/10)%%0001017/07/17 22:47:51 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc332, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:51 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 734 ms on hcdnc332 (4/10)%%0001017/07/17 22:47:52 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:47:52 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc230, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:52 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 594 ms on hcdnc230 (5/10)%%0001017/07/17 22:47:52 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc332, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:52 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 650 ms on hcdnc332 (6/10)%%0001017/07/17 22:47:52 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc230, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:52 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 634 ms on hcdnc230 (7/10)%%0001017/07/17 22:47:53 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc332, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:47:53 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 590 ms on hcdnc332 (8/10)%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://driverPropsFetcher@hcdnc838:57707]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc838:57707] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc838:57707%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 706 ms on hcdnc230 (9/10)%%0001017/07/17 22:47:53 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 612 ms on hcdnc332 (10/10)%%0001017/07/17 22:47:53 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:47:53 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.620 s%%0001017/07/17 22:47:53 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 24.155877 s%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc838:55357/user/Executor#940631458]) with ID 3%%0001017/07/17 22:47:53 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:47:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:47:53 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc838:41966 with 530.0 MB RAM, BlockManagerId(3, hcdnc838, 41966)%%0001017/07/17 22:47:53 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:47:53 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:47:53 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://sparkExecutor@hcdnc230:42966]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc230:42966] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc230:42966%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://sparkExecutor@hcdnc332:34164]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc332:34164] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc332:34164%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:39426] &lt;- [akka.tcp://sparkExecutor@hcdnc838:55357]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc838:55357] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc838:55357%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:47:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:47:53 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:47:53 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:47:53 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:47:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:47:53 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:47:53 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:47:53 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:47:54 INFO Remoting: Remoting shut down%%0001017/07/17 22:47:54 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:47:54 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:47:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-77484cdc-4537-471a-95e0-96364a408f51/pyspark-1825413e-60a9-4db9-9397-984396df8b31%%0001017/07/17 22:47:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-77484cdc-4537-471a-95e0-96364a408f51%%00010"/>
</config>
</config>
<config key="3">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:48:08 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:48:09 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:48:09 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:48:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:48:10 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:48:10 INFO Remoting: Starting remoting%%0001017/07/17 22:48:10 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:46080]%%0001017/07/17 22:48:10 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:46080]%%0001017/07/17 22:48:10 INFO Utils: Successfully started service 'sparkDriver' on port 46080.%%0001017/07/17 22:48:10 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:48:10 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:48:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9c3176ac-3718-4e0c-942d-61475e852842%%0001017/07/17 22:48:10 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:48:10 INFO HttpFileServer: HTTP File server directory is /tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272/httpd-0ffd8150-fdd1-4cb5-a506-e0cd34ca09dc%%0001017/07/17 22:48:10 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:48:10 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:48:10 INFO AbstractConnector: Started SocketConnector@0.0.0.0:36943%%0001017/07/17 22:48:10 INFO Utils: Successfully started service 'HTTP file server' on port 36943.%%0001017/07/17 22:48:10 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:48:10 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:48:10 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:48:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:48:10 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:48:10 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:48:11 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:48:11 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:48:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:48:11 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:48:11 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:48:11 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:48:11 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:48:11 INFO Client: Uploading resource file:/tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272/__spark_conf__728497682939853107.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22074/__spark_conf__728497682939853107.zip%%0001017/07/17 22:48:12 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:48:12 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:48:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:48:12 INFO Client: Submitting application 22074 to ResourceManager%%0001017/07/17 22:48:12 INFO YarnClientImpl: Submitted application application_1491786134915_22074%%0001017/07/17 22:48:13 INFO Client: Application report for application_1491786134915_22074 (state: ACCEPTED)%%0001017/07/17 22:48:13 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302892050%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22074/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:48:14 INFO Client: Application report for application_1491786134915_22074 (state: ACCEPTED)%%0001017/07/17 22:48:15 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:43563/user/YarnAM#-508022274])%%0001017/07/17 22:48:15 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22074,http://hcnnc117:8088/proxy/application_1491786134915_22074), /proxy/application_1491786134915_22074%%0001017/07/17 22:48:15 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:48:15 INFO Client: Application report for application_1491786134915_22074 (state: RUNNING)%%0001017/07/17 22:48:15 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302892050%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22074/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:48:15 INFO YarnClientSchedulerBackend: Application application_1491786134915_22074 has started running.%%0001017/07/17 22:48:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40511.%%0001017/07/17 22:48:15 INFO NettyBlockTransferService: Server created on 40511%%0001017/07/17 22:48:15 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:48:15 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:48:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:40511 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 40511)%%0001017/07/17 22:48:15 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:48:16 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22074%%0001017/07/17 22:48:16 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:40511 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:48:16 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:48:16 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.17:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.28:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.4:50010%%0001017/07/17 22:48:16 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.19:50010%%0001017/07/17 22:48:16 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:48:16 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:48:16 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:48:16 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:48:16 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:48:16 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:48:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:48:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:40511 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:48:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:48:16 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:48:17 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:48:18 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:48:19 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:48:20 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://driverPropsFetcher@hcdnc404:40878]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc404:40878] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc404:40878%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:48:20 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc404:54207/user/Executor#1152446352]) with ID 1%%0001017/07/17 22:48:21 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:48:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc404, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:48:21 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc404:52186 with 530.0 MB RAM, BlockManagerId(1, hcdnc404, 52186)%%0001017/07/17 22:48:21 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://driverPropsFetcher@hcdnc421:43477]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc421:43477] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc421:43477%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:48:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc404:52186 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:48:21 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc421:36227/user/Executor#329786119]) with ID 2%%0001017/07/17 22:48:21 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:48:21 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc421:36055 with 530.0 MB RAM, BlockManagerId(2, hcdnc421, 36055)%%0001017/07/17 22:48:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc404:52186 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:48:24 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc421, partition 1,ANY, 2147 bytes)%%0001017/07/17 22:48:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc421:36055 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:48:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc421:36055 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:48:38 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14683 ms on hcdnc421 (1/2)%%0001017/07/17 22:48:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 20286 ms on hcdnc404 (2/2)%%0001017/07/17 22:48:41 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 24.183 s%%0001017/07/17 22:48:41 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:48:41 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 24.351231 s%%0001017/07/17 22:48:41 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:48:41 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:48:41 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:48:41 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:48:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:48:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:48:41 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:48:41 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:48:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:48:41 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:48:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:48:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:40511 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:48:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:48:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:48:41 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:48:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc404, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:48:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc404:52186 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:48:42 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:48:45 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc421, partition 1,ANY, 2136 bytes)%%0001017/07/17 22:48:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc421:36055 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:49:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21077 ms on hcdnc404 (1/2)%%0001017/07/17 22:49:05 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20190 ms on hcdnc421 (2/2)%%0001017/07/17 22:49:05 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:05 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.080 s%%0001017/07/17 22:49:05 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:49:05 INFO DAGScheduler: running: Set()%%0001017/07/17 22:49:05 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:49:05 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:49:05 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:49:05 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:49:05 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:49:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:49:05 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:49:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:49:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:40511 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:49:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:49:05 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:49:05 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc404, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:05 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc421, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc421:36055 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:49:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc404:52186 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:49:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc421:36227%%0001017/07/17 22:49:05 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:49:05 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc404:54207%%0001017/07/17 22:49:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1619 ms on hcdnc404 (1/2)%%0001017/07/17 22:49:07 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1742 ms on hcdnc421 (2/2)%%0001017/07/17 22:49:07 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:07 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.743 s%%0001017/07/17 22:49:07 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 25.856648 s%%0001017/07/17 22:49:07 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:49:07 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:49:07 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:49:07 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:49:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:49:07 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:49:07 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:49:07 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:49:07 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:49:07 INFO MemoryStore: ensureFreeSpace(3903) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:49:07 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:49:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:40511 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:49:07 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:49:07 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:49:07 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc421, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:07 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc404, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc421:36055 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:49:07 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc404:52186 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:49:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3339 ms on hcdnc421 (1/2)%%0001017/07/17 22:49:10 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3472 ms on hcdnc404 (2/2)%%0001017/07/17 22:49:10 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:10 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.473 s%%0001017/07/17 22:49:10 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.486533 s%%0001017/07/17 22:49:10 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:49:10 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:49:10 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:49:10 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:49:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:49:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:49:10 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:49:10 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272759, maxMem=556038881%%0001017/07/17 22:49:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:49:10 INFO MemoryStore: ensureFreeSpace(4151) called with curMem=279935, maxMem=556038881%%0001017/07/17 22:49:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:49:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:40511 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:49:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:49:10 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:49:10 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc421, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:10 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc404, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc404:52186 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:49:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc421:36055 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3308 ms on hcdnc421 (1/2)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3489 ms on hcdnc404 (2/2)%%0001017/07/17 22:49:14 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:14 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.490 s%%0001017/07/17 22:49:14 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:49:14 INFO DAGScheduler: running: Set()%%0001017/07/17 22:49:14 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:49:14 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:49:14 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284086, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286870, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:40511 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:49:14 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:49:14 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc421, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc404, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc404:52186 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc421:36055 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc404:54207%%0001017/07/17 22:49:14 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 165 bytes%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc421:36227%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc404, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 42 ms on hcdnc404 (1/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc421, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 50 ms on hcdnc421 (2/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc404, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 15 ms on hcdnc404 (3/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc421, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 18 ms on hcdnc421 (4/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc404, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 18 ms on hcdnc404 (5/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc421, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 19 ms on hcdnc421 (6/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc404, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc404 (7/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc421, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc421 (8/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc404 (9/10)%%0001017/07/17 22:49:14 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc421 (10/10)%%0001017/07/17 22:49:14 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.110 s%%0001017/07/17 22:49:14 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:14 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.639900 s%%0001017/07/17 22:49:14 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:49:14 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:49:14 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:49:14 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:49:14 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:49:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:49:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(13320) called with curMem=288483, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.0 KB, free 530.0 MB)%%0001017/07/17 22:49:14 INFO MemoryStore: ensureFreeSpace(7335) called with curMem=301803, maxMem=556038881%%0001017/07/17 22:49:14 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:40511 (size: 7.2 KB, free: 530.2 MB)%%0001017/07/17 22:49:14 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:14 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:49:14 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc404, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:14 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc421, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc421:36055 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc404:52186 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc404:54207%%0001017/07/17 22:49:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc421:36227%%0001017/07/17 22:49:31 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 17305 ms on hcdnc404 (1/2)%%0001017/07/17 22:49:33 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 18866 ms on hcdnc421 (2/2)%%0001017/07/17 22:49:33 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:33 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 18.868 s%%0001017/07/17 22:49:33 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:49:33 INFO DAGScheduler: running: Set()%%0001017/07/17 22:49:33 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:49:33 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:49:33 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:49:33 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:49:33 INFO MemoryStore: ensureFreeSpace(16480) called with curMem=309138, maxMem=556038881%%0001017/07/17 22:49:33 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.1 KB, free 530.0 MB)%%0001017/07/17 22:49:33 INFO MemoryStore: ensureFreeSpace(8726) called with curMem=325618, maxMem=556038881%%0001017/07/17 22:49:33 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:49:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:40511 (size: 8.5 KB, free: 530.2 MB)%%0001017/07/17 22:49:33 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:49:33 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:49:33 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:49:33 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc404, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:33 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc421, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc421:36055 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:49:33 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc404:52186 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:49:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc421:36227%%0001017/07/17 22:49:33 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 157 bytes%%0001017/07/17 22:49:33 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc404:54207%%0001017/07/17 22:49:34 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc421, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:34 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 980 ms on hcdnc421 (1/10)%%0001017/07/17 22:49:34 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc404, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:34 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 1026 ms on hcdnc404 (2/10)%%0001017/07/17 22:49:34 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:49:34 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc421, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:34 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 615 ms on hcdnc421 (3/10)%%0001017/07/17 22:49:35 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc404, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:35 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 770 ms on hcdnc404 (4/10)%%0001017/07/17 22:49:35 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:49:35 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc421, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:35 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 602 ms on hcdnc421 (5/10)%%0001017/07/17 22:49:35 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc404, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:35 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 639 ms on hcdnc404 (6/10)%%0001017/07/17 22:49:36 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc421, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:36 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 560 ms on hcdnc421 (7/10)%%0001017/07/17 22:49:36 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc404, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:49:36 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 704 ms on hcdnc404 (8/10)%%0001017/07/17 22:49:36 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://driverPropsFetcher@hcdnc412:55440]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc412:55440] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc412:55440%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:36 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 625 ms on hcdnc421 (9/10)%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc412:41395/user/Executor#-204020692]) with ID 3%%0001017/07/17 22:49:37 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:49:37 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 584 ms on hcdnc404 (10/10)%%0001017/07/17 22:49:37 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.719 s%%0001017/07/17 22:49:37 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:49:37 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 22.614461 s%%0001017/07/17 22:49:37 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc412:58622 with 530.0 MB RAM, BlockManagerId(3, hcdnc412, 58622)%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:49:37 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:49:37 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:49:37 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:49:37 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:49:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://sparkExecutor@hcdnc404:54207]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc404:54207] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc404:54207%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://sparkExecutor@hcdnc421:36227]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc421:36227] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc421:36227%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:37 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46080] &lt;- [akka.tcp://sparkExecutor@hcdnc412:41395]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc412:41395] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc412:41395%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:49:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:49:37 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:49:37 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:49:37 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:49:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:49:37 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:49:37 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:49:37 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:49:37 INFO Remoting: Remoting shut down%%0001017/07/17 22:49:37 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:49:38 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272/pyspark-48d3e9e9-4600-40d1-b570-bcd32407f90f%%0001017/07/17 22:49:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-e987e2b3-2827-440e-b397-a23980a0d272%%00010"/>
</config>
</config>
<config key="4">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:49:53 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:49:53 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:49:53 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:49:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:49:54 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:49:54 INFO Remoting: Starting remoting%%0001017/07/17 22:49:54 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:43851]%%0001017/07/17 22:49:54 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:43851]%%0001017/07/17 22:49:54 INFO Utils: Successfully started service 'sparkDriver' on port 43851.%%0001017/07/17 22:49:54 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:49:54 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:49:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7a9ab765-6cf4-4a67-8f1c-66903ebfa6c4%%0001017/07/17 22:49:54 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:49:54 INFO HttpFileServer: HTTP File server directory is /tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d/httpd-acf507e0-30ab-4959-a3bd-ccfbb28b26e8%%0001017/07/17 22:49:54 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:49:54 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:49:54 INFO AbstractConnector: Started SocketConnector@0.0.0.0:36646%%0001017/07/17 22:49:54 INFO Utils: Successfully started service 'HTTP file server' on port 36646.%%0001017/07/17 22:49:54 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:49:54 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:49:55 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:49:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:49:55 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:49:55 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:49:55 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:49:56 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:49:56 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:49:56 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:49:56 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:49:56 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:49:56 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:49:56 INFO Client: Uploading resource file:/tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d/__spark_conf__4486679632002301877.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22075/__spark_conf__4486679632002301877.zip%%0001017/07/17 22:49:56 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:49:56 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:49:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:49:56 INFO Client: Submitting application 22075 to ResourceManager%%0001017/07/17 22:49:57 INFO YarnClientImpl: Submitted application application_1491786134915_22075%%0001017/07/17 22:49:58 INFO Client: Application report for application_1491786134915_22075 (state: ACCEPTED)%%0001017/07/17 22:49:58 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302996865%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22075/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:49:59 INFO Client: Application report for application_1491786134915_22075 (state: ACCEPTED)%%0001017/07/17 22:50:00 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:48159/user/YarnAM#859327838])%%0001017/07/17 22:50:00 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22075,http://hcnnc117:8088/proxy/application_1491786134915_22075), /proxy/application_1491786134915_22075%%0001017/07/17 22:50:00 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:50:00 INFO Client: Application report for application_1491786134915_22075 (state: ACCEPTED)%%0001017/07/17 22:50:01 INFO Client: Application report for application_1491786134915_22075 (state: RUNNING)%%0001017/07/17 22:50:01 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500302996865%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22075/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:50:01 INFO YarnClientSchedulerBackend: Application application_1491786134915_22075 has started running.%%0001017/07/17 22:50:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39985.%%0001017/07/17 22:50:01 INFO NettyBlockTransferService: Server created on 39985%%0001017/07/17 22:50:01 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:50:01 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:50:01 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:39985 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 39985)%%0001017/07/17 22:50:01 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:50:01 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22075%%0001017/07/17 22:50:01 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:50:01 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:50:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:50:01 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:50:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:50:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:39985 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:50:01 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:50:02 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.21:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.33:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.27:50010%%0001017/07/17 22:50:02 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.31:50010%%0001017/07/17 22:50:02 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:50:02 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:50:02 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:50:02 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:50:02 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:50:02 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:50:02 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:50:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:50:02 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:50:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:50:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:39985 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:50:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:50:02 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:50:03 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:50:04 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:50:05 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://driverPropsFetcher@hcdnc406:37566]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc406:37566] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc406:37566%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:50:06 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc406:44474/user/Executor#412856414]) with ID 1%%0001017/07/17 22:50:06 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:50:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0, hcdnc406, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:50:06 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc406:48356 with 530.0 MB RAM, BlockManagerId(1, hcdnc406, 48356)%%0001017/07/17 22:50:06 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://driverPropsFetcher@hcdnc823:49248]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:49248] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:49248%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:50:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc406:48356 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:06 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc823:48321/user/Executor#-1348166762]) with ID 2%%0001017/07/17 22:50:06 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:50:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1, hcdnc823, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:50:06 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc823:59036 with 530.0 MB RAM, BlockManagerId(2, hcdnc823, 59036)%%0001017/07/17 22:50:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc406:48356 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:50:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc823:59036 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc823:59036 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:50:21 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 14868 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 15142 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:21 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.785 s%%0001017/07/17 22:50:21 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:22 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 19.948811 s%%0001017/07/17 22:50:22 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:50:22 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:50:22 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:50:22 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:50:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:50:22 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:50:22 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:50:22 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:50:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:50:22 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:50:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:50:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:39985 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:50:22 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:22 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:50:22 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:50:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc823, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:50:22 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc406, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:50:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc823:59036 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc406:48356 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20313 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21019 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:43 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.021 s%%0001017/07/17 22:50:43 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:43 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:50:43 INFO DAGScheduler: running: Set()%%0001017/07/17 22:50:43 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:50:43 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:50:43 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:50:43 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:50:43 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:50:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:50:43 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:50:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:50:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:39985 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:50:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:50:43 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:50:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc406, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc823, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc823:59036 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc406:48356 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:50:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:48321%%0001017/07/17 22:50:43 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:50:43 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc406:44474%%0001017/07/17 22:50:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1605 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:44 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1689 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:44 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:44 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.692 s%%0001017/07/17 22:50:44 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 22.743526 s%%0001017/07/17 22:50:44 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:50:44 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:50:44 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:50:44 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:50:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:50:44 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:50:44 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:50:44 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:50:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:50:44 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:50:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:50:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:39985 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:50:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:44 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:50:44 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:50:44 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:44 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc406, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc406:48356 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:50:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc823:59036 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:50:48 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3395 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3439 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:48 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.439 s%%0001017/07/17 22:50:48 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:48 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.452480 s%%0001017/07/17 22:50:48 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:50:48 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:50:48 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:50:48 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:50:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:50:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:50:48 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:50:48 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:50:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:50:48 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:50:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:50:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:39985 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:50:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:50:48 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:50:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc406, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:48 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc406:48356 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:50:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc823:59036 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3381 ms on hcdnc406 (1/2)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3587 ms on hcdnc823 (2/2)%%0001017/07/17 22:50:51 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:51 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.588 s%%0001017/07/17 22:50:51 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:50:51 INFO DAGScheduler: running: Set()%%0001017/07/17 22:50:51 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:50:51 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:50:51 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:50:51 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:50:51 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:50:51 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:50:51 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:50:51 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:50:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:39985 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:50:51 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:51 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:50:51 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc406, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc823:59036 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:50:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc406:48356 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:50:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc823:48321%%0001017/07/17 22:50:51 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 166 bytes%%0001017/07/17 22:50:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc406:44474%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc823, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 44 ms on hcdnc823 (1/10)%%0001017/07/17 22:50:51 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc406, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:51 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 48 ms on hcdnc406 (2/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc823, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 14 ms on hcdnc823 (3/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc406, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc406 (4/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc823, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 16 ms on hcdnc823 (5/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc406, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc823, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 21 ms on hcdnc406 (6/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc823 (7/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc406, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc406 (8/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc823 (9/10)%%0001017/07/17 22:50:52 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc406 (10/10)%%0001017/07/17 22:50:52 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.108 s%%0001017/07/17 22:50:52 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:50:52 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.743436 s%%0001017/07/17 22:50:52 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:50:52 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:50:52 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:50:52 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:50:52 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:50:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:50:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:50:52 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:50:52 INFO MemoryStore: ensureFreeSpace(13320) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:50:52 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.0 KB, free 530.0 MB)%%0001017/07/17 22:50:52 INFO MemoryStore: ensureFreeSpace(7329) called with curMem=301800, maxMem=556038881%%0001017/07/17 22:50:52 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 530.0 MB)%%0001017/07/17 22:50:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:39985 (size: 7.2 KB, free: 530.2 MB)%%0001017/07/17 22:50:52 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:50:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:50:52 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc406, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:52 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:50:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc406:48356 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:52 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc823:59036 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:50:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc406:44474%%0001017/07/17 22:50:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:48321%%0001017/07/17 22:51:08 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 16308 ms on hcdnc406 (1/2)%%0001017/07/17 22:51:11 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 19680 ms on hcdnc823 (2/2)%%0001017/07/17 22:51:11 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:51:11 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 19.682 s%%0001017/07/17 22:51:11 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:51:11 INFO DAGScheduler: running: Set()%%0001017/07/17 22:51:11 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:51:11 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:51:11 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:51:11 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:51:11 INFO MemoryStore: ensureFreeSpace(16480) called with curMem=309129, maxMem=556038881%%0001017/07/17 22:51:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.1 KB, free 530.0 MB)%%0001017/07/17 22:51:11 INFO MemoryStore: ensureFreeSpace(8730) called with curMem=325609, maxMem=556038881%%0001017/07/17 22:51:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:51:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:39985 (size: 8.5 KB, free: 530.2 MB)%%0001017/07/17 22:51:11 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:51:11 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:51:11 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:51:11 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:11 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc406, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc823:59036 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:51:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc406:48356 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:51:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc823:48321%%0001017/07/17 22:51:11 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 158 bytes%%0001017/07/17 22:51:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc406:44474%%0001017/07/17 22:51:12 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc406, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:12 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 798 ms on hcdnc406 (1/10)%%0001017/07/17 22:51:12 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc823, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:12 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 884 ms on hcdnc823 (2/10)%%0001017/07/17 22:51:12 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:51:13 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc406, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:13 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 796 ms on hcdnc406 (3/10)%%0001017/07/17 22:51:13 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc823, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:13 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 805 ms on hcdnc823 (4/10)%%0001017/07/17 22:51:13 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:51:13 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc406, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:13 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 558 ms on hcdnc406 (5/10)%%0001017/07/17 22:51:14 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc823, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:14 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 797 ms on hcdnc823 (6/10)%%0001017/07/17 22:51:14 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc406, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:14 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 504 ms on hcdnc406 (7/10)%%0001017/07/17 22:51:14 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc823, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:51:14 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 582 ms on hcdnc823 (8/10)%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://driverPropsFetcher@hcdnc217:58309]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc217:58309] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc217:58309%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 620 ms on hcdnc406 (9/10)%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc217:48862/user/Executor#896528954]) with ID 3%%0001017/07/17 22:51:15 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:51:15 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc217:60710 with 530.0 MB RAM, BlockManagerId(3, hcdnc217, 60710)%%0001017/07/17 22:51:15 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 568 ms on hcdnc823 (10/10)%%0001017/07/17 22:51:15 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:51:15 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.633 s%%0001017/07/17 22:51:15 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 23.341989 s%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:51:15 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:51:15 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:51:15 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:51:15 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://sparkExecutor@hcdnc217:48862]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc217:48862] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc217:48862%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://sparkExecutor@hcdnc823:48321]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc823:48321] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc823:48321%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:43851] &lt;- [akka.tcp://sparkExecutor@hcdnc406:44474]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc406:44474] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc406:44474%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:51:15 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:51:15 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:51:15 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:51:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:51:15 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:51:15 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:51:15 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:51:15 INFO Remoting: Remoting shut down%%0001017/07/17 22:51:15 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:51:16 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:51:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d/pyspark-fb8538d5-418f-4a8d-b902-f5dd1aec3172%%0001017/07/17 22:51:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-640cbf44-b027-4d4b-b024-f7cc437ae89d%%00010"/>
</config>
</config>
<config key="5">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:51:32 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:51:32 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:51:32 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:51:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:51:33 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:51:33 INFO Remoting: Starting remoting%%0001017/07/17 22:51:33 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:42112]%%0001017/07/17 22:51:33 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:42112]%%0001017/07/17 22:51:33 INFO Utils: Successfully started service 'sparkDriver' on port 42112.%%0001017/07/17 22:51:33 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:51:33 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:51:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c81417c4-353a-44af-ae45-99fdd06fdf0d%%0001017/07/17 22:51:33 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:51:34 INFO HttpFileServer: HTTP File server directory is /tmp/spark-76e3aa34-9463-4bf6-8a0a-0b38288ab660/httpd-8fda9927-cc88-4aa5-89a0-8794ea3d7938%%0001017/07/17 22:51:34 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:51:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:51:34 INFO AbstractConnector: Started SocketConnector@0.0.0.0:40037%%0001017/07/17 22:51:34 INFO Utils: Successfully started service 'HTTP file server' on port 40037.%%0001017/07/17 22:51:34 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:51:34 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:51:34 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:51:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:51:34 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:51:34 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:51:35 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:51:35 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:51:35 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:51:35 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:51:35 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:51:35 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:51:35 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:51:35 INFO Client: Uploading resource file:/tmp/spark-76e3aa34-9463-4bf6-8a0a-0b38288ab660/__spark_conf__4891555663197474770.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22076/__spark_conf__4891555663197474770.zip%%0001017/07/17 22:51:35 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:51:35 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:51:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:51:35 INFO Client: Submitting application 22076 to ResourceManager%%0001017/07/17 22:51:36 INFO YarnClientImpl: Submitted application application_1491786134915_22076%%0001017/07/17 22:51:37 INFO Client: Application report for application_1491786134915_22076 (state: ACCEPTED)%%0001017/07/17 22:51:37 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303096001%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22076/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:51:38 INFO Client: Application report for application_1491786134915_22076 (state: ACCEPTED)%%0001017/07/17 22:51:39 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:49556/user/YarnAM#-836501001])%%0001017/07/17 22:51:39 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22076,http://hcnnc117:8088/proxy/application_1491786134915_22076), /proxy/application_1491786134915_22076%%0001017/07/17 22:51:39 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:51:39 INFO Client: Application report for application_1491786134915_22076 (state: RUNNING)%%0001017/07/17 22:51:39 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303096001%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22076/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:51:39 INFO YarnClientSchedulerBackend: Application application_1491786134915_22076 has started running.%%0001017/07/17 22:51:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39013.%%0001017/07/17 22:51:39 INFO NettyBlockTransferService: Server created on 39013%%0001017/07/17 22:51:39 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:51:39 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:51:39 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:39013 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 39013)%%0001017/07/17 22:51:39 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:51:40 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22076%%0001017/07/17 22:51:40 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:39013 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:51:40 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:51:40 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.39:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.30:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.20:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.5:50010%%0001017/07/17 22:51:40 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.38:50010%%0001017/07/17 22:51:40 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:51:40 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:51:40 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:51:40 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:51:40 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:51:40 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:51:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:51:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:39013 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:51:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:51:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:51:40 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:51:41 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:51:42 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:51:44 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://driverPropsFetcher@hcdnc310:50563]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc310:50563] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc310:50563%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:44 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc310:57760/user/Executor#-171104123]) with ID 1%%0001017/07/17 22:51:44 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:51:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc310, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:51:44 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc310:52993 with 530.0 MB RAM, BlockManagerId(1, hcdnc310, 52993)%%0001017/07/17 22:51:44 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://driverPropsFetcher@hcdnc301:33858]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc301:33858] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc301:33858%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:51:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc310:52993 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:51:45 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc301:57727/user/Executor#-966426685]) with ID 2%%0001017/07/17 22:51:45 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:51:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc301, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:51:45 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc301:48467 with 530.0 MB RAM, BlockManagerId(2, hcdnc301, 48467)%%0001017/07/17 22:51:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc310:52993 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:51:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc301:48467 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:51:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc301:48467 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:51:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 14905 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 15533 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:00 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 20.007 s%%0001017/07/17 22:52:00 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:00 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 20.155176 s%%0001017/07/17 22:52:00 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:52:00 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:52:00 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:52:00 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:52:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:52:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:52:00 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:52:00 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:52:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:52:00 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:52:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:52:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:39013 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:52:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:00 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:52:00 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:52:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc310, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:52:00 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc301, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:52:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc301:48467 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc310:52993 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:21 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 20907 ms on hcdnc301 (1/2)%%0001017/07/17 22:52:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21112 ms on hcdnc310 (2/2)%%0001017/07/17 22:52:22 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:22 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.113 s%%0001017/07/17 22:52:22 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:52:22 INFO DAGScheduler: running: Set()%%0001017/07/17 22:52:22 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:52:22 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:52:22 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:52:22 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:52:22 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:52:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:52:22 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:52:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:52:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:39013 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:52:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:52:22 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:52:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc310, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:22 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc301, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc310:52993 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:52:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc301:48467 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:52:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc310:57760%%0001017/07/17 22:52:22 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:52:22 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc301:57727%%0001017/07/17 22:52:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1682 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:23 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1779 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:23 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:23 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.780 s%%0001017/07/17 22:52:23 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 22.926765 s%%0001017/07/17 22:52:23 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:52:23 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:52:23 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:52:23 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:52:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:52:23 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:52:23 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:52:23 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:52:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:52:23 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:52:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:52:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:39013 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:52:23 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:52:23 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:52:23 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc310, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:23 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc301, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc301:48467 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:52:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc310:52993 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:52:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3485 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:27 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3572 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:27 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:27 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.574 s%%0001017/07/17 22:52:27 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.587185 s%%0001017/07/17 22:52:27 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:52:27 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:52:27 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:52:27 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:52:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:52:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:52:27 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:52:27 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:52:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:52:27 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:52:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:52:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:39013 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:52:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:27 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:52:27 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:52:27 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc310, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:27 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc301, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc310:52993 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:52:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc301:48467 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3516 ms on hcdnc310 (1/2)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3525 ms on hcdnc301 (2/2)%%0001017/07/17 22:52:31 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:31 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.527 s%%0001017/07/17 22:52:31 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:52:31 INFO DAGScheduler: running: Set()%%0001017/07/17 22:52:31 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:52:31 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:52:31 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:39013 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:52:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:52:31 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc301, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc310, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc310:52993 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc301:48467 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc310:57760%%0001017/07/17 22:52:31 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 165 bytes%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc301:57727%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc310, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 34 ms on hcdnc310 (1/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc301, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 44 ms on hcdnc301 (2/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc310, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 13 ms on hcdnc310 (3/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc301, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 15 ms on hcdnc301 (4/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc310, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 18 ms on hcdnc310 (5/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc301, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc310, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 12 ms on hcdnc310 (6/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 17 ms on hcdnc301 (7/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc310, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc310 (8/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 16 ms on hcdnc301 (9/10)%%0001017/07/17 22:52:31 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc310 (10/10)%%0001017/07/17 22:52:31 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.098 s%%0001017/07/17 22:52:31 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:31 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.671597 s%%0001017/07/17 22:52:31 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:52:31 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes%%0001017/07/17 22:52:31 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:52:31 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:52:31 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:52:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:52:31 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(13320) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.0 KB, free 530.0 MB)%%0001017/07/17 22:52:31 INFO MemoryStore: ensureFreeSpace(7329) called with curMem=301800, maxMem=556038881%%0001017/07/17 22:52:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.2 KB, free 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:39013 (size: 7.2 KB, free: 530.2 MB)%%0001017/07/17 22:52:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:52:31 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc301, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:31 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc310, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc301:48467 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc310:52993 (size: 7.2 KB, free: 530.0 MB)%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc310:57760%%0001017/07/17 22:52:31 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc301:57727%%0001017/07/17 22:52:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 18620 ms on hcdnc301 (1/2)%%0001017/07/17 22:52:49 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 18781 ms on hcdnc310 (2/2)%%0001017/07/17 22:52:49 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:49 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 18.782 s%%0001017/07/17 22:52:49 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:52:49 INFO DAGScheduler: running: Set()%%0001017/07/17 22:52:49 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:52:49 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:52:49 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:52:49 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:52:49 INFO MemoryStore: ensureFreeSpace(16480) called with curMem=309129, maxMem=556038881%%0001017/07/17 22:52:49 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 16.1 KB, free 530.0 MB)%%0001017/07/17 22:52:49 INFO MemoryStore: ensureFreeSpace(8730) called with curMem=325609, maxMem=556038881%%0001017/07/17 22:52:49 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:52:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:39013 (size: 8.5 KB, free: 530.2 MB)%%0001017/07/17 22:52:49 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:52:49 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:52:49 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc310, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc301, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc310:52993 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:52:50 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc301:48467 (size: 8.5 KB, free: 530.0 MB)%%0001017/07/17 22:52:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc301:57727%%0001017/07/17 22:52:50 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 157 bytes%%0001017/07/17 22:52:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc310:57760%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc310, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 827 ms on hcdnc310 (1/10)%%0001017/07/17 22:52:50 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc301, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:50 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 936 ms on hcdnc301 (2/10)%%0001017/07/17 22:52:51 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:52:51 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc310, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:51 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 560 ms on hcdnc310 (3/10)%%0001017/07/17 22:52:51 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc301, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:51 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 724 ms on hcdnc301 (4/10)%%0001017/07/17 22:52:51 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc310, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:51 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 550 ms on hcdnc310 (5/10)%%0001017/07/17 22:52:52 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 4)%%0001017/07/17 22:52:52 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc301, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:52 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 659 ms on hcdnc301 (6/10)%%0001017/07/17 22:52:52 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc310, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:52 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 537 ms on hcdnc310 (7/10)%%0001017/07/17 22:52:52 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc301, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:52:52 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 677 ms on hcdnc301 (8/10)%%0001017/07/17 22:52:53 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 562 ms on hcdnc310 (9/10)%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://driverPropsFetcher@hcdnc403:43670]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc403:43670] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc403:43670%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 567 ms on hcdnc301 (10/10)%%0001017/07/17 22:52:53 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:52:53 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 3.563 s%%0001017/07/17 22:52:53 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 22.372064 s%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc403:37422/user/Executor#-672322753]) with ID 3%%0001017/07/17 22:52:53 INFO ExecutorAllocationManager: New executor 3 has registered (new total is 3)%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:52:53 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:52:53 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:52:53 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:52:53 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://sparkExecutor@hcdnc301:57727]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc301:57727] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc301:57727%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://sparkExecutor@hcdnc310:57760]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc310:57760] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc310:57760%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc403:38823 with 530.0 MB RAM, BlockManagerId(3, hcdnc403, 38823)%%0001017/07/17 22:52:53 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockManagerAdded(1500303173760,BlockManagerId(3, hcdnc403, 38823),555755765)%%0001017/07/17 22:52:53 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:42112] &lt;- [akka.tcp://sparkExecutor@hcdnc403:37422]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc403:37422] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc403:37422%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:52:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:52:53 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:52:53 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:52:53 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:52:53 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:52:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:52:53 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:52:53 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:52:53 INFO Remoting: Remoting shut down%%0001017/07/17 22:52:53 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:52:54 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:52:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-76e3aa34-9463-4bf6-8a0a-0b38288ab660%%00010"/>
</config>
</config>
<config key="6">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:53:09 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:53:09 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:53:09 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:53:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:53:10 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:53:10 INFO Remoting: Starting remoting%%0001017/07/17 22:53:10 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:35095]%%0001017/07/17 22:53:10 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:35095]%%0001017/07/17 22:53:10 INFO Utils: Successfully started service 'sparkDriver' on port 35095.%%0001017/07/17 22:53:10 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:53:10 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:53:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0b2d4a83-57dc-4f1f-bbb4-005e5fc1acbc%%0001017/07/17 22:53:10 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:53:11 INFO HttpFileServer: HTTP File server directory is /tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48/httpd-126d83f5-1d94-4417-b2a9-21a1637a492e%%0001017/07/17 22:53:11 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:53:11 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:53:11 INFO AbstractConnector: Started SocketConnector@0.0.0.0:42803%%0001017/07/17 22:53:11 INFO Utils: Successfully started service 'HTTP file server' on port 42803.%%0001017/07/17 22:53:11 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:53:11 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:53:12 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:53:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:53:12 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:53:12 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:53:12 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:53:12 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:53:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:53:12 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:53:12 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:53:12 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:53:12 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:53:13 INFO Client: Uploading resource file:/tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48/__spark_conf__6698094196194503130.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22077/__spark_conf__6698094196194503130.zip%%0001017/07/17 22:53:13 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:53:13 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:53:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:53:13 INFO Client: Submitting application 22077 to ResourceManager%%0001017/07/17 22:53:13 INFO YarnClientImpl: Submitted application application_1491786134915_22077%%0001017/07/17 22:53:14 INFO Client: Application report for application_1491786134915_22077 (state: ACCEPTED)%%0001017/07/17 22:53:14 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303193492%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22077/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:53:15 INFO Client: Application report for application_1491786134915_22077 (state: ACCEPTED)%%0001017/07/17 22:53:16 INFO Client: Application report for application_1491786134915_22077 (state: ACCEPTED)%%0001017/07/17 22:53:16 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:45041/user/YarnAM#920749199])%%0001017/07/17 22:53:16 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22077,http://hcnnc117:8088/proxy/application_1491786134915_22077), /proxy/application_1491786134915_22077%%0001017/07/17 22:53:16 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:53:17 INFO Client: Application report for application_1491786134915_22077 (state: RUNNING)%%0001017/07/17 22:53:17 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303193492%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22077/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:53:17 INFO YarnClientSchedulerBackend: Application application_1491786134915_22077 has started running.%%0001017/07/17 22:53:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38757.%%0001017/07/17 22:53:17 INFO NettyBlockTransferService: Server created on 38757%%0001017/07/17 22:53:17 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:53:17 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:53:17 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:38757 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 38757)%%0001017/07/17 22:53:17 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:53:18 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22077%%0001017/07/17 22:53:18 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:38757 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:53:18 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:53:18 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.30:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc03/172.16.3.9:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.4:50010%%0001017/07/17 22:53:18 INFO NetworkTopology: Adding a new node: /hc04/172.16.4.32:50010%%0001017/07/17 22:53:18 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:53:18 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:53:18 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:53:18 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:53:18 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:53:18 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:53:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:53:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:38757 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:53:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:53:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:53:18 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:53:19 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:53:20 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:53:22 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://driverPropsFetcher@hcdnc330:37003]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc330:37003] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc330:37003%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:53:22 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc330:57355/user/Executor#16623872]) with ID 1%%0001017/07/17 22:53:22 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:53:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc330, partition 0,NODE_LOCAL, 2147 bytes)%%0001017/07/17 22:53:22 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc330:34190 with 530.0 MB RAM, BlockManagerId(1, hcdnc330, 34190)%%0001017/07/17 22:53:22 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://driverPropsFetcher@hcdnc320:39346]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc320:39346] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc320:39346%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:53:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc330:34190 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:53:23 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc320:59130/user/Executor#-602691211]) with ID 2%%0001017/07/17 22:53:23 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:53:23 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc320:47660 with 530.0 MB RAM, BlockManagerId(2, hcdnc320, 47660)%%0001017/07/17 22:53:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc330:34190 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:53:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc320, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:53:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc320:47660 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:53:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc320:47660 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:53:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15779 ms on hcdnc330 (1/2)%%0001017/07/17 22:53:41 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 15987 ms on hcdnc320 (2/2)%%0001017/07/17 22:53:41 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 22.901 s%%0001017/07/17 22:53:41 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:53:41 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 23.077911 s%%0001017/07/17 22:53:41 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:53:41 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:53:41 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:53:41 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:53:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:53:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:53:41 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:53:41 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:53:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:53:41 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:53:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:53:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:38757 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:53:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:53:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:53:41 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:53:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc320, partition 0,NODE_LOCAL, 2136 bytes)%%0001017/07/17 22:53:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc320:47660 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:53:42 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:53:45 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc330, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:53:45 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc330:34190 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:54:03 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 22084 ms on hcdnc320 (1/2)%%0001017/07/17 22:54:06 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 21018 ms on hcdnc330 (2/2)%%0001017/07/17 22:54:06 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:06 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.959 s%%0001017/07/17 22:54:06 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:54:06 INFO DAGScheduler: running: Set()%%0001017/07/17 22:54:06 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:54:06 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:54:06 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:54:06 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:54:06 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:54:06 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:54:06 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:54:06 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:54:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:38757 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:54:06 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:54:06 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:54:06 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc330, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:06 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc320, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc330:34190 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:54:06 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc320:47660 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:54:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc330:57355%%0001017/07/17 22:54:06 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:54:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc320:59130%%0001017/07/17 22:54:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1658 ms on hcdnc330 (1/2)%%0001017/07/17 22:54:08 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1739 ms on hcdnc320 (2/2)%%0001017/07/17 22:54:08 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:08 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.741 s%%0001017/07/17 22:54:08 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 26.732969 s%%0001017/07/17 22:54:08 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:54:08 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:54:08 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:54:08 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:54:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:54:08 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:54:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:54:08 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:54:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:54:08 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:54:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:54:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:38757 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:54:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:54:08 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:54:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc330, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:08 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc320, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc320:47660 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:54:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc330:34190 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:54:12 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3465 ms on hcdnc330 (1/2)%%0001017/07/17 22:54:12 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3597 ms on hcdnc320 (2/2)%%0001017/07/17 22:54:12 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:12 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.599 s%%0001017/07/17 22:54:12 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.614624 s%%0001017/07/17 22:54:12 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:54:12 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:54:12 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:54:12 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:54:12 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:54:12 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:54:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:54:12 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:54:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:54:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:38757 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:54:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:54:12 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:54:12 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc320, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:12 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc330, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc330:34190 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:54:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc320:47660 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3544 ms on hcdnc320 (1/2)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3568 ms on hcdnc330 (2/2)%%0001017/07/17 22:54:15 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:15 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.569 s%%0001017/07/17 22:54:15 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:54:15 INFO DAGScheduler: running: Set()%%0001017/07/17 22:54:15 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:54:15 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:54:15 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:38757 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:54:15 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:54:15 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc320, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc330, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc330:34190 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc320:47660 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc320:59130%%0001017/07/17 22:54:15 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 164 bytes%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc330:57355%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc330, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 46 ms on hcdnc330 (1/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc320, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 50 ms on hcdnc320 (2/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc330, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 15 ms on hcdnc330 (3/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc320, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc320 (4/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc330, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 20 ms on hcdnc330 (5/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc320, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 18 ms on hcdnc320 (6/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc330, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc320, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc330 (7/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 14 ms on hcdnc320 (8/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc330 (9/10)%%0001017/07/17 22:54:15 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 14 ms on hcdnc320 (10/10)%%0001017/07/17 22:54:15 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:15 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.106 s%%0001017/07/17 22:54:15 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.699156 s%%0001017/07/17 22:54:15 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:54:15 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:54:15 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:54:15 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:54:15 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:54:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:54:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(12520) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 530.0 MB)%%0001017/07/17 22:54:15 INFO MemoryStore: ensureFreeSpace(6887) called with curMem=301000, maxMem=556038881%%0001017/07/17 22:54:15 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.7 KB, free 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:38757 (size: 6.7 KB, free: 530.2 MB)%%0001017/07/17 22:54:15 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:54:15 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc320, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:15 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc330, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc320:47660 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc330:34190 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc320:59130%%0001017/07/17 22:54:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc330:57355%%0001017/07/17 22:54:28 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 12884 ms on hcdnc330 (1/2)%%0001017/07/17 22:54:29 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 13455 ms on hcdnc320 (2/2)%%0001017/07/17 22:54:29 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:29 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 13.456 s%%0001017/07/17 22:54:29 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:54:29 INFO DAGScheduler: running: Set()%%0001017/07/17 22:54:29 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:54:29 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:54:29 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:54:29 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:54:29 INFO MemoryStore: ensureFreeSpace(15504) called with curMem=307887, maxMem=556038881%%0001017/07/17 22:54:29 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.1 KB, free 530.0 MB)%%0001017/07/17 22:54:29 INFO MemoryStore: ensureFreeSpace(8173) called with curMem=323391, maxMem=556038881%%0001017/07/17 22:54:29 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 530.0 MB)%%0001017/07/17 22:54:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:38757 (size: 8.0 KB, free: 530.2 MB)%%0001017/07/17 22:54:29 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:29 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:54:29 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc320, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc330, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc320:47660 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:54:29 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc330:34190 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:54:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc320:59130%%0001017/07/17 22:54:29 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 160 bytes%%0001017/07/17 22:54:29 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc330:57355%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc330, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 550 ms on hcdnc330 (1/10)%%0001017/07/17 22:54:29 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc320, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:29 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 565 ms on hcdnc320 (2/10)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc330, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 454 ms on hcdnc330 (3/10)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc320, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 446 ms on hcdnc320 (4/10)%%0001017/07/17 22:54:30 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc330, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 444 ms on hcdnc330 (5/10)%%0001017/07/17 22:54:30 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc320, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:30 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 477 ms on hcdnc320 (6/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc330, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 449 ms on hcdnc330 (7/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc320, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 428 ms on hcdnc320 (8/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 499 ms on hcdnc330 (9/10)%%0001017/07/17 22:54:31 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 578 ms on hcdnc320 (10/10)%%0001017/07/17 22:54:31 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:54:31 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 2.490 s%%0001017/07/17 22:54:31 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 15.973522 s%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:54:32 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:54:32 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:54:32 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:54:32 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:54:32 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://sparkExecutor@hcdnc330:57355]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc330:57355] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc330:57355%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:54:32 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35095] &lt;- [akka.tcp://sparkExecutor@hcdnc320:59130]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc320:59130] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc320:59130%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:54:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:54:32 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:54:32 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:54:32 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:54:32 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:54:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:54:32 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:54:32 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:54:32 INFO Remoting: Remoting shut down%%0001017/07/17 22:54:32 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:54:34 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:54:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48/pyspark-871dfbab-a140-4841-8937-029089e1a71e%%0001017/07/17 22:54:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-223ea2cb-8647-4606-8b52-2fc0c56cef48%%00010"/>
</config>
</config>
<config key="7">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:54:48 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:54:49 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:54:49 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:54:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:54:49 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:54:50 INFO Remoting: Starting remoting%%0001017/07/17 22:54:50 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:46487]%%0001017/07/17 22:54:50 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:46487]%%0001017/07/17 22:54:50 INFO Utils: Successfully started service 'sparkDriver' on port 46487.%%0001017/07/17 22:54:50 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:54:50 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:54:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3dcd78b0-4e3d-49d2-adc6-71755624913e%%0001017/07/17 22:54:50 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:54:50 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45/httpd-d76724b4-6bfc-40cc-9d85-943120e99b81%%0001017/07/17 22:54:50 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:54:50 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:54:50 INFO AbstractConnector: Started SocketConnector@0.0.0.0:40438%%0001017/07/17 22:54:50 INFO Utils: Successfully started service 'HTTP file server' on port 40438.%%0001017/07/17 22:54:50 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:54:51 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:54:51 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:54:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:54:51 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:54:51 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:54:52 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:54:52 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:54:52 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:54:52 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:54:52 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:54:52 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:54:52 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:54:52 INFO Client: Uploading resource file:/tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45/__spark_conf__6610156780387149534.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22078/__spark_conf__6610156780387149534.zip%%0001017/07/17 22:54:52 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:54:52 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:54:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:54:52 INFO Client: Submitting application 22078 to ResourceManager%%0001017/07/17 22:54:53 INFO YarnClientImpl: Submitted application application_1491786134915_22078%%0001017/07/17 22:54:54 INFO Client: Application report for application_1491786134915_22078 (state: ACCEPTED)%%0001017/07/17 22:54:54 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303292900%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22078/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:54:55 INFO Client: Application report for application_1491786134915_22078 (state: ACCEPTED)%%0001017/07/17 22:54:55 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:35633/user/YarnAM#-976557814])%%0001017/07/17 22:54:55 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22078,http://hcnnc117:8088/proxy/application_1491786134915_22078), /proxy/application_1491786134915_22078%%0001017/07/17 22:54:55 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:54:56 INFO Client: Application report for application_1491786134915_22078 (state: RUNNING)%%0001017/07/17 22:54:56 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303292900%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22078/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:54:56 INFO YarnClientSchedulerBackend: Application application_1491786134915_22078 has started running.%%0001017/07/17 22:54:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35797.%%0001017/07/17 22:54:56 INFO NettyBlockTransferService: Server created on 35797%%0001017/07/17 22:54:56 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:54:56 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:54:56 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:35797 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 35797)%%0001017/07/17 22:54:56 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:54:56 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22078%%0001017/07/17 22:54:56 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:54:56 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:54:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:54:56 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:54:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:54:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:35797 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:54:56 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:54:57 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.12:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc02/172.16.2.13:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.31:50010%%0001017/07/17 22:54:57 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.25:50010%%0001017/07/17 22:54:57 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:54:57 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:54:57 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:54:57 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:54:57 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:54:57 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:54:57 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:54:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:54:57 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:54:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:54:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:35797 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:54:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:54:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:54:57 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:54:58 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:54:59 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:55:00 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://driverPropsFetcher@hcdnc823:45961]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:45961] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc823:45961%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:55:01 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc823:40788/user/Executor#-1325376196]) with ID 1%%0001017/07/17 22:55:01 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:55:01 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 0, hcdnc823, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:55:01 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc823:51216 with 530.0 MB RAM, BlockManagerId(1, hcdnc823, 51216)%%0001017/07/17 22:55:01 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://driverPropsFetcher@hcdnc825:48017]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc825:48017] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc825:48017%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:55:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc823:51216 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:01 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc825:34020/user/Executor#376040495]) with ID 2%%0001017/07/17 22:55:01 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:55:01 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc825:58420 with 530.0 MB RAM, BlockManagerId(2, hcdnc825, 58420)%%0001017/07/17 22:55:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc823:51216 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:55:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 1, hcdnc825, partition 0,ANY, 2147 bytes)%%0001017/07/17 22:55:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc825:58420 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc825:58420 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:55:16 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 0) in 15347 ms on hcdnc823 (1/2)%%0001017/07/17 22:55:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 1) in 15812 ms on hcdnc825 (2/2)%%0001017/07/17 22:55:20 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:20 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 23.671 s%%0001017/07/17 22:55:20 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 23.854399 s%%0001017/07/17 22:55:20 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:55:20 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:55:20 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:55:20 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:55:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:55:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:55:20 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:55:20 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:55:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:55:20 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:55:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:55:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:35797 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:55:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:20 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:55:20 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:55:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, hcdnc825, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:55:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc825:58420 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:55:22 INFO ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:55:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, hcdnc823, partition 0,ANY, 2136 bytes)%%0001017/07/17 22:55:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc823:51216 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:55:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 21732 ms on hcdnc825 (1/2)%%0001017/07/17 22:55:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 21159 ms on hcdnc823 (2/2)%%0001017/07/17 22:55:45 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:45 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 24.278 s%%0001017/07/17 22:55:45 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:55:45 INFO DAGScheduler: running: Set()%%0001017/07/17 22:55:45 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:55:45 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:55:45 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:55:45 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:55:45 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:55:45 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:55:45 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:55:45 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:55:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:35797 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:55:45 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:55:45 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:55:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:45 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc825, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc823:51216 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:45 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc825:58420 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:55:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc825:34020%%0001017/07/17 22:55:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:55:45 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:40788%%0001017/07/17 22:55:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1615 ms on hcdnc823 (1/2)%%0001017/07/17 22:55:47 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1776 ms on hcdnc825 (2/2)%%0001017/07/17 22:55:47 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:47 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.778 s%%0001017/07/17 22:55:47 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 26.087244 s%%0001017/07/17 22:55:47 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:55:47 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:55:47 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:55:47 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:55:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:55:47 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:55:47 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:55:47 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:55:47 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:55:47 INFO MemoryStore: ensureFreeSpace(3904) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:55:47 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:35797 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:55:47 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:55:47 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:55:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc825, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:47 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc823, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc825:58420 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:55:47 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc823:51216 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:55:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 3440 ms on hcdnc825 (1/2)%%0001017/07/17 22:55:50 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3451 ms on hcdnc823 (2/2)%%0001017/07/17 22:55:50 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 3.453 s%%0001017/07/17 22:55:50 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:50 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 3.466924 s%%0001017/07/17 22:55:50 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:55:50 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:55:50 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:55:50 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:55:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:55:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:55:50 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:55:50 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272760, maxMem=556038881%%0001017/07/17 22:55:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:55:50 INFO MemoryStore: ensureFreeSpace(4151) called with curMem=279936, maxMem=556038881%%0001017/07/17 22:55:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:55:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:35797 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:55:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:50 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:55:50 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:55:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc825, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:50 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc823:51216 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:55:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc825:58420 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 3460 ms on hcdnc823 (1/2)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3496 ms on hcdnc825 (2/2)%%0001017/07/17 22:55:54 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:54 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 3.497 s%%0001017/07/17 22:55:54 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:55:54 INFO DAGScheduler: running: Set()%%0001017/07/17 22:55:54 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:55:54 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:55:54 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284087, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286871, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:35797 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:55:54 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:55:54 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc825, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc823, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc825:58420 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc823:51216 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc825:34020%%0001017/07/17 22:55:54 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 164 bytes%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc823:40788%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc825, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc823, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 45 ms on hcdnc825 (1/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 45 ms on hcdnc823 (2/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc825, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc823, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 18 ms on hcdnc825 (3/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 17 ms on hcdnc823 (4/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc825, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc825 (5/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc823, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 20 ms on hcdnc823 (6/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc825, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc825 (7/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc823, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 14 ms on hcdnc823 (8/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 13 ms on hcdnc825 (9/10)%%0001017/07/17 22:55:54 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc823 (10/10)%%0001017/07/17 22:55:54 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.103 s%%0001017/07/17 22:55:54 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:55:54 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 3.621325 s%%0001017/07/17 22:55:54 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:55:54 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 152 bytes%%0001017/07/17 22:55:54 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:55:54 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:55:54 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:55:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:55:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(12520) called with curMem=288484, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 530.0 MB)%%0001017/07/17 22:55:54 INFO MemoryStore: ensureFreeSpace(6889) called with curMem=301004, maxMem=556038881%%0001017/07/17 22:55:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.7 KB, free 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:35797 (size: 6.7 KB, free: 530.2 MB)%%0001017/07/17 22:55:54 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:55:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:55:54 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc825, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:54 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc823, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc825:58420 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc823:51216 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc825:34020%%0001017/07/17 22:55:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc823:40788%%0001017/07/17 22:56:07 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 13446 ms on hcdnc825 (1/2)%%0001017/07/17 22:56:08 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 14284 ms on hcdnc823 (2/2)%%0001017/07/17 22:56:08 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:56:08 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 14.286 s%%0001017/07/17 22:56:08 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:56:08 INFO DAGScheduler: running: Set()%%0001017/07/17 22:56:08 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:56:08 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:56:08 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:56:08 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:56:08 INFO MemoryStore: ensureFreeSpace(15504) called with curMem=307893, maxMem=556038881%%0001017/07/17 22:56:08 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.1 KB, free 530.0 MB)%%0001017/07/17 22:56:08 INFO MemoryStore: ensureFreeSpace(8176) called with curMem=323397, maxMem=556038881%%0001017/07/17 22:56:08 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 530.0 MB)%%0001017/07/17 22:56:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:35797 (size: 8.0 KB, free: 530.2 MB)%%0001017/07/17 22:56:08 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:56:08 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:56:08 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:56:08 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc823, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:08 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc825, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc823:51216 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:56:08 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc825:58420 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:56:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc825:34020%%0001017/07/17 22:56:08 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 160 bytes%%0001017/07/17 22:56:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc823:40788%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc825, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 586 ms on hcdnc825 (1/10)%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc823, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 678 ms on hcdnc823 (2/10)%%0001017/07/17 22:56:09 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc825, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 539 ms on hcdnc825 (3/10)%%0001017/07/17 22:56:09 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc823, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:09 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 497 ms on hcdnc823 (4/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc825, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 415 ms on hcdnc825 (5/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc823, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 464 ms on hcdnc823 (6/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc825, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 461 ms on hcdnc825 (7/10)%%0001017/07/17 22:56:10 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc823, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:56:10 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 472 ms on hcdnc823 (8/10)%%0001017/07/17 22:56:11 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 491 ms on hcdnc825 (9/10)%%0001017/07/17 22:56:11 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 531 ms on hcdnc823 (10/10)%%0001017/07/17 22:56:11 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:56:11 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 2.638 s%%0001017/07/17 22:56:11 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 16.950343 s%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:56:11 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:56:11 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:56:11 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:56:11 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:56:11 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://sparkExecutor@hcdnc825:34020]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc825:34020] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc825:34020%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:11 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:46487] &lt;- [akka.tcp://sparkExecutor@hcdnc823:40788]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc823:40788] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc823:40788%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:56:11 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:56:11 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:56:11 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:56:11 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:56:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:56:11 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:56:11 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:56:11 INFO Remoting: Remoting shut down%%0001017/07/17 22:56:11 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:56:12 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:56:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45/pyspark-2342f14d-a3a8-4f50-ba61-fec8ac943925%%0001017/07/17 22:56:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0e550b3-ec06-4bde-a6f2-125dc7038a45%%00010"/>
</config>
</config>
<config key="8">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="STDERR message:%%00010SLF4J: Class path contains multiple SLF4J bindings.%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/avro-tools-1.7.6-cdh5.5.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]%%00010SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.%%0001017/07/17 22:56:26 INFO SparkContext: Running Spark version 1.5.0-cdh5.5.1%%0001017/07/17 22:56:27 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:56:27 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:56:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:56:27 INFO Slf4jLogger: Slf4jLogger started%%0001017/07/17 22:56:27 INFO Remoting: Starting remoting%%0001017/07/17 22:56:27 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.16.1.12:35779]%%0001017/07/17 22:56:27 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@172.16.1.12:35779]%%0001017/07/17 22:56:27 INFO Utils: Successfully started service 'sparkDriver' on port 35779.%%0001017/07/17 22:56:27 INFO SparkEnv: Registering MapOutputTracker%%0001017/07/17 22:56:27 INFO SparkEnv: Registering BlockManagerMaster%%0001017/07/17 22:56:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7c9ffc16-fdb1-4eed-9472-75805bfb9c91%%0001017/07/17 22:56:27 INFO MemoryStore: MemoryStore started with capacity 530.3 MB%%0001017/07/17 22:56:27 INFO HttpFileServer: HTTP File server directory is /tmp/spark-6289b39e-6363-449c-b361-9f7949145b57/httpd-787e9417-b4ea-449f-8e30-75296370cdea%%0001017/07/17 22:56:27 INFO HttpServer: Starting HTTP Server%%0001017/07/17 22:56:28 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:56:28 INFO AbstractConnector: Started SocketConnector@0.0.0.0:38817%%0001017/07/17 22:56:28 INFO Utils: Successfully started service 'HTTP file server' on port 38817.%%0001017/07/17 22:56:28 INFO SparkEnv: Registering OutputCommitCoordinator%%0001017/07/17 22:56:28 INFO Server: jetty-8.y.z-SNAPSHOT%%0001017/07/17 22:56:28 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040%%0001017/07/17 22:56:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.%%0001017/07/17 22:56:28 INFO SparkUI: Started SparkUI at http://172.16.1.12:4040%%0001017/07/17 22:56:28 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.%%0001017/07/17 22:56:28 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm571%%0001017/07/17 22:56:28 INFO Client: Requesting a new application from cluster with 122 NodeManagers%%0001017/07/17 22:56:28 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49152 MB per container)%%0001017/07/17 22:56:28 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead%%0001017/07/17 22:56:28 INFO Client: Setting up container launch context for our AM%%0001017/07/17 22:56:28 INFO Client: Setting up the launch environment for our AM container%%0001017/07/17 22:56:28 INFO Client: Preparing resources for our AM container%%0001017/07/17 22:56:29 INFO Client: Uploading resource file:/tmp/spark-6289b39e-6363-449c-b361-9f7949145b57/__spark_conf__5587776092955324535.zip -&gt; hdfs://nn/user/y23ycc01/.sparkStaging/application_1491786134915_22079/__spark_conf__5587776092955324535.zip%%0001017/07/17 22:56:30 INFO SecurityManager: Changing view acls to: y23ycc01%%0001017/07/17 22:56:30 INFO SecurityManager: Changing modify acls to: y23ycc01%%0001017/07/17 22:56:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(y23ycc01); users with modify permissions: Set(y23ycc01)%%0001017/07/17 22:56:30 INFO Client: Submitting application 22079 to ResourceManager%%0001017/07/17 22:56:30 INFO YarnClientImpl: Submitted application application_1491786134915_22079%%0001017/07/17 22:56:31 INFO Client: Application report for application_1491786134915_22079 (state: ACCEPTED)%%0001017/07/17 22:56:31 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: N/A%%00010%%00009 ApplicationMaster RPC port: -1%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303390346%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22079/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:56:32 INFO Client: Application report for application_1491786134915_22079 (state: ACCEPTED)%%0001017/07/17 22:56:33 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@172.16.2.35:51525/user/YarnAM#-1584097551])%%0001017/07/17 22:56:33 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; hcnnc115,hcnnc117, PROXY_URI_BASES -&gt; http://hcnnc115:8088/proxy/application_1491786134915_22079,http://hcnnc117:8088/proxy/application_1491786134915_22079), /proxy/application_1491786134915_22079%%0001017/07/17 22:56:33 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter%%0001017/07/17 22:56:33 INFO Client: Application report for application_1491786134915_22079 (state: RUNNING)%%0001017/07/17 22:56:33 INFO Client: %%00010%%00009 client token: N/A%%00010%%00009 diagnostics: N/A%%00010%%00009 ApplicationMaster host: 172.16.2.35%%00010%%00009 ApplicationMaster RPC port: 0%%00010%%00009 queue: root.y23ycc01%%00010%%00009 start time: 1500303390346%%00010%%00009 final status: UNDEFINED%%00010%%00009 tracking URL: http://hcnnc117:8088/proxy/application_1491786134915_22079/%%00010%%00009 user: y23ycc01%%0001017/07/17 22:56:33 INFO YarnClientSchedulerBackend: Application application_1491786134915_22079 has started running.%%0001017/07/17 22:56:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33185.%%0001017/07/17 22:56:33 INFO NettyBlockTransferService: Server created on 33185%%0001017/07/17 22:56:33 INFO BlockManager: external shuffle service port = 7337%%0001017/07/17 22:56:33 INFO BlockManagerMaster: Trying to register BlockManager%%0001017/07/17 22:56:33 INFO BlockManagerMasterEndpoint: Registering block manager 172.16.1.12:33185 with 530.3 MB RAM, BlockManagerId(driver, 172.16.1.12, 33185)%%0001017/07/17 22:56:33 INFO BlockManagerMaster: Registered BlockManager%%0001017/07/17 22:56:33 INFO EventLoggingListener: Logging events to hdfs://nn/user/spark/applicationHistory/application_1491786134915_22079%%0001017/07/17 22:56:33 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(202768) called with curMem=0, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.0 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(23868) called with curMem=202768, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.1.12:33185 (size: 23.3 KB, free: 530.3 MB)%%0001017/07/17 22:56:34 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:-2%%0001017/07/17 22:56:34 INFO FileInputFormat: Total input paths to process : 1%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.31:50010%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.35:50010%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /default/172.16.3.20:50010%%0001017/07/17 22:56:34 INFO NetworkTopology: Adding a new node: /hc08/172.16.8.29:50010%%0001017/07/17 22:56:34 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65%%0001017/07/17 22:56:34 INFO DAGScheduler: Got job 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) with 2 output partitions%%0001017/07/17 22:56:34 INFO DAGScheduler: Final stage: ResultStage 0(count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:56:34 INFO DAGScheduler: Parents of final stage: List()%%0001017/07/17 22:56:34 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:56:34 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65), which has no missing parents%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(6792) called with curMem=226636, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.6 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO MemoryStore: ensureFreeSpace(3971) called with curMem=233428, maxMem=556038881%%0001017/07/17 22:56:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.1 MB)%%0001017/07/17 22:56:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.1.12:33185 (size: 3.9 KB, free: 530.3 MB)%%0001017/07/17 22:56:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:56:34 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:65)%%0001017/07/17 22:56:34 INFO YarnScheduler: Adding task set 0.0 with 2 tasks%%0001017/07/17 22:56:35 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)%%0001017/07/17 22:56:36 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)%%0001017/07/17 22:56:38 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://driverPropsFetcher@hcdnc323:50799]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc323:50799] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc323:50799%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:38 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc323:39660/user/Executor#-348958769]) with ID 1%%0001017/07/17 22:56:38 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)%%0001017/07/17 22:56:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hcdnc323, partition 0,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:56:38 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc323:38204 with 530.0 MB RAM, BlockManagerId(1, hcdnc323, 38204)%%0001017/07/17 22:56:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc323:38204 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:56:38 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://driverPropsFetcher@hcdnc827:44467]: Error [Shut down address: akka.tcp://driverPropsFetcher@hcdnc827:44467] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://driverPropsFetcher@hcdnc827:44467%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:56:39 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@hcdnc827:37555/user/Executor#1208374042]) with ID 2%%0001017/07/17 22:56:39 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)%%0001017/07/17 22:56:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hcdnc827, partition 1,RACK_LOCAL, 2147 bytes)%%0001017/07/17 22:56:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc323:38204 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:56:39 INFO BlockManagerMasterEndpoint: Registering block manager hcdnc827:51633 with 530.0 MB RAM, BlockManagerId(2, hcdnc827, 51633)%%0001017/07/17 22:56:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hcdnc827:51633 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:56:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hcdnc827:51633 (size: 23.3 KB, free: 530.0 MB)%%0001017/07/17 22:56:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 15367 ms on hcdnc323 (1/2)%%0001017/07/17 22:56:53 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 14699 ms on hcdnc827 (2/2)%%0001017/07/17 22:56:53 INFO DAGScheduler: ResultStage 0 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:65) finished in 19.309 s%%0001017/07/17 22:56:53 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool %%0001017/07/17 22:56:53 INFO DAGScheduler: Job 0 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:65, took 19.456195 s%%0001017/07/17 22:56:54 INFO SparkContext: Starting job: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70%%0001017/07/17 22:56:54 INFO DAGScheduler: Registering RDD 4 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:56:54 INFO DAGScheduler: Got job 1 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) with 2 output partitions%%0001017/07/17 22:56:54 INFO DAGScheduler: Final stage: ResultStage 2(count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:56:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)%%0001017/07/17 22:56:54 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)%%0001017/07/17 22:56:54 INFO DAGScheduler: Submitting ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69), which has no missing parents%%0001017/07/17 22:56:54 INFO MemoryStore: ensureFreeSpace(8656) called with curMem=237399, maxMem=556038881%%0001017/07/17 22:56:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.5 KB, free 530.0 MB)%%0001017/07/17 22:56:54 INFO MemoryStore: ensureFreeSpace(5302) called with curMem=246055, maxMem=556038881%%0001017/07/17 22:56:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.2 KB, free 530.0 MB)%%0001017/07/17 22:56:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.1.12:33185 (size: 5.2 KB, free: 530.2 MB)%%0001017/07/17 22:56:54 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:56:54 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (PairwiseRDD[4] at combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69)%%0001017/07/17 22:56:54 INFO YarnScheduler: Adding task set 1.0 with 2 tasks%%0001017/07/17 22:56:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, hcdnc323, partition 0,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:56:54 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, hcdnc827, partition 1,RACK_LOCAL, 2136 bytes)%%0001017/07/17 22:56:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc323:38204 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:56:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hcdnc827:51633 (size: 5.2 KB, free: 530.0 MB)%%0001017/07/17 22:57:13 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 19928 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 21714 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:15 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:15 INFO DAGScheduler: ShuffleMapStage 1 (combineByKey at /home_i1/y23ycc01/TCMAnalyzer2.py:69) finished in 21.714 s%%0001017/07/17 22:57:15 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:57:15 INFO DAGScheduler: running: Set()%%0001017/07/17 22:57:15 INFO DAGScheduler: waiting: Set(ResultStage 2)%%0001017/07/17 22:57:15 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:57:15 INFO DAGScheduler: Missing parents for ResultStage 2: List()%%0001017/07/17 22:57:15 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70), which is now runnable%%0001017/07/17 22:57:15 INFO MemoryStore: ensureFreeSpace(6936) called with curMem=251357, maxMem=556038881%%0001017/07/17 22:57:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 6.8 KB, free 530.0 MB)%%0001017/07/17 22:57:15 INFO MemoryStore: ensureFreeSpace(4043) called with curMem=258293, maxMem=556038881%%0001017/07/17 22:57:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.9 KB, free 530.0 MB)%%0001017/07/17 22:57:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.16.1.12:33185 (size: 3.9 KB, free: 530.2 MB)%%0001017/07/17 22:57:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (PythonRDD[7] at count at /home_i1/y23ycc01/TCMAnalyzer2.py:70)%%0001017/07/17 22:57:15 INFO YarnScheduler: Adding task set 2.0 with 2 tasks%%0001017/07/17 22:57:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, hcdnc323, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:15 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, hcdnc827, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc323:38204 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:57:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hcdnc827:51633 (size: 3.9 KB, free: 530.0 MB)%%0001017/07/17 22:57:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc323:39660%%0001017/07/17 22:57:15 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:57:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc827:37555%%0001017/07/17 22:57:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 1646 ms on hcdnc323 (1/2)%%0001017/07/17 22:57:17 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 1710 ms on hcdnc827 (2/2)%%0001017/07/17 22:57:17 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:17 INFO DAGScheduler: ResultStage 2 (count at /home_i1/y23ycc01/TCMAnalyzer2.py:70) finished in 1.711 s%%0001017/07/17 22:57:17 INFO DAGScheduler: Job 1 finished: count at /home_i1/y23ycc01/TCMAnalyzer2.py:70, took 23.458945 s%%0001017/07/17 22:57:17 WARN FPGrowth: Input data is not cached.%%0001017/07/17 22:57:17 INFO SparkContext: Starting job: count at FPGrowth.scala:121%%0001017/07/17 22:57:17 INFO DAGScheduler: Got job 2 (count at FPGrowth.scala:121) with 2 output partitions%%0001017/07/17 22:57:17 INFO DAGScheduler: Final stage: ResultStage 4(count at FPGrowth.scala:121)%%0001017/07/17 22:57:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)%%0001017/07/17 22:57:17 INFO DAGScheduler: Missing parents: List()%%0001017/07/17 22:57:17 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544), which has no missing parents%%0001017/07/17 22:57:17 INFO MemoryStore: ensureFreeSpace(6520) called with curMem=262336, maxMem=556038881%%0001017/07/17 22:57:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 6.4 KB, free 530.0 MB)%%0001017/07/17 22:57:17 INFO MemoryStore: ensureFreeSpace(3902) called with curMem=268856, maxMem=556038881%%0001017/07/17 22:57:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 530.0 MB)%%0001017/07/17 22:57:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.16.1.12:33185 (size: 3.8 KB, free: 530.2 MB)%%0001017/07/17 22:57:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[10] at map at PythonMLLibAPI.scala:544)%%0001017/07/17 22:57:17 INFO YarnScheduler: Adding task set 4.0 with 2 tasks%%0001017/07/17 22:57:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, hcdnc323, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:17 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, hcdnc827, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc827:51633 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:57:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hcdnc323:38204 (size: 3.8 KB, free: 530.0 MB)%%0001017/07/17 22:57:20 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 3419 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:21 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 4321 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:21 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:21 INFO DAGScheduler: ResultStage 4 (count at FPGrowth.scala:121) finished in 4.322 s%%0001017/07/17 22:57:21 INFO DAGScheduler: Job 2 finished: count at FPGrowth.scala:121, took 4.335987 s%%0001017/07/17 22:57:21 INFO SparkContext: Starting job: collect at FPGrowth.scala:156%%0001017/07/17 22:57:21 INFO DAGScheduler: Registering RDD 12 (map at FPGrowth.scala:153)%%0001017/07/17 22:57:21 INFO DAGScheduler: Got job 3 (collect at FPGrowth.scala:156) with 10 output partitions%%0001017/07/17 22:57:21 INFO DAGScheduler: Final stage: ResultStage 7(collect at FPGrowth.scala:156)%%0001017/07/17 22:57:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)%%0001017/07/17 22:57:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)%%0001017/07/17 22:57:21 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153), which has no missing parents%%0001017/07/17 22:57:21 INFO MemoryStore: ensureFreeSpace(7176) called with curMem=272758, maxMem=556038881%%0001017/07/17 22:57:21 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.0 KB, free 530.0 MB)%%0001017/07/17 22:57:21 INFO MemoryStore: ensureFreeSpace(4149) called with curMem=279934, maxMem=556038881%%0001017/07/17 22:57:21 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.1 KB, free 530.0 MB)%%0001017/07/17 22:57:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.16.1.12:33185 (size: 4.1 KB, free: 530.2 MB)%%0001017/07/17 22:57:21 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[12] at map at FPGrowth.scala:153)%%0001017/07/17 22:57:21 INFO YarnScheduler: Adding task set 6.0 with 2 tasks%%0001017/07/17 22:57:21 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, hcdnc827, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:21 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9, hcdnc323, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc323:38204 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:57:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hcdnc827:51633 (size: 4.1 KB, free: 530.0 MB)%%0001017/07/17 22:57:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 3469 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 4543 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:26 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:26 INFO DAGScheduler: ShuffleMapStage 6 (map at FPGrowth.scala:153) finished in 4.545 s%%0001017/07/17 22:57:26 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:57:26 INFO DAGScheduler: running: Set()%%0001017/07/17 22:57:26 INFO DAGScheduler: waiting: Set(ResultStage 7)%%0001017/07/17 22:57:26 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:57:26 INFO DAGScheduler: Missing parents for ResultStage 7: List()%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155), which is now runnable%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(2784) called with curMem=284083, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.7 KB, free 530.0 MB)%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(1613) called with curMem=286867, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1613.0 B, free 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.16.1.12:33185 (size: 1613.0 B, free: 530.2 MB)%%0001017/07/17 22:57:26 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 7 (MapPartitionsRDD[14] at filter at FPGrowth.scala:155)%%0001017/07/17 22:57:26 INFO YarnScheduler: Adding task set 7.0 with 10 tasks%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 10, hcdnc827, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 11, hcdnc323, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc323:38204 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hcdnc827:51633 (size: 1613.0 B, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc323:39660%%0001017/07/17 22:57:26 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 166 bytes%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to hcdnc827:37555%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 12, hcdnc323, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 13, hcdnc827, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 11) in 47 ms on hcdnc323 (1/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 10) in 47 ms on hcdnc827 (2/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 14, hcdnc323, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 15, hcdnc827, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 12) in 17 ms on hcdnc323 (3/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 13) in 16 ms on hcdnc827 (4/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 16, hcdnc323, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 14) in 17 ms on hcdnc323 (5/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 17, hcdnc827, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 15) in 19 ms on hcdnc827 (6/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 18, hcdnc323, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 16) in 15 ms on hcdnc323 (7/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 19, hcdnc827, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 17) in 15 ms on hcdnc827 (8/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 18) in 14 ms on hcdnc323 (9/10)%%0001017/07/17 22:57:26 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 19) in 13 ms on hcdnc827 (10/10)%%0001017/07/17 22:57:26 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:26 INFO DAGScheduler: ResultStage 7 (collect at FPGrowth.scala:156) finished in 0.106 s%%0001017/07/17 22:57:26 INFO DAGScheduler: Job 3 finished: collect at FPGrowth.scala:156, took 4.673833 s%%0001017/07/17 22:57:26 INFO SparkContext: Starting job: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73%%0001017/07/17 22:57:26 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 154 bytes%%0001017/07/17 22:57:26 INFO DAGScheduler: Registering RDD 15 (flatMap at FPGrowth.scala:175)%%0001017/07/17 22:57:26 INFO DAGScheduler: Got job 4 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) with 10 output partitions%%0001017/07/17 22:57:26 INFO DAGScheduler: Final stage: ResultStage 10(collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:57:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)%%0001017/07/17 22:57:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175), which has no missing parents%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(12520) called with curMem=288480, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.2 KB, free 530.0 MB)%%0001017/07/17 22:57:26 INFO MemoryStore: ensureFreeSpace(6887) called with curMem=301000, maxMem=556038881%%0001017/07/17 22:57:26 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.7 KB, free 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.16.1.12:33185 (size: 6.7 KB, free: 530.2 MB)%%0001017/07/17 22:57:26 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[15] at flatMap at FPGrowth.scala:175)%%0001017/07/17 22:57:26 INFO YarnScheduler: Adding task set 9.0 with 2 tasks%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20, hcdnc827, partition 0,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:26 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21, hcdnc323, partition 1,PROCESS_LOCAL, 1890 bytes)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc827:51633 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hcdnc323:38204 (size: 6.7 KB, free: 530.0 MB)%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc323:39660%%0001017/07/17 22:57:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to hcdnc827:37555%%0001017/07/17 22:57:40 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 13722 ms on hcdnc827 (1/2)%%0001017/07/17 22:57:40 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 14031 ms on hcdnc323 (2/2)%%0001017/07/17 22:57:40 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:40 INFO DAGScheduler: ShuffleMapStage 9 (flatMap at FPGrowth.scala:175) finished in 14.032 s%%0001017/07/17 22:57:40 INFO DAGScheduler: looking for newly runnable stages%%0001017/07/17 22:57:40 INFO DAGScheduler: running: Set()%%0001017/07/17 22:57:40 INFO DAGScheduler: waiting: Set(ResultStage 10)%%0001017/07/17 22:57:40 INFO DAGScheduler: failed: Set()%%0001017/07/17 22:57:40 INFO DAGScheduler: Missing parents for ResultStage 10: List()%%0001017/07/17 22:57:40 INFO DAGScheduler: Submitting ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73), which is now runnable%%0001017/07/17 22:57:40 INFO MemoryStore: ensureFreeSpace(15504) called with curMem=307887, maxMem=556038881%%0001017/07/17 22:57:40 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 15.1 KB, free 530.0 MB)%%0001017/07/17 22:57:40 INFO MemoryStore: ensureFreeSpace(8173) called with curMem=323391, maxMem=556038881%%0001017/07/17 22:57:40 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 8.0 KB, free 530.0 MB)%%0001017/07/17 22:57:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.16.1.12:33185 (size: 8.0 KB, free: 530.2 MB)%%0001017/07/17 22:57:40 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861%%0001017/07/17 22:57:40 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 10 (PythonRDD[22] at collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73)%%0001017/07/17 22:57:40 INFO YarnScheduler: Adding task set 10.0 with 10 tasks%%0001017/07/17 22:57:40 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 22, hcdnc323, partition 0,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:40 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 23, hcdnc827, partition 1,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc323:38204 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:57:40 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hcdnc827:51633 (size: 8.0 KB, free: 530.0 MB)%%0001017/07/17 22:57:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc323:39660%%0001017/07/17 22:57:40 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 162 bytes%%0001017/07/17 22:57:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to hcdnc827:37555%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 24, hcdnc827, partition 2,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 23) in 578 ms on hcdnc827 (1/10)%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 25, hcdnc323, partition 3,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 22) in 633 ms on hcdnc323 (2/10)%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 26, hcdnc827, partition 4,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 24) in 459 ms on hcdnc827 (3/10)%%0001017/07/17 22:57:41 INFO ExecutorAllocationManager: Requesting 3 new executors because tasks are backlogged (new desired total will be 3)%%0001017/07/17 22:57:41 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 27, hcdnc323, partition 5,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:41 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 25) in 498 ms on hcdnc323 (4/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 28, hcdnc827, partition 6,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 26) in 437 ms on hcdnc827 (5/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 29, hcdnc323, partition 7,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 27) in 497 ms on hcdnc323 (6/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 8.0 in stage 10.0 (TID 30, hcdnc827, partition 8,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 28) in 428 ms on hcdnc827 (7/10)%%0001017/07/17 22:57:42 INFO TaskSetManager: Starting task 9.0 in stage 10.0 (TID 31, hcdnc323, partition 9,PROCESS_LOCAL, 1901 bytes)%%0001017/07/17 22:57:42 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 29) in 448 ms on hcdnc323 (8/10)%%0001017/07/17 22:57:43 INFO TaskSetManager: Finished task 8.0 in stage 10.0 (TID 30) in 516 ms on hcdnc827 (9/10)%%0001017/07/17 22:57:43 INFO TaskSetManager: Finished task 9.0 in stage 10.0 (TID 31) in 520 ms on hcdnc323 (10/10)%%0001017/07/17 22:57:43 INFO YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool %%0001017/07/17 22:57:43 INFO DAGScheduler: ResultStage 10 (collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73) finished in 2.594 s%%0001017/07/17 22:57:43 INFO DAGScheduler: Job 4 finished: collect at /home_i1/y23ycc01/TCMAnalyzer2.py:73, took 16.652190 s%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}%%0001017/07/17 22:57:43 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}%%0001017/07/17 22:57:43 INFO SparkUI: Stopped Spark web UI at http://172.16.1.12:4040%%0001017/07/17 22:57:43 INFO DAGScheduler: Stopping DAGScheduler%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Shutting down all executors%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Interrupting monitor thread%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Asking each executor to shut down%%0001017/07/17 22:57:43 INFO YarnClientSchedulerBackend: Stopped%%0001017/07/17 22:57:43 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://sparkExecutor@hcdnc323:39660]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc323:39660] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc323:39660%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:57:43 ERROR ErrorMonitor: AssociationError [akka.tcp://sparkDriver@172.16.1.12:35779] &lt;- [akka.tcp://sparkExecutor@hcdnc827:37555]: Error [Shut down address: akka.tcp://sparkExecutor@hcdnc827:37555] [%%00010akka.remote.ShutDownAssociation: Shut down address: akka.tcp://sparkExecutor@hcdnc827:37555%%00010Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.%%00010]%%00010akka.event.Logging$Error$NoCause$%%0001017/07/17 22:57:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!%%0001017/07/17 22:57:43 INFO MemoryStore: MemoryStore cleared%%0001017/07/17 22:57:43 INFO BlockManager: BlockManager stopped%%0001017/07/17 22:57:43 INFO BlockManagerMaster: BlockManagerMaster stopped%%0001017/07/17 22:57:43 INFO SparkContext: Successfully stopped SparkContext%%0001017/07/17 22:57:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!%%0001017/07/17 22:57:43 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.%%0001017/07/17 22:57:43 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.%%0001017/07/17 22:57:43 INFO Remoting: Remoting shut down%%0001017/07/17 22:57:43 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.%%0001017/07/17 22:57:44 INFO ShutdownHookManager: Shutdown hook called%%0001017/07/17 22:57:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-6289b39e-6363-449c-b361-9f7949145b57%%00010"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_2">
<entry key="column_name" type="xstring" value="FailingNode"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="Bash"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_3">
<entry key="column_name" type="xstring" value="currentIteration"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.IntCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.IntCell"/>
<config key="org.knime.core.data.def.IntCell">
<entry key="IntCell" type="xint" value="0"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.IntCell"/>
<config key="org.knime.core.data.def.IntCell">
<entry key="IntCell" type="xint" value="8"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_4">
<entry key="column_name" type="xstring" value="maxIterations"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.IntCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.IntCell"/>
<config key="org.knime.core.data.def.IntCell">
<entry key="IntCell" type="xint" value="9"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.IntCell"/>
<config key="org.knime.core.data.def.IntCell">
<entry key="IntCell" type="xint" value="9"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_5">
<entry key="column_name" type="xstring" value="index_of_drugColumn"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="4.0"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="4.0"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_6">
<entry key="column_name" type="xstring" value="minSupport"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.005"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.009999999999999998"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_7">
<entry key="column_name" type="xstring" value="minConfidence"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.2"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.4000000000000001"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_8">
<entry key="column_name" type="xstring" value="cmd"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="9"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.005 0.2"/>
</config>
</config>
<config key="1">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.005 0.3"/>
</config>
</config>
<config key="2">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.005 0.4"/>
</config>
</config>
<config key="3">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.0075 0.2"/>
</config>
</config>
<config key="4">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.0075 0.3"/>
</config>
</config>
<config key="5">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.0075 0.4"/>
</config>
</config>
<config key="6">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.01 0.2"/>
</config>
</config>
<config key="7">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.01 0.3"/>
</config>
</config>
<config key="8">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="klink -ssh -auto_store_sshkey -pw 1q2w3e4r y23ycc01@140.110.30.32  spark-submit TCMAnalyzer2.py /home_i1/y23ycc01/incoming/tid.eczema.csv 4 0.01 0.4"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_9">
<entry key="column_name" type="xstring" value="RowID"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="9"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="0"/>
</config>
</config>
<config key="1">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="1"/>
</config>
</config>
<config key="2">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="2"/>
</config>
</config>
<config key="3">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="3"/>
</config>
</config>
<config key="4">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="4"/>
</config>
</config>
<config key="5">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="5"/>
</config>
</config>
<config key="6">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="6"/>
</config>
</config>
<config key="7">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="7"/>
</config>
</config>
<config key="8">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="8"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_10">
<entry key="column_name" type="xstring" value="config_host_ip"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="140.110.30.32"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_11">
<entry key="column_name" type="xstring" value="task_minConfidenceLower"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="20.0"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="20.0"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_12">
<entry key="column_name" type="xstring" value="task_local_attr_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData\metadata.DRUG.feature.TCM.summary.level123.QiFlavorMeri.20170621.csv"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_13">
<entry key="column_name" type="xstring" value="config_password"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="1q2w3e4r"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_14">
<entry key="column_name" type="xstring" value="config_local_NHIcode_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="metadata.DRUG.TCM.HF_SH.20170703.csv"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_15">
<entry key="column_name" type="xstring" value="config_local_metadata_fullpath"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_16">
<entry key="column_name" type="xstring" value="task_minConfidenceStep"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="10.0"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="10.0"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_17">
<entry key="column_name" type="xstring" value="task_local_keyword_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData\metadata.DRUG.keyword.tab"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_18">
<entry key="column_name" type="xstring" value="task_drugColumnIndex"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="4.0"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="4.0"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_19">
<entry key="column_name" type="xstring" value="task_updateCachedDataFile"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="Y"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_20">
<entry key="column_name" type="xstring" value="task_name_Ch"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="濕疹中醫治療圖譜"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_21">
<entry key="column_name" type="xstring" value="config_local_output_fullpath"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData/Sample"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_22">
<entry key="column_name" type="xstring" value="task_minSupportUpper"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="1.25"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="1.25"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_23">
<entry key="column_name" type="xstring" value="task_local_cachedData_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData\metadata.cachedData.knime"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_24">
<entry key="column_name" type="xstring" value="task_updateCachedPubmedFile"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="N"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_25">
<entry key="column_name" type="xstring" value="task_name"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="Eczema"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_26">
<entry key="column_name" type="xstring" value="task_minConfidenceUpper"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="50.0"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="50.0"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_27">
<entry key="column_name" type="xstring" value="config_local_translation_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="metadata.DRUG.NAME.translation_shortform.20170625.tab"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_28">
<entry key="column_name" type="xstring" value="task_local_translation_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData\metadata.DRUG.NAME.translation_shortform.20170625.tab"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_29">
<entry key="column_name" type="xstring" value="config_graphspace_id"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="yuchn.chen@gmail.com"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_30">
<entry key="column_name" type="xstring" value="task_local_NHIcode_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData\metadata.DRUG.TCM.HF_SH.20170703.csv"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_31">
<entry key="column_name" type="xstring" value="config_graphspace_pwd"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="1q2w3e4r"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_32">
<entry key="column_name" type="xstring" value="task_local_cachedPubmed_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:/Dropbox/1.Projects/Cytoscape/testData\metadata.cachedPubmed.knime"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_33">
<entry key="column_name" type="xstring" value="config_local_cachedData_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="metadata.cachedData.knime"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_34">
<entry key="column_name" type="xstring" value="task_remote_srcFilemname"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="/home_i1/y23ycc01/incoming/tid.eczema.csv"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_35">
<entry key="column_name" type="xstring" value="config_local_cachedPubmed_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="metadata.cachedPubmed.knime"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_36">
<entry key="column_name" type="xstring" value="task_remote_srcFilepath"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="/home_i1/y23ycc01/incoming"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_37">
<entry key="column_name" type="xstring" value="config_user_id"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="y23ycc01"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_38">
<entry key="column_name" type="xstring" value="task_forceReanalyze"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="Y"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_39">
<entry key="column_name" type="xstring" value="config_workpath"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="incoming"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_40">
<entry key="column_name" type="xstring" value="task_reuploadSrcFile"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="Y"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_41">
<entry key="column_name" type="xstring" value="task_minSupportLower"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.5"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.5"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_42">
<entry key="column_name" type="xstring" value="config_local_keyword_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="metadata.DRUG.keyword.tab"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_43">
<entry key="column_name" type="xstring" value="task_minSupportStep"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
</config>
<config key="column_domain">
<config key="lower_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.25"/>
</config>
</config>
<config key="upper_bound">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
<config key="org.knime.core.data.def.DoubleCell">
<entry key="DoubleCell" type="xdouble" value="0.25"/>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_44">
<entry key="column_name" type="xstring" value="config_local_attr_filename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="metadata.DRUG.feature.TCM.summary.level123.QiFlavorMeri.20170621.csv"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_45">
<entry key="column_name" type="xstring" value="task_srcDirectory"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:\Dropbox\1.Projects\Cytoscape\testData\Sample"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_46">
<entry key="column_name" type="xstring" value="task_srcFilename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="tid.eczema"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_47">
<entry key="column_name" type="xstring" value="task_srcFileExt"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value=".csv"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_48">
<entry key="column_name" type="xstring" value="task_local_srcfilename"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:\Dropbox\1.Projects\Cytoscape\testData\Sample\tid.eczema.csv"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
<config key="column_spec_49">
<entry key="column_name" type="xstring" value="knime.workspace"/>
<config key="element_names">
<entry key="array-size" type="xint" value="0"/>
</config>
<config key="column_type">
<entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
</config>
<config key="column_domain">
<config key="possible_values">
<entry key="array-size" type="xint" value="1"/>
<config key="0">
<entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
<config key="org.knime.core.data.def.StringCell">
<entry key="StringCell" type="xstring" value="C:\Users\yuchn\knime-workspace"/>
</config>
</config>
</config>
</config>
<config key="column_properties"/>
</config>
</config>
